2025-06-22 12:30:40,595 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2025-06-22 12:30:40,595 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Configure stats pid to 3714665
2025-06-22 12:30:40,595 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/.config/wandb/settings
2025-06-22 12:30:40,595 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/causal-core-su25/wandb/settings
2025-06-22 12:30:40,595 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'mfrl_train_eval.py', 'program_abspath': '/home/kpatherya3/causal-core-su25/mfrl_train_eval.py', 'program': 'mfrl_train_eval.py'}
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_setup.py:_flush():79] Applying login settings: {}
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:_log_setup():533] Logging user logs to /home/kpatherya3/causal-core-su25/wandb/run-20250622_123040-6nw6wxw3/logs/debug.log
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:_log_setup():534] Logging internal logs to /home/kpatherya3/causal-core-su25/wandb/run-20250622_123040-6nw6wxw3/logs/debug-internal.log
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:init():619] calling init triggers
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:init():627] wandb.init called with sweep_config: {}
config: {'task_name': 'picking', 'maximum_episode_length': 600, 'skip_frame': 3, 'seed_num': 0, 'total_time_steps': 60000000, 'n_sampled_goal': 4, 'goal_selection_strategy': 'future', 'gamma': 0.98, 'tau': 0.01, 'ent_coef': 'auto', 'target_entropy': -9, 'learning_rate': 0.00025, 'buffer_size': 1000000, 'learning_starts': 1000, 'batch_size': 256, 'train_freq': 1, 'gradient_steps': 1, 'tensorboard_log': 'her_sac_picking_20250622-123040'}
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:init():669] starting backend
2025-06-22 12:30:40,596 INFO    MainThread:3714665 [wandb_init.py:init():673] sending inform_init request
2025-06-22 12:30:40,599 INFO    MainThread:3714665 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-06-22 12:30:40,599 INFO    MainThread:3714665 [wandb_init.py:init():686] backend started and connected
2025-06-22 12:30:40,602 INFO    MainThread:3714665 [wandb_init.py:init():781] updated telemetry
2025-06-22 12:30:40,619 INFO    MainThread:3714665 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2025-06-22 12:30:41,040 INFO    MainThread:3714665 [wandb_init.py:init():867] starting run threads in backend
2025-06-22 12:30:41,131 INFO    MainThread:3714665 [wandb_run.py:_console_start():2456] atexit reg
2025-06-22 12:30:41,132 INFO    MainThread:3714665 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2025-06-22 12:30:41,132 INFO    MainThread:3714665 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-06-22 12:30:41,132 INFO    MainThread:3714665 [wandb_run.py:_redirect():2395] Redirects installed.
2025-06-22 12:30:41,134 INFO    MainThread:3714665 [wandb_init.py:init():911] run started, returning control to user process
2025-06-22 12:30:43,298 INFO    MainThread:3714665 [wandb_run.py:_tensorboard_callback():1544] tensorboard callback: her_sac_picking_20250622-123040/her_sac_1, True
2025-06-22 12:30:43,302 INFO    MainThread:3714665 [wandb_watch.py:_watch():71] Watching
2025-06-22 12:30:43,303 INFO    MainThread:3714665 [wandb_run.py:_config_callback():1387] config_cb None None {'algo': 'SAC', 'policy_class': "<class 'stable_baselines3.sac.policies.MultiInputPolicy'>", 'device': 'cuda', 'env': '<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f0f3512ee50>', '_vec_normalize_env': 'None', 'verbose': 1, 'policy_kwargs': "{'use_sde': False}", 'observation_space': 'Dict(achieved_goal:Box(6,), desired_goal:Box(6,), observation:Box(56,))', 'action_space': 'Box(9,)', 'n_envs': 1, 'num_timesteps': 0, '_total_timesteps': 60000000, '_num_timesteps_at_start': 0, 'eval_env': 'None', 'seed': 0, 'action_noise': 'None', 'start_time': 1750609843.295049, 'policy': 'MultiInputPolicy(\n  (actor): Actor(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=68, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=9, bias=True)\n    (log_std): Linear(in_features=256, out_features=9, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=77, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=77, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (critic_target): ContinuousCritic(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=77, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=77, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)', 'lr_schedule': '<function constant_fn.<locals>.func at 0x7f0f350d25f0>', '_last_obs': "OrderedDict([('achieved_goal', array([[-3.25000000e-02, -3.25000000e-02, -3.46944695e-18,\n         3.25000000e-02,  3.25000000e-02,  6.50000000e-02]])), ('desired_goal', array([[-0.0325, -0.0325,  0.1175,  0.0325,  0.0325,  0.1825]])), ('observation', array([[ 1.        ,  0.22178988,  0.43350048, -0.26179939,  0.22178988,\n         0.43350048, -0.26179939,  0.22178988,  0.43350048, -0.26179939,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        , -0.1189    ,\n         0.31727417, -0.77654834,  0.33421749, -0.05566666, -0.77654834,\n        -0.21531749, -0.26160751, -0.77654834,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        , -0.87      ,\n         0.        ,  0.        ,  0.        ,  0.1       ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        20.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        , -0.4       ,  0.        ,  0.        ,  0.        ,\n         0.1       ]]))])", '_last_episode_starts': '[ True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_logger': '<stable_baselines3.common.logger.Logger object at 0x7f0f351db690>', '_custom_logger': 'False', 'optimize_memory_usage': 'False', 'replay_buffer_class': "<class 'stable_baselines3.her.her_replay_buffer.HerReplayBuffer'>", 'replay_buffer_kwargs': "{'n_sampled_goal': 4, 'goal_selection_strategy': 'future', 'max_episode_length': 600}", '_episode_storage': 'None', 'remove_time_limit_termination': 'False', 'actor': 'Actor(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (latent_pi): Sequential(\n    (0): Linear(in_features=68, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): ReLU()\n  )\n  (mu): Linear(in_features=256, out_features=9, bias=True)\n  (log_std): Linear(in_features=256, out_features=9, bias=True)\n)', 'replay_buffer': '<stable_baselines3.her.her_replay_buffer.HerReplayBuffer object at 0x7f0f35182e90>', 'use_sde_at_warmup': 'False', 'log_ent_coef': "tensor([0.], device='cuda:0', requires_grad=True)", 'target_update_interval': 1, 'ent_coef_optimizer': 'Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: False\n    lr: 0.00025\n    maximize: False\n    weight_decay: 0\n)', 'critic': 'ContinuousCritic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=77, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=77, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)', 'critic_target': 'ContinuousCritic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=77, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=77, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)'}
