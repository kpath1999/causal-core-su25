
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 10
   student training steps: 10000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:227: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4623', '1.2807', '1.2986', '1.2467', '1.3687']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.4949
action disagreement: 0.5153
total CM score: 1.7433
goal is complete. CM score: 1.7433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6173', '1.5881', '1.5435', '1.7845', '1.4572']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.5085
action disagreement: 0.5139
total CM score: 1.5571
mass is complete. CM score: 1.5571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3495', '1.5457', '1.8414', '1.6155', '1.6662']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.4994
action disagreement: 0.5360
total CM score: 1.5964
friction is complete. CM score: 1.5964
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.6746', '1.3558', '1.4321', '1.5538']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.4757
action disagreement: 0.5367
total CM score: 1.4193
visual is complete. CM score: 1.4193
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7699', '1.7515', '1.6568', '1.7130', '1.9990']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5431
action disagreement: 0.5396
total CM score: 1.6187
pose is complete. CM score: 1.6187
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3205', '1.5355', '1.5504', '1.3586', '1.4045']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.5242
action disagreement: 0.5467
total CM score: 1.8817
random is complete. CM score: 1.8817
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 431       |
|    iterations      | 1         |
|    time_elapsed    | 9         |
|    total_timesteps | 5050368   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 272        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 5054464    |
| train/                  |            |
|    approx_kl            | 0.07661824 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.391     |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 2.71       |
|    value_loss           | 0.967      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 3           |
|    time_elapsed         | 50          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.082165435 |
|    clip_fraction        | 0.497       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 2.74        |
|    value_loss           | 0.233       |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-0.529, success=False
episode 2: length=501, reward=-0.529, success=False
episode 3: length=501, reward=-0.529, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.529
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 1.211
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -4.597
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.321
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.653
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8139', '8.3840', '7.4632', '7.7983', '8.2479']
Training reward models...
Reward model losses: ['0.0070', '0.1873', '0.0226', '1.2905', '1.9432']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5852', '1.5802', '1.3357', '1.3299', '1.4113']
Training action VAE models...
Action VAE losses: ['1.3664', '1.3491', '1.3878', '1.4321', '1.4796']
CM score components:
transition disagreement: 0.4527
reward disagreement: 0.4897
state disagreement: 0.5094
action disagreement: 0.5429
total CM score: 1.9947
goal is complete. CM score: 1.9947
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.776
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 0.960
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.335
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.646
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7901', '7.4058', '8.1565', '7.5496', '6.8763']
Training reward models...
Reward model losses: ['0.5048', '0.5136', '0.0171', '0.1977', '0.1992']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7375', '1.4908', '1.7464', '1.6836', '2.0111']
Training action VAE models...
Action VAE losses: ['1.4007', '1.4121', '1.2986', '1.4541', '1.4689']
CM score components:
transition disagreement: 0.4510
reward disagreement: 0.2856
state disagreement: 0.5126
action disagreement: 0.5359
total CM score: 1.7851
mass is complete. CM score: 1.7851
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.428
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.145
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.456
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.422
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7255', '7.3530', '7.3551', '6.8776', '8.0426']
Training reward models...
Reward model losses: ['0.7098', '0.1099', '0.0327', '0.0529', '0.2626']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.0541', '1.9098', '1.6674', '2.0737', '1.7862']
Training action VAE models...
Action VAE losses: ['1.4191', '1.4343', '1.3859', '1.4457', '1.4730']
CM score components:
transition disagreement: 0.4217
reward disagreement: 0.2601
state disagreement: 0.5453
action disagreement: 0.5470
total CM score: 1.7741
friction is complete. CM score: 1.7741
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -0.529
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.529
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -0.529
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.529
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7703', '7.5384', '7.7411', '7.6892', '7.3529']
Training reward models...
Reward model losses: ['0.5774', '0.5690', '0.0079', '0.0047', '0.1671']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7792', '2.1081', '1.9988', '1.8700', '1.7326']
Training action VAE models...
Action VAE losses: ['1.3947', '1.4263', '1.6450', '1.4297', '1.3746']
CM score components:
transition disagreement: 0.4084
reward disagreement: 0.2512
state disagreement: 0.5582
action disagreement: 0.5818
total CM score: 1.7996
visual is complete. CM score: 1.7996
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 2.330
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.572
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.536
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.425
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9353', '7.2144', '8.2229', '8.0715', '6.7258']
Training reward models...
Reward model losses: ['0.3855', '0.0125', '0.1418', '0.0131', '0.0222']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1509', '2.1968', '1.9709', '1.8205', '2.0004']
Training action VAE models...
Action VAE losses: ['1.4474', '1.4105', '1.3476', '1.4441', '1.4399']
CM score components:
transition disagreement: 0.4287
reward disagreement: 0.0922
state disagreement: 0.5890
action disagreement: 0.5175
total CM score: 1.6274
pose is complete. CM score: 1.6274
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.720
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.726
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.087
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.581
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7537', '7.6662', '6.9746', '7.8864', '7.9396']
Training reward models...
Reward model losses: ['0.8278', '0.4002', '0.1158', '1.0617', '0.0392']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3294', '1.4135', '1.4936', '1.4072', '1.4505']
Training action VAE models...
Action VAE losses: ['1.4451', '1.5636', '1.3979', '1.3912', '1.3494']
CM score components:
transition disagreement: 0.4212
reward disagreement: 0.4614
state disagreement: 0.5077
action disagreement: 0.5568
total CM score: 1.9471
random is complete. CM score: 1.9471
INFO:root:Meta-Episode 1/10: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 1.211
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -4.597
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.321
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.653
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8475', '7.1836', '7.4591', '7.3193', '7.2293']
Training reward models...
Reward model losses: ['0.2449', '0.0999', '0.7838', '0.4785', '0.1477']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3773', '1.4216', '1.4813', '1.5384', '1.4899']
Training action VAE models...
Action VAE losses: ['1.2963', '1.3152', '1.3242', '1.4807', '1.3768']
CM score components:
transition disagreement: 0.3706
reward disagreement: 0.2902
state disagreement: 0.4951
action disagreement: 0.5397
total CM score: 1.6956
goal is complete. CM score: 1.6956
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.776
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 0.960
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.335
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.646
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4549', '7.5404', '7.5177', '7.1412', '7.3066']
Training reward models...
Reward model losses: ['0.2839', '0.2708', '0.0111', '0.2625', '0.0138']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7259', '1.9617', '2.0137', '1.8797', '1.7185']
Training action VAE models...
Action VAE losses: ['1.3159', '1.4306', '1.4689', '1.4107', '1.6319']
CM score components:
transition disagreement: 0.4163
reward disagreement: 0.1257
state disagreement: 0.5719
action disagreement: 0.5866
total CM score: 1.7005
mass is complete. CM score: 1.7005
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.428
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.145
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.456
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.422
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1204', '7.2288', '7.2652', '7.1224', '7.6179']
Training reward models...
Reward model losses: ['0.1389', '0.2233', '0.0160', '0.0114', '0.0167']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8640', '1.6182', '1.6953', '1.7994', '1.7389']
Training action VAE models...
Action VAE losses: ['1.3184', '1.4745', '1.4192', '1.6270', '1.3099']
CM score components:
transition disagreement: 0.4341
reward disagreement: 0.0860
state disagreement: 0.5241
action disagreement: 0.5620
total CM score: 1.6061
friction is complete. CM score: 1.6061
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -0.529
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.529
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -0.529
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.529
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9426', '7.2647', '7.3421', '6.9701', '7.3374']
Training reward models...
Reward model losses: ['0.6786', '0.0435', '0.0043', '0.0147', '0.0191']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7054', '1.8262', '1.5880', '1.7622', '1.9020']
Training action VAE models...
Action VAE losses: ['1.4283', '1.3409', '1.4833', '1.4127', '1.3864']
CM score components:
transition disagreement: 0.4345
reward disagreement: 0.1680
state disagreement: 0.5276
action disagreement: 0.5610
total CM score: 1.6912
visual is complete. CM score: 1.6912
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 2.330
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.572
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.536
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.425
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.6942', '7.6484', '8.2433', '7.1545', '7.8850']
Training reward models...
Reward model losses: ['0.1817', '0.5282', '0.0138', '0.1903', '0.0682']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7585', '1.4200', '1.8622', '1.7581', '1.6055']
Training action VAE models...
Action VAE losses: ['1.4781', '1.4613', '1.4160', '1.4317', '1.3990']
CM score components:
transition disagreement: 0.4232
reward disagreement: 0.1857
state disagreement: 0.5308
action disagreement: 0.5803
total CM score: 1.7199
pose is complete. CM score: 1.7199
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.720
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.726
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.087
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.581
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2973', '7.9420', '7.3424', '7.4756', '7.3790']
Training reward models...
Reward model losses: ['0.3690', '0.1566', '0.1307', '0.1815', '0.0492']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5172', '1.5174', '1.4861', '1.4109', '1.4148']
Training action VAE models...
Action VAE losses: ['1.3467', '1.6113', '1.4854', '1.4525', '1.4976']
CM score components:
transition disagreement: 0.4248
reward disagreement: 0.1363
state disagreement: 0.5206
action disagreement: 0.6129
total CM score: 1.6946
random is complete. CM score: 1.6946
IntervenedCausalWorld created with pose intervention
Reset #1: pose intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: pose intervention applied (success: True)
Reset #3: pose intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 443       |
|    iterations      | 1         |
|    time_elapsed    | 9         |
|    total_timesteps | 5062656   |
----------------------------------
Traceback (most recent call last):
  File "meta_teacher_student.py", line 755, in <module>
    main()
  File "meta_teacher_student.py", line 726, in main
    student_model.learn(total_timesteps=args.student_train_steps, reset_num_timesteps=False)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 250, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 207, in collect_rollouts
    rollout_buffer.add(self._last_obs, actions, rewards, self._last_episode_starts, values, log_probs)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/buffers.py", line 432, in add
    self.values[self.pos] = value.clone().cpu().numpy().flatten()
KeyboardInterrupt
