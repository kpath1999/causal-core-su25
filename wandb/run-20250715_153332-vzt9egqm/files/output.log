
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 10
   student training steps: 10000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:208: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6558', '7.2590', '7.4603', '7.0444', '7.8105']
Training reward models...
Reward model losses: ['0.0199', '0.0129', '0.0604', '0.6542', '1.3469']
Training state VAE models...
State VAE losses: ['339382.6688', '325178.6088', '337494.8650', '287234.8750', '286734.8738']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.4329
reward disagreement: 0.3679
state disagreement: 1.1411
action disagreement: nan
total CM score: nan
goal is complete. CM score: nan
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8274', '7.1797', '7.7339', '7.4570', '7.0774']
Training reward models...
Reward model losses: ['0.0928', '0.0294', '0.0751', '0.0149', '0.0241']
Training state VAE models...
State VAE losses: ['432084.4025', '278429.7475', '363146.4537', '498652.6275', '331146.3088']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.4323
reward disagreement: 0.0959
state disagreement: 1.2219
action disagreement: nan
total CM score: nan
mass is complete. CM score: nan
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3644', '7.2733', '7.7272', '7.1229', '7.5792']
Training reward models...
Reward model losses: ['0.0454', '0.1350', '0.0794', '0.0490', '0.0102']
Training state VAE models...
State VAE losses: ['362438.2975', '385135.7400', '374291.7975', '303221.7750', '396002.4913']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.4503
reward disagreement: 0.1070
state disagreement: 1.2145
action disagreement: nan
total CM score: nan
friction is complete. CM score: nan
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8927', '7.3291', '6.7310', '7.6955', '8.1756']
Training reward models...
Reward model losses: ['0.0504', '0.0480', '0.0248', '0.1619', '0.0191']
Training state VAE models...
State VAE losses: ['306238.6238', '365576.2313', '358987.7363', '775403.0275', '259327.7631']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.4662
reward disagreement: 0.0815
state disagreement: 1.2585
action disagreement: nan
total CM score: nan
visual is complete. CM score: nan
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0420', '7.9146', '7.0298', '7.7693', '7.6735']
Training reward models...
Reward model losses: ['0.0669', '1.7440', '0.0598', '0.0736', '0.0192']
Training state VAE models...
State VAE losses: ['329496.3063', '513170.2225', '425714.6863', '533234.9025', '351590.3550']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.4948
reward disagreement: 0.4424
state disagreement: 1.2805
action disagreement: nan
total CM score: nan
pose is complete. CM score: nan
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3545', '7.3995', '7.3378', '7.2298', '7.3644']
Training reward models...
Reward model losses: ['0.0541', '0.0408', '0.1387', '0.0601', '0.0735']
Training state VAE models...
State VAE losses: ['62209.5395', '69036.6356', '67104.7384', '64556.0987', '61786.3192']
Training action VAE models...
Action VAE losses: ['nan', 'nan', 'nan', 'nan', 'nan']
CM score components:
transition disagreement: 0.2127
reward disagreement: 0.1023
state disagreement: 0.7321
action disagreement: nan
total CM score: nan
random is complete. CM score: nan
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 318       |
|    iterations      | 1         |
|    time_elapsed    | 12        |
|    total_timesteps | 5050368   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 2          |
|    time_elapsed         | 40         |
|    total_timesteps      | 5054464    |
| train/                  |            |
|    approx_kl            | 0.07661824 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.391     |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 2.71       |
|    value_loss           | 0.967      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 3           |
|    time_elapsed         | 68          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.082165435 |
|    clip_fraction        | 0.497       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 2.74        |
|    value_loss           | 0.233       |
-----------------------------------------
Traceback (most recent call last):
  File "meta_teacher_student.py", line 622, in <module>
    main()
  File "meta_teacher_student.py", line 593, in main
    student_model.learn(total_timesteps=args.student_train_steps, reset_num_timesteps=False)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 270, in learn
    self.train()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 258, in train
    self.policy.optimizer.zero_grad()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/optim/optimizer.py", line 279, in zero_grad
    p.grad.zero_()
KeyboardInterrupt
