
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 30
   student training steps: 50000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:228: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4623', '1.2807', '1.2986', '1.2467', '1.3687']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.4949
action disagreement: 0.5153
total CM score: 1.7433
goal is complete. CM score: 1.7433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6173', '1.5881', '1.5435', '1.7845', '1.4572']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.5085
action disagreement: 0.5139
total CM score: 1.5571
mass is complete. CM score: 1.5571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3495', '1.5457', '1.8414', '1.6155', '1.6662']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.4994
action disagreement: 0.5360
total CM score: 1.5964
friction is complete. CM score: 1.5964
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.6746', '1.3558', '1.4321', '1.5538']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.4757
action disagreement: 0.5367
total CM score: 1.4193
visual is complete. CM score: 1.4193
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7699', '1.7515', '1.6568', '1.7130', '1.9990']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5431
action disagreement: 0.5396
total CM score: 1.6187
pose is complete. CM score: 1.6187
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3205', '1.5355', '1.5504', '1.3586', '1.4045']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.5242
action disagreement: 0.5467
total CM score: 1.8817
random is complete. CM score: 1.8817
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 461       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.033029757 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.535      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0954     |
|    std                  | 2.66        |
|    value_loss           | 0.0398      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 3           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.035446957 |
|    clip_fraction        | 0.396       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.527      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0843     |
|    std                  | 2.66        |
|    value_loss           | 0.0269      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 4           |
|    time_elapsed         | 68          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.035549812 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.536      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.091      |
|    std                  | 2.68        |
|    value_loss           | 0.0177      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 5           |
|    time_elapsed         | 88          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.041886266 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.478      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0682     |
|    std                  | 2.71        |
|    value_loss           | 0.0567      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 6          |
|    time_elapsed         | 107        |
|    total_timesteps      | 5070848    |
| train/                  |            |
|    approx_kl            | 0.46430597 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.541     |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 2.73       |
|    value_loss           | 0.0205     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 7           |
|    time_elapsed         | 127         |
|    total_timesteps      | 5074944     |
| train/                  |             |
|    approx_kl            | 0.038512163 |
|    clip_fraction        | 0.453       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.54       |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.0966     |
|    std                  | 2.75        |
|    value_loss           | 0.023       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 8           |
|    time_elapsed         | 147         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.038204994 |
|    clip_fraction        | 0.439       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.539      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0913     |
|    std                  | 2.77        |
|    value_loss           | 0.0357      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 9          |
|    time_elapsed         | 167        |
|    total_timesteps      | 5083136    |
| train/                  |            |
|    approx_kl            | 0.04584127 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.528     |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.0931    |
|    std                  | 2.81       |
|    value_loss           | 0.0415     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 10          |
|    time_elapsed         | 187         |
|    total_timesteps      | 5087232     |
| train/                  |             |
|    approx_kl            | 0.041341044 |
|    clip_fraction        | 0.452       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.556      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.1        |
|    std                  | 2.83        |
|    value_loss           | 0.0267      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 11          |
|    time_elapsed         | 206         |
|    total_timesteps      | 5091328     |
| train/                  |             |
|    approx_kl            | 0.046489157 |
|    clip_fraction        | 0.513       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.543      |
|    n_updates            | 1305        |
|    policy_gradient_loss | -0.0971     |
|    std                  | 2.85        |
|    value_loss           | 0.0291      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 12         |
|    time_elapsed         | 226        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.04609365 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.561     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.87       |
|    value_loss           | 0.0208     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 13          |
|    time_elapsed         | 245         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.052456357 |
|    clip_fraction        | 0.523       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.3       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.563      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.103      |
|    std                  | 2.89        |
|    value_loss           | 0.0206      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_0.zip

evaluating student performance (5 episodes)...
episode 1: length=28, reward=0.201, success=True
episode 2: length=28, reward=0.201, success=True
episode 3: length=28, reward=0.201, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 0.201
average episode length: 28.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 0.546
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.251
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.432
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.668
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6600', '8.3693', '7.3452', '7.7547', '8.1663']
Training reward models...
Reward model losses: ['0.0050', '0.2329', '0.0286', '1.3615', '1.7798']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6447', '1.6117', '1.4343', '1.3699', '1.4455']
Training action VAE models...
Action VAE losses: ['1.3829', '1.3433', '1.4357', '1.4565', '1.4456']
CM score components:
transition disagreement: 0.4419
reward disagreement: 0.4586
state disagreement: 0.5240
action disagreement: 0.5415
total CM score: 1.9659
goal is complete. CM score: 1.9659
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 5.938
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 6.075
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 5.977
total data points collected: 2505
average episode length: 501.0
average episode reward: 6.119
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6791', '7.3496', '7.7814', '7.5427', '6.8157']
Training reward models...
Reward model losses: ['0.6807', '0.4189', '0.0087', '0.1139', '0.2720']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7148', '1.4406', '1.6725', '1.6333', '1.9517']
Training action VAE models...
Action VAE losses: ['1.3630', '1.3957', '1.2601', '1.3735', '1.4298']
CM score components:
transition disagreement: 0.4359
reward disagreement: 0.3343
state disagreement: 0.5148
action disagreement: 0.5221
total CM score: 1.8071
mass is complete. CM score: 1.8071
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.318
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.134
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.139
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.597
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4755', '7.3441', '7.2535', '6.7952', '8.0813']
Training reward models...
Reward model losses: ['0.7373', '0.0394', '0.0240', '0.0404', '0.2485']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9378', '1.8523', '1.5238', '2.0189', '1.6490']
Training action VAE models...
Action VAE losses: ['1.3120', '1.3508', '1.2820', '1.3409', '1.3273']
CM score components:
transition disagreement: 0.4153
reward disagreement: 0.2785
state disagreement: 0.5258
action disagreement: 0.5445
total CM score: 1.7640
friction is complete. CM score: 1.7640
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.126
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.126
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.126
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.126
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6767', '7.5017', '7.6931', '7.6225', '7.3385']
Training reward models...
Reward model losses: ['0.5405', '0.6468', '0.0058', '0.0185', '0.1961']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6627', '1.9627', '1.9322', '1.8297', '1.6372']
Training action VAE models...
Action VAE losses: ['1.1746', '1.2557', '1.2616', '1.2763', '1.2600']
CM score components:
transition disagreement: 0.4114
reward disagreement: 0.2354
state disagreement: 0.5457
action disagreement: 0.5718
total CM score: 1.7644
visual is complete. CM score: 1.7644
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 3.881
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.120
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.114
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9545', '7.1566', '8.1219', '7.9200', '6.5696']
Training reward models...
Reward model losses: ['0.4341', '0.0121', '0.1875', '0.0097', '0.0297']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1615', '2.0734', '1.8914', '1.7520', '1.9400']
Training action VAE models...
Action VAE losses: ['1.3324', '1.3158', '1.1466', '1.4115', '1.3567']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.1159
state disagreement: 0.5785
action disagreement: 0.5103
total CM score: 1.6304
pose is complete. CM score: 1.6304
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.151
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.077
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.817
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.071
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6803', '7.5753', '6.9102', '7.8302', '7.8223']
Training reward models...
Reward model losses: ['0.9204', '0.4246', '0.0650', '0.9631', '0.0098']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3381', '1.3779', '1.4317', '1.3415', '1.4005']
Training action VAE models...
Action VAE losses: ['1.4035', '1.5343', '1.4418', '1.3387', '1.3607']
CM score components:
transition disagreement: 0.4190
reward disagreement: 0.4805
state disagreement: 0.5000
action disagreement: 0.5489
total CM score: 1.9485
random is complete. CM score: 1.9485
INFO:root:Meta-Episode 1/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_0.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 0.546
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.251
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.432
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.668
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3073', '7.0841', '6.7828', '7.1555', '7.6099']
Training reward models...
Reward model losses: ['0.2346', '0.2040', '0.2570', '0.0035', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4635', '1.3450', '1.3792', '1.5967', '1.5780']
Training action VAE models...
Action VAE losses: ['1.5102', '1.3749', '1.5696', '1.3820', '1.3583']
CM score components:
transition disagreement: 0.4061
reward disagreement: 0.0979
state disagreement: 0.4921
action disagreement: 0.5593
total CM score: 1.5555
goal is complete. CM score: 1.5555
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 5.938
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 6.075
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 5.977
total data points collected: 2505
average episode length: 501.0
average episode reward: 6.119
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0583', '7.6339', '7.0574', '6.8824', '8.1730']
Training reward models...
Reward model losses: ['0.0035', '0.2961', '0.0056', '0.2835', '0.5608']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6419', '1.6767', '1.7777', '1.7878', '2.2758']
Training action VAE models...
Action VAE losses: ['1.3816', '1.3672', '1.3023', '1.3880', '1.3313']
CM score components:
transition disagreement: 0.4158
reward disagreement: 0.1750
state disagreement: 0.5891
action disagreement: 0.5499
total CM score: 1.7297
mass is complete. CM score: 1.7297
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.318
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.134
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.139
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.597
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8746', '7.2395', '6.8932', '7.6096', '7.0196']
Training reward models...
Reward model losses: ['0.0935', '1.0181', '0.3828', '0.0191', '0.1947']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7405', '1.6567', '1.7768', '1.7537', '1.8029']
Training action VAE models...
Action VAE losses: ['1.1313', '1.3008', '1.1347', '1.1367', '1.2485']
CM score components:
transition disagreement: 0.4148
reward disagreement: 0.3827
state disagreement: 0.5453
action disagreement: 0.5217
total CM score: 1.8645
friction is complete. CM score: 1.8645
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.126
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.126
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.126
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.126
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1384', '7.7649', '7.1259', '7.2592', '7.4961']
Training reward models...
Reward model losses: ['0.0401', '0.2021', '0.0631', '0.0064', '0.0406']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5378', '1.4899', '1.6868', '1.5812', '1.7192']
Training action VAE models...
Action VAE losses: ['1.1634', '1.0966', '1.1731', '1.2336', '1.2827']
CM score components:
transition disagreement: 0.4388
reward disagreement: 0.1126
state disagreement: 0.4834
action disagreement: 0.5264
total CM score: 1.5613
visual is complete. CM score: 1.5613
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 3.881
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.120
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.114
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7899', '7.7074', '7.7299', '6.6138', '7.3974']
Training reward models...
Reward model losses: ['3.2546', '1.1052', '0.1599', '0.0143', '0.0239']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.2591', '1.7126', '1.7209', '1.7544', '1.6301']
Training action VAE models...
Action VAE losses: ['1.2068', '1.3910', '1.3606', '1.3686', '1.1990']
CM score components:
transition disagreement: 0.4404
reward disagreement: 0.7524
state disagreement: 0.5660
action disagreement: 0.5547
total CM score: 2.3135
pose is complete. CM score: 2.3135
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.151
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.077
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.817
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.071
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5452', '7.3277', '8.2951', '7.4403', '7.3352']
Training reward models...
Reward model losses: ['0.1064', '0.0257', '0.0201', '0.2511', '0.1674']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4047', '1.3330', '1.4363', '1.3700', '1.4021']
Training action VAE models...
Action VAE losses: ['1.4407', '1.3099', '1.3988', '1.4534', '1.5134']
CM score components:
transition disagreement: 0.3832
reward disagreement: 0.1022
state disagreement: 0.5004
action disagreement: 0.5783
total CM score: 1.5641
random is complete. CM score: 1.5641
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 464       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5103616   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 451         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5107712     |
| train/                  |             |
|    approx_kl            | 0.045231037 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.504      |
|    n_updates            | 1365        |
|    policy_gradient_loss | -0.0508     |
|    std                  | 2.93        |
|    value_loss           | 0.0515      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.030364338 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0418     |
|    std                  | 2.97        |
|    value_loss           | 0.0392      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 441        |
|    iterations           | 4          |
|    time_elapsed         | 37         |
|    total_timesteps      | 5115904    |
| train/                  |            |
|    approx_kl            | 0.03203421 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.493     |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.0445    |
|    std                  | 3          |
|    value_loss           | 0.0632     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 251       |
|    ep_rew_mean          | 2.2542775 |
| time/                   |           |
|    fps                  | 440       |
|    iterations           | 5         |
|    time_elapsed         | 46        |
|    total_timesteps      | 5120000   |
| train/                  |           |
|    approx_kl            | 0.0432564 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.48     |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0306   |
|    std                  | 3.03      |
|    value_loss           | 0.0993    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 6           |
|    time_elapsed         | 55          |
|    total_timesteps      | 5124096     |
| train/                  |             |
|    approx_kl            | 0.025868267 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1425        |
|    policy_gradient_loss | -0.0478     |
|    std                  | 3.05        |
|    value_loss           | 0.0585      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5128192     |
| train/                  |             |
|    approx_kl            | 0.031373758 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.48       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0521     |
|    std                  | 3.07        |
|    value_loss           | 0.103       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 8           |
|    time_elapsed         | 74          |
|    total_timesteps      | 5132288     |
| train/                  |             |
|    approx_kl            | 0.025940482 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.9       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.482      |
|    n_updates            | 1455        |
|    policy_gradient_loss | -0.042      |
|    std                  | 3.1         |
|    value_loss           | 0.0899      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 9           |
|    time_elapsed         | 83          |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.019549722 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23         |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.502      |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0473     |
|    std                  | 3.12        |
|    value_loss           | 0.0556      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 5140480     |
| train/                  |             |
|    approx_kl            | 0.025581973 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.1       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.493      |
|    n_updates            | 1485        |
|    policy_gradient_loss | -0.0429     |
|    std                  | 3.15        |
|    value_loss           | 0.0517      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 438        |
|    iterations           | 11         |
|    time_elapsed         | 102        |
|    total_timesteps      | 5144576    |
| train/                  |            |
|    approx_kl            | 0.02706469 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.499     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0395    |
|    std                  | 3.17       |
|    value_loss           | 0.0563     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 12          |
|    time_elapsed         | 112         |
|    total_timesteps      | 5148672     |
| train/                  |             |
|    approx_kl            | 0.018755198 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.2       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.496      |
|    n_updates            | 1515        |
|    policy_gradient_loss | -0.0332     |
|    std                  | 3.19        |
|    value_loss           | 0.0653      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 13          |
|    time_elapsed         | 121         |
|    total_timesteps      | 5152768     |
| train/                  |             |
|    approx_kl            | 0.018374024 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.2       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.48       |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0376     |
|    std                  | 3.21        |
|    value_loss           | 0.0857      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_1.zip

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-0.814, success=False
episode 2: length=501, reward=-0.814, success=False
episode 3: length=501, reward=-0.814, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.814
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.739
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -0.825
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.849
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.813
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1702', '7.2273', '7.7917', '7.8239', '8.8966']
Training reward models...
Reward model losses: ['0.1361', '0.0151', '0.2222', '0.0023', '0.0080']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1660', '1.2042', '1.2539', '1.1526', '1.1983']
Training action VAE models...
Action VAE losses: ['0.7736', '0.8674', '0.7279', '0.8124', '0.7717']
CM score components:
transition disagreement: 0.4608
reward disagreement: 0.0971
state disagreement: 0.4741
action disagreement: 0.5403
total CM score: 1.5722
goal is complete. CM score: 1.5722
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -0.814
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -0.814
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.814
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.814
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3120', '7.2532', '7.6183', '8.6337', '7.9728']
Training reward models...
Reward model losses: ['0.0034', '0.1472', '1.6965', '0.1506', '0.1901']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6159', '1.4326', '1.6720', '1.8057', '1.8255']
Training action VAE models...
Action VAE losses: ['0.7651', '0.7694', '0.7296', '0.7251', '0.6893']
CM score components:
transition disagreement: 0.4214
reward disagreement: 0.4343
state disagreement: 0.5651
action disagreement: 0.5120
total CM score: 1.9328
mass is complete. CM score: 1.9328
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.816
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -0.813
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.815
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.816
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1214', '7.5343', '7.0384', '7.5943', '6.4722']
Training reward models...
Reward model losses: ['0.0087', '0.0199', '0.1647', '0.0158', '0.1937']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5074', '1.5948', '1.6191', '1.5339', '1.8829']
Training action VAE models...
Action VAE losses: ['0.8219', '0.7924', '0.7029', '0.6500', '0.8235']
CM score components:
transition disagreement: 0.4308
reward disagreement: 0.1137
state disagreement: 0.5391
action disagreement: 0.5194
total CM score: 1.6032
friction is complete. CM score: 1.6032
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -0.814
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.814
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -0.814
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.814
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8706', '7.6472', '7.2647', '6.9186', '7.5324']
Training reward models...
Reward model losses: ['0.0015', '0.0319', '0.2584', '0.2360', '0.2054']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5307', '1.5758', '1.4258', '1.8665', '1.6124']
Training action VAE models...
Action VAE losses: ['0.9251', '0.7885', '0.8013', '0.8237', '0.8056']
CM score components:
transition disagreement: 0.4379
reward disagreement: 0.1362
state disagreement: 0.5511
action disagreement: 0.5499
total CM score: 1.6750
visual is complete. CM score: 1.6750
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 3.657
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.178
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -0.534
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.504
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4089', '7.2239', '7.5142', '7.3413', '7.3377']
Training reward models...
Reward model losses: ['0.1788', '0.0303', '0.3670', '0.0161', '0.0110']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8212', '1.9370', '1.6521', '1.9283', '1.8105']
Training action VAE models...
Action VAE losses: ['1.2344', '1.3103', '1.3425', '1.4684', '1.4321']
CM score components:
transition disagreement: 0.4233
reward disagreement: 0.1642
state disagreement: 0.5521
action disagreement: 0.5237
total CM score: 1.6633
pose is complete. CM score: 1.6633
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -1.284
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.400
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.235
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.097
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8255', '7.6818', '8.4500', '7.8708', '7.3902']
Training reward models...
Reward model losses: ['0.0263', '0.0155', '0.3262', '0.0138', '0.1481']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3446', '1.5308', '1.5495', '1.2903', '1.2874']
Training action VAE models...
Action VAE losses: ['2.2362', '2.8066', '2.2520', '2.5116', '2.1778']
CM score components:
transition disagreement: 0.4363
reward disagreement: 0.1517
state disagreement: 0.5028
action disagreement: 0.5884
total CM score: 1.6791
random is complete. CM score: 1.6791
INFO:root:Meta-Episode 2/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_1.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
Traceback (most recent call last):
  File "meta_teacher_student.py", line 864, in <module>
    else:
  File "meta_teacher_student.py", line 807, in main
    epsilon_final = 0.1
  File "meta_teacher_student.py", line 474, in get_teacher_state
    action_mode='joint_torques',
  File "meta_teacher_student.py", line 200, in evaluate_cm_score
    for episode in range(episodes):
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/base_class.py", line 562, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/policies.py", line 338, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/policies.py", line 630, in _predict
    return self.get_distribution(observation).get_actions(deterministic=deterministic)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/policies.py", line 658, in get_distribution
    latent_pi = self.mlp_extractor.forward_actor(features)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/torch_layers.py", line 231, in forward_actor
    return self.policy_net(self.shared_net(features))
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1189, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
KeyboardInterrupt
