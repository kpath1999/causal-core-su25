Using cuda device
Logging to ppo_pushing_sb3/ppo_sb3_1
/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/callbacks.py:337: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7ffa58324c50> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ffa58368c10>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
------------------------------
| time/              |       |
|    fps             | 1864  |
|    iterations      | 1     |
|    time_elapsed    | 35    |
|    total_timesteps | 65536 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1595        |
|    iterations           | 2           |
|    time_elapsed         | 82          |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.005448374 |
|    clip_fraction        | 0.0509      |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.9       |
|    explained_variance   | 0.02        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.254      |
|    n_updates            | 15          |
|    policy_gradient_loss | 0.000358    |
|    std                  | 1.02        |
|    value_loss           | 0.0227      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1452       |
|    iterations           | 3          |
|    time_elapsed         | 135        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.00685087 |
|    clip_fraction        | 0.0589     |
|    clip_range           | 0.2        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.261     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.000949  |
|    std                  | 1.04       |
|    value_loss           | 0.0118     |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1379         |
|    iterations           | 4            |
|    time_elapsed         | 189          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0075534815 |
|    clip_fraction        | 0.0773       |
|    clip_range           | 0.2          |
|    entropy_loss         | -13.3        |
|    explained_variance   | 0.634        |
|    learning_rate        | 0.00025      |
|    loss                 | -0.273       |
|    n_updates            | 45           |
|    policy_gradient_loss | -0.00331     |
|    std                  | 1.06         |
|    value_loss           | 0.0058       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1332        |
|    iterations           | 5           |
|    time_elapsed         | 245         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.008778273 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.275      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00672    |
|    std                  | 1.07        |
|    value_loss           | 0.0029      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1309        |
|    iterations           | 6           |
|    time_elapsed         | 300         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.008358613 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.5       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.278      |
|    n_updates            | 75          |
|    policy_gradient_loss | -0.00804    |
|    std                  | 1.08        |
|    value_loss           | 0.00136     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1282        |
|    iterations           | 7           |
|    time_elapsed         | 357         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.008865831 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.5       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.277      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00684    |
|    std                  | 1.09        |
|    value_loss           | 0.000725    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1258        |
|    iterations           | 8           |
|    time_elapsed         | 416         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.008999504 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.7       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.28       |
|    n_updates            | 105         |
|    policy_gradient_loss | -0.00557    |
|    std                  | 1.1         |
|    value_loss           | 0.00038     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1246        |
|    iterations           | 9           |
|    time_elapsed         | 473         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.008929315 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.8       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.285      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00499    |
|    std                  | 1.12        |
|    value_loss           | 0.000134    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1239        |
|    iterations           | 10          |
|    time_elapsed         | 528         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.007775694 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -13.9       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.283      |
|    n_updates            | 135         |
|    policy_gradient_loss | -0.0029     |
|    std                  | 1.13        |
|    value_loss           | 0.000514    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 11          |
|    time_elapsed         | 583         |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.008627893 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14         |
|    explained_variance   | 0.009       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.274      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00211    |
|    std                  | 1.15        |
|    value_loss           | 0.000313    |
-----------------------------------------
Traceback (most recent call last):
  File "ppo_vanilla.py", line 211, in <module>
    if __name__ == '__main__':
  File "ppo_vanilla.py", line 204, in main
    task_name=args.task,
  File "ppo_vanilla.py", line 151, in train_policy
    callback=callback,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 270, in learn
    self.train()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 259, in train
    loss.backward()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
