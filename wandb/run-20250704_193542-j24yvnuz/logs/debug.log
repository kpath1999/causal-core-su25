2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Configure stats pid to 998857
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/.config/wandb/settings
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/causal-core-su25/wandb/settings
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'autocalc_curriculum.py', 'program_abspath': '/home/kpatherya3/causal-core-su25/autocalc_curriculum.py', 'program': 'autocalc_curriculum.py'}
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_setup.py:_flush():79] Applying login settings: {}
2025-07-04 19:35:42,669 INFO    MainThread:998857 [wandb_init.py:_log_setup():533] Logging user logs to /home/kpatherya3/causal-core-su25/wandb/run-20250704_193542-j24yvnuz/logs/debug.log
2025-07-04 19:35:42,670 INFO    MainThread:998857 [wandb_init.py:_log_setup():534] Logging internal logs to /home/kpatherya3/causal-core-su25/wandb/run-20250704_193542-j24yvnuz/logs/debug-internal.log
2025-07-04 19:35:42,670 INFO    MainThread:998857 [wandb_init.py:init():619] calling init triggers
2025-07-04 19:35:42,670 INFO    MainThread:998857 [wandb_init.py:init():627] wandb.init called with sweep_config: {}
config: {'task_name': 'reaching', 'curriculum_type': 'AutoCaLC', 'max_episode_length': 250, 'skip_frame': 3, 'seed': 0, 'total_timesteps': 2000000, 'gamma': 0.995, 'n_steps': 256, 'ent_coef': 0.02, 'learning_rate': 0.00025, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'gae_lambda': 0.97, 'batch_size': 512, 'n_epochs': 15, 'interventions': [{'type': 'goal'}, {'type': 'mass'}, {'type': 'friction'}, {'type': 'visual'}], 'divergence_threshold': 0.1, 'min_training_steps': 50000, 'max_training_steps': 200000, 'reference_states_size': 1000, 'patience': 3}
2025-07-04 19:35:42,670 INFO    MainThread:998857 [wandb_init.py:init():669] starting backend
2025-07-04 19:35:42,670 INFO    MainThread:998857 [wandb_init.py:init():673] sending inform_init request
2025-07-04 19:35:42,672 INFO    MainThread:998857 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-07-04 19:35:42,672 INFO    MainThread:998857 [wandb_init.py:init():686] backend started and connected
2025-07-04 19:35:42,675 INFO    MainThread:998857 [wandb_init.py:init():781] updated telemetry
2025-07-04 19:35:42,705 INFO    MainThread:998857 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2025-07-04 19:35:43,103 INFO    MainThread:998857 [wandb_init.py:init():867] starting run threads in backend
2025-07-04 19:35:43,252 INFO    MainThread:998857 [wandb_run.py:_console_start():2456] atexit reg
2025-07-04 19:35:43,253 INFO    MainThread:998857 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2025-07-04 19:35:43,253 INFO    MainThread:998857 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-07-04 19:35:43,253 INFO    MainThread:998857 [wandb_run.py:_redirect():2395] Redirects installed.
2025-07-04 19:35:43,256 INFO    MainThread:998857 [wandb_init.py:init():911] run started, returning control to user process
2025-07-04 19:35:51,243 INFO    MainThread:998857 [wandb_run.py:_tensorboard_callback():1544] tensorboard callback: autocalc_reaching/autocalc_curriculum_1, True
2025-07-04 19:35:51,259 INFO    MainThread:998857 [wandb_watch.py:_watch():71] Watching
2025-07-04 19:35:51,261 INFO    MainThread:998857 [wandb_run.py:_config_callback():1387] config_cb None None {'algo': 'PPO', 'policy_class': "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>", 'device': 'cuda', 'env': '<stable_baselines3.common.vec_env.vec_monitor.VecMonitor object at 0x7f06d75eb6d0>', '_vec_normalize_env': 'None', 'verbose': 1, 'policy_kwargs': "{'activation_fn': <class 'torch.nn.modules.activation.LeakyReLU'>, 'net_arch': [512, 256]}", 'observation_space': 'Box(37,)', 'action_space': 'Box(9,)', 'n_envs': 16, 'num_timesteps': 0, '_total_timesteps': 2000000, '_num_timesteps_at_start': 0, 'eval_env': 'None', 'action_noise': 'None', 'start_time': 1751672151.1737146, 'policy': 'ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (shared_net): Sequential(\n      (0): Linear(in_features=37, out_features=512, bias=True)\n      (1): LeakyReLU(negative_slope=0.01)\n      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): LeakyReLU(negative_slope=0.01)\n    )\n    (policy_net): Sequential()\n    (value_net): Sequential()\n  )\n  (action_net): Linear(in_features=256, out_features=9, bias=True)\n  (value_net): Linear(in_features=256, out_features=1, bias=True)\n)', 'tensorboard_log': 'autocalc_reaching', 'lr_schedule': '<function constant_fn.<locals>.func at 0x7f06cfd69830>', '_last_obs': '[[ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  0.          0.\n  -0.6         0.          0.         -0.48        0.          0.\n  -0.36      ]]', '_last_episode_starts': '[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': -0.009254400000000107, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 1155, '_logger': '<stable_baselines3.common.logger.Logger object at 0x7f06cfd60610>', '_custom_logger': 'False', 'rollout_buffer': '<stable_baselines3.common.buffers.RolloutBuffer object at 0x7f06cfdbb390>', 'clip_range': '<function constant_fn.<locals>.func at 0x7f06cfdbfd40>', 'clip_range_vf': 'None', 'target_kl': 'None'}
2025-07-04 20:11:22,472 INFO    MainThread:998857 [sb3.py:save_model():153] Saving model checkpoint to autocalc_reaching/wandb_models/model.zip
2025-07-04 20:11:23,737 INFO    MainThread:998857 [wandb_run.py:_finish():2155] finishing run kausarpatherya-georgia-institute-of-technology/autocalc-curriculum-reaching/j24yvnuz
2025-07-04 20:11:23,738 INFO    MainThread:998857 [wandb_run.py:_atexit_cleanup():2420] got exitcode: 0
2025-07-04 20:11:23,738 INFO    MainThread:998857 [wandb_run.py:_restore():2402] restore
2025-07-04 20:11:23,738 INFO    MainThread:998857 [wandb_run.py:_restore():2408] restore done
2025-07-04 20:11:26,612 INFO    MainThread:998857 [wandb_run.py:_footer_history_summary_info():3960] rendering history
2025-07-04 20:11:26,614 INFO    MainThread:998857 [wandb_run.py:_footer_history_summary_info():3992] rendering summary
2025-07-04 20:11:26,621 INFO    MainThread:998857 [wandb_run.py:_footer_sync_info():3921] logging synced files
