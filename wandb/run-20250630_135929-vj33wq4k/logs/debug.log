2025-06-30 13:59:29,913 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Configure stats pid to 3590897
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/.config/wandb/settings
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Loading settings from /home/kpatherya3/causal-core-su25/wandb/settings
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'ppo_vanilla.py', 'program_abspath': '/home/kpatherya3/causal-core-su25/ppo_vanilla.py', 'program': 'ppo_vanilla.py'}
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_setup.py:_flush():79] Applying login settings: {}
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:_log_setup():533] Logging user logs to /home/kpatherya3/causal-core-su25/wandb/run-20250630_135929-vj33wq4k/logs/debug.log
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:_log_setup():534] Logging internal logs to /home/kpatherya3/causal-core-su25/wandb/run-20250630_135929-vj33wq4k/logs/debug-internal.log
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:init():619] calling init triggers
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:init():627] wandb.init called with sweep_config: {}
config: {'task_name': 'picking', 'max_episode_length': 250, 'skip_frame': 3, 'seed': 0, 'total_timesteps': 5000000, 'gamma': 0.995, 'n_steps': 4096, 'ent_coef': 0.02, 'learning_rate': 0.00025, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'gae_lambda': 0.97, 'batch_size': 512, 'n_epochs': 15}
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:init():669] starting backend
2025-06-30 13:59:29,914 INFO    MainThread:3590897 [wandb_init.py:init():673] sending inform_init request
2025-06-30 13:59:29,916 INFO    MainThread:3590897 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-06-30 13:59:29,917 INFO    MainThread:3590897 [wandb_init.py:init():686] backend started and connected
2025-06-30 13:59:29,919 INFO    MainThread:3590897 [wandb_init.py:init():781] updated telemetry
2025-06-30 13:59:29,939 INFO    MainThread:3590897 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2025-06-30 13:59:30,196 INFO    MainThread:3590897 [wandb_init.py:init():867] starting run threads in backend
2025-06-30 13:59:30,270 INFO    MainThread:3590897 [wandb_run.py:_console_start():2456] atexit reg
2025-06-30 13:59:30,270 INFO    MainThread:3590897 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2025-06-30 13:59:30,270 INFO    MainThread:3590897 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-06-30 13:59:30,271 INFO    MainThread:3590897 [wandb_run.py:_redirect():2395] Redirects installed.
2025-06-30 13:59:30,273 INFO    MainThread:3590897 [wandb_init.py:init():911] run started, returning control to user process
2025-06-30 13:59:34,401 INFO    MainThread:3590897 [wandb_run.py:_tensorboard_callback():1544] tensorboard callback: ppo_picking_sb3/ppo_sb3_1, True
2025-06-30 13:59:34,410 INFO    MainThread:3590897 [wandb_watch.py:_watch():71] Watching
2025-06-30 13:59:34,411 INFO    MainThread:3590897 [wandb_run.py:_config_callback():1387] config_cb None None {'algo': 'PPO', 'policy_class': "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>", 'device': 'cuda', 'env': '<stable_baselines3.common.vec_env.vec_monitor.VecMonitor object at 0x7f7805e484d0>', '_vec_normalize_env': 'None', 'verbose': 1, 'policy_kwargs': "{'activation_fn': <class 'torch.nn.modules.activation.LeakyReLU'>, 'net_arch': [512, 256]}", 'observation_space': 'Box(56,)', 'action_space': 'Box(9,)', 'n_envs': 16, 'num_timesteps': 0, '_total_timesteps': 5000000, '_num_timesteps_at_start': 0, 'eval_env': 'None', 'action_noise': 'None', 'start_time': 1751306374.3923216, 'policy': 'ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (shared_net): Sequential(\n      (0): Linear(in_features=56, out_features=512, bias=True)\n      (1): LeakyReLU(negative_slope=0.01)\n      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): LeakyReLU(negative_slope=0.01)\n    )\n    (policy_net): Sequential()\n    (value_net): Sequential()\n  )\n  (action_net): Linear(in_features=256, out_features=9, bias=True)\n  (value_net): Linear(in_features=256, out_features=1, bias=True)\n)', 'tensorboard_log': 'ppo_picking_sb3', 'lr_schedule': '<function constant_fn.<locals>.func at 0x7f7805c82560>', '_last_obs': '[[ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]\n [ 1.          0.22178988  0.43350048 -0.26179939  0.22178988  0.43350048\n  -0.26179939  0.22178988  0.43350048 -0.26179939  0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.         -0.1189      0.31727417 -0.77654834  0.33421749 -0.05566666\n  -0.77654834 -0.21531749 -0.26160751 -0.77654834  1.          0.\n   0.          0.          0.          0.         -0.87        0.\n   0.          0.          0.1         0.          0.          0.\n   0.          0.          0.         20.          0.          0.\n   0.          0.          0.         -0.4         0.          0.\n   0.          0.1       ]]', '_last_episode_starts': '[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_logger': '<stable_baselines3.common.logger.Logger object at 0x7f7805cd5350>', '_custom_logger': 'False', 'rollout_buffer': '<stable_baselines3.common.buffers.RolloutBuffer object at 0x7f7805c6e890>', 'clip_range': '<function constant_fn.<locals>.func at 0x7f7804b74f80>', 'clip_range_vf': 'None', 'target_kl': 'None'}
2025-06-30 13:59:34,964 WARNING MsgRouterThr:3590897 [router.py:message_loop():75] message_loop has been closed
