
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 30
   student training steps: 50000
==================================================
2025-07-16 15:44:12,208 3218693 INFO Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
2025-07-16 15:44:12,838 3218693 INFO [EVAL EPISODE END] Episode 1 ended. Total reward: -0.7731337815882281, Length: 27, Success: True
episode 2: length=27, reward=-0.773, success=True
2025-07-16 15:44:12,934 3218693 INFO [EVAL EPISODE END] Episode 2 ended. Total reward: -0.7731337815882281, Length: 27, Success: True
episode 3: length=27, reward=-0.773, success=True
2025-07-16 15:44:13,029 3218693 INFO [EVAL EPISODE END] Episode 3 ended. Total reward: -0.7731337815882281, Length: 27, Success: True
2025-07-16 15:44:13,124 3218693 INFO [EVAL EPISODE END] Episode 4 ended. Total reward: -0.7731337815882281, Length: 27, Success: True
2025-07-16 15:44:13,218 3218693 INFO [EVAL EPISODE END] Episode 5 ended. Total reward: -0.7731337815882281, Length: 27, Success: True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
2025-07-16 15:44:13,219 3218693 INFO [EVAL SUMMARY] Success rate: 1.0, Avg reward: -0.7731337815882281, Avg length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
2025-07-16 15:44:14,253 3218693 INFO [EPISODE END] Episode ended. Reward: 0.004990433830231209, Info: {'desired_goal': array([[[-0.03246735,  0.01336478,  0.        ],
        [ 0.05866552,  0.10449766,  0.065     ]]]), 'achieved_goal': array([[[-3.54547453e-02,  6.46690227e-02, -1.29817795e-05],
        [ 4.79504678e-02,  1.48073871e-01,  6.49899008e-02]]]), 'success': False, 'fractional_success': 0.38559384191640494}
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
2025-07-16 15:44:15,250 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0018945536905243692, Info: {'desired_goal': array([[[-0.07526177, -0.00756685,  0.        ],
        [ 0.0004061 ,  0.06810102,  0.065     ]]]), 'achieved_goal': array([[[-4.37615297e-02,  4.96361206e-02, -1.29657614e-05],
        [ 4.47258284e-02,  1.38122483e-01,  6.49892242e-02]]]), 'success': False, 'fractional_success': 0.14241506298287965}
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
2025-07-16 15:44:16,258 3218693 INFO [EPISODE END] Episode ended. Reward: 0.001939084706684764, Info: {'desired_goal': array([[[-1.00122459e-01, -1.61834099e-02,  6.93889390e-18],
        [-1.14383566e-02,  7.25006921e-02,  6.50000000e-02]]]), 'achieved_goal': array([[[-4.25127385e-02,  4.52371050e-02, -1.49201641e-05],
        [ 4.65641915e-02,  1.34313261e-01,  6.49911802e-02]]]), 'success': False, 'fractional_success': 0.10770478927942052}
episode 3: 501 steps, reward: 1.888
2025-07-16 15:44:17,273 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0018013655851727602, Info: {'desired_goal': array([[[-9.67000227e-02,  2.42933163e-02, -6.93889390e-18],
        [-2.05001920e-02,  1.00493147e-01,  6.50000000e-02]]]), 'achieved_goal': array([[[-3.47295332e-02,  5.19421635e-02, -1.26230984e-05],
        [ 5.34007619e-02,  1.40071428e-01,  6.49894653e-02]]]), 'success': False, 'fractional_success': 0.11896094898053579}
2025-07-16 15:44:18,279 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0010125346101001102, Info: {'desired_goal': array([[[-0.03624416, -0.03452174,  0.        ],
        [ 0.05284012,  0.05456255,  0.065     ]]]), 'achieved_goal': array([[[-4.19637489e-02,  4.89247324e-02, -1.19681441e-05],
        [ 4.54530487e-02,  1.36342518e-01,  6.49894754e-02]]]), 'success': False, 'fractional_success': 0.058029040816418236}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:247: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4623', '1.2807', '1.2986', '1.2467', '1.3687']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.4949
action disagreement: 0.5153
total CM score: 1.7433
goal is complete. CM score: 1.7433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
2025-07-16 15:44:20,421 3218693 INFO [EPISODE END] Episode ended. Reward: 0.013547198055804916, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.10204021e-02,  3.77642253e-02, -1.71096107e-05],
        [ 5.73617138e-02,  1.26144712e-01,  6.49927262e-02]]]), 'success': True, 'fractional_success': 0.9771275979687465}
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
2025-07-16 15:44:21,446 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01220468124130855, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.29518285e-02,  2.20400314e-02, -1.17235523e-05],
        [ 5.84339420e-02,  1.13424913e-01,  6.49895854e-02]]]), 'success': True, 'fractional_success': 0.9998397757512346}
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
2025-07-16 15:44:22,482 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012398572446345823, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.94177697e-02,  2.56261227e-02, -1.09788568e-05],
        [ 6.12498452e-02,  1.16293738e-01,  6.49890211e-02]]]), 'success': True, 'fractional_success': 0.952420176898892}
episode 3: 501 steps, reward: 4.497
2025-07-16 15:44:23,497 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012480086784424803, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.05818868e-02,  3.70574236e-02, -1.31235030e-05],
        [ 5.73511643e-02,  1.24991504e-01,  6.49903797e-02]]]), 'success': True, 'fractional_success': 0.9703469290396}
2025-07-16 15:44:24,512 3218693 INFO [EPISODE END] Episode ended. Reward: 0.010657203146219894, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.50200149e-02,  4.87056081e-02, -1.33255349e-05],
        [ 5.22644424e-02,  1.25989357e-01,  6.49902250e-02]]]), 'success': False, 'fractional_success': 0.8683793010374722}
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6173', '1.5881', '1.5435', '1.7845', '1.4572']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.5085
action disagreement: 0.5139
total CM score: 1.5571
mass is complete. CM score: 1.5571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
2025-07-16 15:44:26,611 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012243056868615109, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.23506864e-02,  4.78774387e-02, -1.32351857e-05],
        [ 5.10708935e-02,  1.31301146e-01,  6.49897129e-02]]]), 'success': True, 'fractional_success': 0.9917524741120146}
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
2025-07-16 15:44:27,628 3218693 INFO [EPISODE END] Episode ended. Reward: 0.00981102768706365, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05303623e-02,  6.10269348e-02, -1.46511931e-05],
        [ 4.44892693e-02,  1.46046941e-01,  6.49909792e-02]]]), 'success': False, 'fractional_success': 0.7917834102932616}
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
2025-07-16 15:44:28,633 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009792955695350007, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.04434018e-02,  6.11798744e-02, -1.59532676e-05],
        [ 4.45807803e-02,  1.46207932e-01,  6.49900137e-02]]]), 'success': False, 'fractional_success': 0.78941909234027}
episode 3: 501 steps, reward: 3.374
2025-07-16 15:44:29,654 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012193266879062317, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.23945386e-02,  4.78769332e-02, -1.32333124e-05],
        [ 5.10857702e-02,  1.31359364e-01,  6.49897128e-02]]]), 'success': True, 'fractional_success': 0.9924308615364141}
2025-07-16 15:44:30,663 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012111204383714892, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.33873247e-02,  4.80817471e-02, -1.29725246e-05],
        [ 4.98681002e-02,  1.31336803e-01,  6.49899021e-02]]]), 'success': True, 'fractional_success': 0.9908960815196387}
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3495', '1.5457', '1.8414', '1.6155', '1.6662']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.4994
action disagreement: 0.5360
total CM score: 1.5964
friction is complete. CM score: 1.5964
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
2025-07-16 15:44:32,751 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009832248081066284, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05138637e-02,  6.10041649e-02, -1.54044112e-05],
        [ 4.45143337e-02,  1.46036315e-01,  6.49901000e-02]]]), 'success': False, 'fractional_success': 0.7921229528410417}
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
2025-07-16 15:44:33,768 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009832248081066284, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05138637e-02,  6.10041649e-02, -1.54044112e-05],
        [ 4.45143337e-02,  1.46036315e-01,  6.49901000e-02]]]), 'success': False, 'fractional_success': 0.7921229528410417}
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
2025-07-16 15:44:34,781 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009832248081066284, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05138637e-02,  6.10041649e-02, -1.54044112e-05],
        [ 4.45143337e-02,  1.46036315e-01,  6.49901000e-02]]]), 'success': False, 'fractional_success': 0.7921229528410417}
episode 3: 501 steps, reward: 3.394
2025-07-16 15:44:35,797 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009832248081066284, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05138637e-02,  6.10041649e-02, -1.54044112e-05],
        [ 4.45143337e-02,  1.46036315e-01,  6.49901000e-02]]]), 'success': False, 'fractional_success': 0.7921229528410417}
2025-07-16 15:44:36,812 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009832248081066284, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.05138637e-02,  6.10041649e-02, -1.54044112e-05],
        [ 4.45143337e-02,  1.46036315e-01,  6.49901000e-02]]]), 'success': False, 'fractional_success': 0.7921229528410417}
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.6746', '1.3558', '1.4321', '1.5538']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.4757
action disagreement: 0.5367
total CM score: 1.4193
visual is complete. CM score: 1.4193
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
2025-07-16 15:44:38,994 3218693 INFO [EPISODE END] Episode ended. Reward: 0.010568846460479822, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24768264e-02,  1.33642741e-02, -1.09967331e-05],
        [ 5.86538121e-02,  1.04494927e-01,  6.49890249e-02]]]), 'success': False, 'fractional_success': 0.8763844195649769}
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
2025-07-16 15:44:40,055 3218693 INFO [EPISODE END] Episode ended. Reward: 0.004582858169157157, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.52756999e-02, -7.57005075e-03, -1.39690742e-05],
        [ 3.53352149e-04,  6.80578091e-02,  6.49895589e-02]]]), 'success': False, 'fractional_success': 0.15983063945565198}
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
2025-07-16 15:44:41,029 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0015350612687977001, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.00101650e-01, -1.61821041e-02, -1.32397041e-05],
        [-1.14186901e-02,  7.25000438e-02,  6.49903927e-02]]]), 'success': False, 'fractional_success': 0.12472325997656067}
episode 3: 501 steps, reward: -1.746
2025-07-16 15:44:42,086 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0018027245233439002, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.66944541e-02,  2.42942073e-02, -1.09788568e-05],
        [-2.04921116e-02,  1.00496550e-01,  6.49890211e-02]]]), 'success': False, 'fractional_success': 0.15059625268780572}
2025-07-16 15:44:42,992 3218693 INFO [EPISODE END] Episode ended. Reward: 0.006846613459651868, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.46438064e-02,  6.37687374e-02, -1.58742126e-05],
        [ 1.69664944e-02,  1.55379658e-01,  6.49899847e-02]]]), 'success': False, 'fractional_success': 0.5704599542942543}
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7699', '1.7515', '1.6568', '1.7130', '1.9990']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5431
action disagreement: 0.5396
total CM score: 1.6187
pose is complete. CM score: 1.6187
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
2025-07-16 15:44:45,072 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0004043771032890602, Info: {'desired_goal': array([[[-0.03754987,  0.04276486,  0.        ],
        [ 0.03754987,  0.11723514,  0.0716524 ]]]), 'achieved_goal': array([[[ 4.99110302e-03, -3.87488486e-03, -1.65434059e-05],
        [ 7.65112847e-02,  6.68024142e-02,  7.16422839e-02]]]), 'success': False, 'fractional_success': 0.13991861033117314}
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
2025-07-16 15:44:46,108 3218693 INFO [EPISODE END] Episode ended. Reward: 0.00010098722502358549, Info: {'desired_goal': array([[[ 0.00321706, -0.13235515,  0.0033262 ],
        [ 0.08216464, -0.05340757,  0.0683262 ]]]), 'achieved_goal': array([[[-3.13931460e-02,  3.28567159e-02, -1.19659930e-05],
        [ 5.59945754e-02,  1.20245426e-01,  6.49894755e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
2025-07-16 15:44:47,077 3218693 INFO [EPISODE END] Episode ended. Reward: -0.004203141616903294, Info: {'desired_goal': array([[[-0.04068463, -0.0424988 ,  0.0033262 ],
        [ 0.04232706,  0.04051289,  0.0683262 ]]]), 'achieved_goal': array([[[ 4.06958441e-02,  2.33121454e-02, -2.47453761e-05],
        [ 1.30532657e-01,  1.13145665e-01,  6.49903489e-02]]]), 'success': False, 'fractional_success': 0.003862781793827264}
episode 3: 501 steps, reward: -0.050
2025-07-16 15:44:48,055 3218693 INFO [EPISODE END] Episode ended. Reward: 0.002182132497768967, Info: {'desired_goal': array([[[-0.04260181, -0.03139132,  0.0033262 ],
        [ 0.04860821,  0.05981869,  0.0683262 ]]]), 'achieved_goal': array([[[-4.89507329e-02, -1.21427174e-02, -1.44253121e-05],
        [ 4.19909732e-02,  7.87969749e-02,  6.49900549e-02]]]), 'success': False, 'fractional_success': 0.6941690322067346}
2025-07-16 15:44:49,118 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0005933448159593902, Info: {'desired_goal': array([[[-3.89573263e-02,  4.12792138e-02,  6.93889390e-18],
        [ 3.89573263e-02,  1.18720786e-01,  6.41228113e-02]]]), 'achieved_goal': array([[[-8.20558697e-02,  1.73466908e-02, -1.36597289e-05],
        [ 1.31553927e-03,  1.00974125e-01,  6.41120670e-02]]]), 'success': False, 'fractional_success': 0.398367374169516}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3205', '1.5355', '1.5504', '1.3586', '1.4045']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.5242
action disagreement: 0.5467
total CM score: 1.8817
random is complete. CM score: 1.8817
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
2025-07-16 15:44:51,298 3218693 INFO [EPISODE END] Episode ended. Reward: -0.06290760252663516, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.32175838e-02,  2.37272058e-02, -1.39444973e-05],
        [ 3.08665388e-02,  9.78132086e-02,  6.49905302e-02]]]), 'success': False, 'fractional_success': 0.7544874340560478}
Reset #2: visual intervention applied (success: True)
2025-07-16 15:44:52,381 3218693 INFO [EPISODE END] Episode ended. Reward: -0.015874013062405108, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.28662929e-02,  3.15811315e-02, -1.61756887e-05],
        [ 2.55657419e-02,  1.00017412e-01,  6.49899987e-02]]]), 'success': False, 'fractional_success': 0.7216552092942147}
Reset #3: visual intervention applied (success: True)
2025-07-16 15:44:53,449 3218693 INFO [EPISODE END] Episode ended. Reward: -0.07741936514815267, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.19812545e-02,  9.11927068e-02, -8.92007777e-05],
        [ 1.00622832e-02,  1.63218464e-01,  6.49363566e-02]]]), 'success': False, 'fractional_success': 0.21443765323209255}
2025-07-16 15:44:54,535 3218693 INFO [EPISODE END] Episode ended. Reward: 0.072749727542775, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24283959e-02,  2.97788041e-02, -1.29174206e-05],
        [ 5.92817145e-02,  1.21488283e-01,  6.49898302e-02]]]), 'success': True, 'fractional_success': 0.9987421129074376}
2025-07-16 15:44:55,615 3218693 INFO [EPISODE END] Episode ended. Reward: -0.017744887812956096, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.61127329e-02,  6.38328114e-02, -1.37144663e-05],
        [ 4.22489197e-02,  1.52196226e-01,  6.49895763e-02]]]), 'success': False, 'fractional_success': 0.7486059099081259}
2025-07-16 15:44:56,715 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0018374277243322062, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.55508678e-02,  1.60027835e-02, -1.91289373e-05],
        [ 3.03093783e-02,  9.18571218e-02,  6.49903656e-02]]]), 'success': False, 'fractional_success': 0.6593207784626692}
2025-07-16 15:44:57,799 3218693 INFO [EPISODE END] Episode ended. Reward: 0.017063603556154074, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.09865246e-02,  4.45586717e-02, -1.29316037e-05],
        [ 3.56958498e-02,  1.21243023e-01,  6.49898650e-02]]]), 'success': True, 'fractional_success': 0.9998440772615751}
2025-07-16 15:44:58,898 3218693 INFO [EPISODE END] Episode ended. Reward: 0.024251143572798933, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.31837928e-02,  8.52073600e-02, -1.21118591e-05],
        [ 6.77031303e-04,  1.59068091e-01,  6.49896770e-02]]]), 'success': False, 'fractional_success': 0.21428283217487099}
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 458       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
2025-07-16 15:45:10,924 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05867971678545801, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.66956427e-02,  4.73253059e-02, -1.26951123e-05],
        [ 3.01190324e-02,  1.24141511e-01,  6.49901757e-02]]]), 'success': True, 'fractional_success': 0.9632241225950766}
2025-07-16 15:45:12,018 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08206325721506869, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.35674283e-02,  3.17440043e-02, -1.64214354e-05],
        [ 3.81970919e-02,  1.23510758e-01,  6.49918594e-02]]]), 'success': True, 'fractional_success': 0.9998747594849263}
2025-07-16 15:45:13,113 3218693 INFO [EPISODE END] Episode ended. Reward: 0.021577104871878142, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.36790680e-02,  5.43214553e-02, -1.17257657e-05],
        [ 3.48184308e-02,  1.22818362e-01,  6.49891583e-02]]]), 'success': False, 'fractional_success': 0.8949052438115517}
2025-07-16 15:45:14,196 3218693 INFO [EPISODE END] Episode ended. Reward: -0.023413254691060644, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.63003158e-02,  3.40893164e-02, -1.20560786e-05],
        [ 2.09335250e-02,  1.11322440e-01,  6.49893062e-02]]]), 'success': False, 'fractional_success': 0.8070288450697}
2025-07-16 15:45:15,291 3218693 INFO [EPISODE END] Episode ended. Reward: -0.020609762673674672, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.43636837e-02,  6.54854816e-02, -1.30258987e-05],
        [ 7.30488924e-03,  1.57154590e-01,  6.49896579e-02]]]), 'success': False, 'fractional_success': 0.4428662578862154}
2025-07-16 15:45:16,382 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03485856234128804, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.03078885e-02,  2.50304479e-02, -1.16395354e-05],
        [ 2.67514040e-02,  9.20902205e-02,  6.49891431e-02]]]), 'success': False, 'fractional_success': 0.6252288459791611}
2025-07-16 15:45:17,475 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03609689578628978, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.74817251e-02,  4.42507590e-02, -1.78375675e-05],
        [ 2.70027441e-02,  1.18730451e-01,  6.49902498e-02]]]), 'success': True, 'fractional_success': 0.9152895148386354}
2025-07-16 15:45:18,574 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009583344975927109, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.30103132e-02,  2.01059437e-02, -1.20996279e-05],
        [ 3.70084740e-02,  1.10124497e-01,  6.49895092e-02]]]), 'success': True, 'fractional_success': 0.9632983061159189}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.033029757 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.535      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0954     |
|    std                  | 2.66        |
|    value_loss           | 0.0398      |
-----------------------------------------
2025-07-16 15:45:29,585 3218693 INFO [EPISODE END] Episode ended. Reward: -0.011051837820105812, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.24370143e-02,  5.95325258e-02, -1.21215544e-05],
        [ 3.33405345e-02,  1.35310784e-01,  6.49893282e-02]]]), 'success': False, 'fractional_success': 0.8147504301796598}
2025-07-16 15:45:30,674 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0872835272869507, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.40715948e-02,  4.80947085e-02, -1.38530751e-05],
        [ 2.21573608e-02,  1.24326522e-01,  6.49895575e-02]]]), 'success': False, 'fractional_success': 0.8330550812262134}
2025-07-16 15:45:31,754 3218693 INFO [EPISODE END] Episode ended. Reward: -0.05278630363318881, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.11068713e-02,  2.72725977e-02, -1.35720013e-05],
        [ 2.82402817e-02,  1.16619991e-01,  6.49895463e-02]]]), 'success': True, 'fractional_success': 0.9343155849645595}
2025-07-16 15:45:32,838 3218693 INFO [EPISODE END] Episode ended. Reward: -0.01690210220824183, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.19078811e-02,  2.95695331e-02, -1.69175667e-05],
        [ 3.30859888e-02,  1.14559179e-01,  6.49901316e-02]]]), 'success': True, 'fractional_success': 0.9998481780124271}
2025-07-16 15:45:33,932 3218693 INFO [EPISODE END] Episode ended. Reward: -0.012123328160716906, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.02638579e-02,  1.34576434e-02, -1.29393219e-05],
        [ 1.59433195e-02,  8.96639505e-02,  6.49898792e-02]]]), 'success': False, 'fractional_success': 0.4833712864916596}
2025-07-16 15:45:35,030 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0322056270389098, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.49769663e-02,  6.69043689e-03, -1.33915598e-05],
        [ 4.22375854e-02,  9.39032311e-02,  6.49895675e-02]]]), 'success': False, 'fractional_success': 0.7137812825506493}
2025-07-16 15:45:36,120 3218693 INFO [EPISODE END] Episode ended. Reward: -0.046563325530468984, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.51932377e-02,  1.80831158e-02, -1.31952632e-05],
        [ 3.45324954e-02,  1.07806684e-01,  6.49902364e-02]]]), 'success': True, 'fractional_success': 0.9276557826872853}
2025-07-16 15:45:37,201 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0578279545354973, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.21826377e-02,  8.53256726e-02, -1.25747863e-05],
        [ 2.46148878e-02,  1.62123821e-01,  6.49898224e-02]]]), 'success': False, 'fractional_success': 0.3672936433179517}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 3           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.035446957 |
|    clip_fraction        | 0.396       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.527      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0843     |
|    std                  | 2.66        |
|    value_loss           | 0.0269      |
-----------------------------------------
2025-07-16 15:45:49,092 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03651139685720944, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.82949883e-02,  6.52823290e-02, -4.84219602e-05],
        [ 2.44356754e-02,  1.38017065e-01,  6.49571839e-02]]]), 'success': False, 'fractional_success': 0.6358814531976915}
2025-07-16 15:45:50,171 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0007687717001636903, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.99102285e-02,  2.45614066e-02, -1.27010176e-05],
        [ 2.82614670e-02,  1.02734161e-01,  6.49894925e-02]]]), 'success': False, 'fractional_success': 0.7942168345253545}
2025-07-16 15:45:51,278 3218693 INFO [EPISODE END] Episode ended. Reward: -0.052153331279404476, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.68161310e-01, -4.25055893e-02, -1.16957235e-05],
        [ 2.39055712e-01,  2.83893489e-02,  6.49891763e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:45:52,358 3218693 INFO [EPISODE END] Episode ended. Reward: 0.11392696960242839, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.59847330e-02,  2.87924255e-02, -1.58179286e-05],
        [ 2.48401751e-02,  1.19617483e-01,  6.49900068e-02]]]), 'success': False, 'fractional_success': 0.8820209158594351}
2025-07-16 15:45:53,425 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0601634216147438, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.32560505e-02,  2.97170831e-02, -1.21028190e-05],
        [ 4.08040014e-02,  1.13777886e-01,  6.49897926e-02]]]), 'success': True, 'fractional_success': 0.9998429623342757}
2025-07-16 15:45:54,511 3218693 INFO [EPISODE END] Episode ended. Reward: 0.016081345559545025, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.17429374e-02,  2.93492748e-02, -1.23264169e-05],
        [ 6.02909206e-03,  1.17121650e-01,  6.49892616e-02]]]), 'success': False, 'fractional_success': 0.5926573354497572}
2025-07-16 15:45:55,583 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03674857685191399, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.96672697e-02,  5.50766879e-02, -1.37850183e-05],
        [ 3.17617370e-02,  1.36503230e-01,  6.49895618e-02]]]), 'success': False, 'fractional_success': 0.8732613462702924}
2025-07-16 15:45:56,672 3218693 INFO [EPISODE END] Episode ended. Reward: -0.05681892186615473, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.44121369e-02,  4.09750729e-02, -1.57131230e-05],
        [ 4.74801956e-02,  1.32868493e-01,  6.49900274e-02]]]), 'success': True, 'fractional_success': 0.9998465754112623}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 4           |
|    time_elapsed         | 67          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.035549812 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.536      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.091      |
|    std                  | 2.68        |
|    value_loss           | 0.0177      |
-----------------------------------------
2025-07-16 15:46:08,733 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01177616522771241, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.98969408e-02,  6.98142406e-02, -1.37025951e-05],
        [ 2.39495144e-02,  1.43663505e-01,  6.49895041e-02]]]), 'success': False, 'fractional_success': 0.5702251596862015}
2025-07-16 15:46:09,832 3218693 INFO [EPISODE END] Episode ended. Reward: -0.020788027129006213, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.03899799e-02,  5.23489749e-02, -1.44214451e-05],
        [ 1.13094048e-02,  1.44049707e-01,  6.49896416e-02]]]), 'success': False, 'fractional_success': 0.6236119916688029}
2025-07-16 15:46:10,943 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02638970234438973, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.00996210e-02,  3.52001798e-02, -1.29265599e-05],
        [ 1.85431273e-02,  1.23843744e-01,  6.49902313e-02]]]), 'success': False, 'fractional_success': 0.7851608636706467}
2025-07-16 15:46:12,031 3218693 INFO [EPISODE END] Episode ended. Reward: 0.016876924370407324, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.20014598e-02,  2.39514866e-02, -1.30704196e-05],
        [ 2.99158699e-02,  1.15868200e-01,  6.49896119e-02]]]), 'success': True, 'fractional_success': 0.9600906884572872}
2025-07-16 15:46:13,105 3218693 INFO [EPISODE END] Episode ended. Reward: 0.010135267960384513, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.28081989e-02,  2.80312998e-02, -1.63827843e-05],
        [ 5.01875337e-02,  1.11027849e-01,  6.49899720e-02]]]), 'success': True, 'fractional_success': 0.9772007462550264}
2025-07-16 15:46:14,181 3218693 INFO [EPISODE END] Episode ended. Reward: 0.06270175491205497, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.92403710e-02,  4.16850790e-02, -1.38945873e-05],
        [ 1.98620873e-02,  1.30785872e-01,  6.49891853e-02]]]), 'success': False, 'fractional_success': 0.8054365423439228}
2025-07-16 15:46:15,264 3218693 INFO [EPISODE END] Episode ended. Reward: 0.050796635089769324, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.48056853e-02,  5.52984002e-02, -1.35584766e-05],
        [ 3.64119248e-02,  1.46513423e-01,  6.49910087e-02]]]), 'success': False, 'fractional_success': 0.8799028804598883}
2025-07-16 15:46:16,388 3218693 INFO [EPISODE END] Episode ended. Reward: -0.010712418582195201, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.08238317e-02,  2.03242157e-03, -1.28716365e-05],
        [ 8.88265884e-03,  9.17392168e-02,  6.49898874e-02]]]), 'success': False, 'fractional_success': 0.4332429809390527}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 5           |
|    time_elapsed         | 87          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.041886266 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.478      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0682     |
|    std                  | 2.71        |
|    value_loss           | 0.0567      |
-----------------------------------------
2025-07-16 15:46:28,451 3218693 INFO [EPISODE END] Episode ended. Reward: -0.048410387697041354, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.01532727e-02,  3.07067577e-02, -1.20456556e-05],
        [ 2.14256779e-02,  1.12285121e-01,  6.49892227e-02]]]), 'success': False, 'fractional_success': 0.8267461140193695}
2025-07-16 15:46:29,540 3218693 INFO [EPISODE END] Episode ended. Reward: 0.005000231261751879, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.90862450e-02,  6.05456817e-02, -1.35368981e-05],
        [ 3.26267573e-02,  1.42260545e-01,  6.49897344e-02]]]), 'success': False, 'fractional_success': 0.7991709697099891}
2025-07-16 15:46:30,626 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08586833476935389, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.11082060e-02,  2.33003783e-02, -1.61881150e-05],
        [ 2.23288206e-02,  9.67320134e-02,  6.49900106e-02]]]), 'success': False, 'fractional_success': 0.6387972511717059}
2025-07-16 15:46:31,699 3218693 INFO [EPISODE END] Episode ended. Reward: 0.013398373715800788, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.12868731e-02,  5.59694622e-02, -1.57568708e-05],
        [ 3.93541119e-02,  1.26606543e-01,  6.49897370e-02]]]), 'success': False, 'fractional_success': 0.853334176434448}
2025-07-16 15:46:32,778 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03106574454209932, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.93466861e-02,  4.12375919e-02, -1.30896994e-05],
        [ 1.09153917e-02,  1.31499111e-01,  6.49901561e-02]]]), 'success': False, 'fractional_success': 0.6678279497608806}
2025-07-16 15:46:33,850 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04559311108559426, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.20871779e-02,  5.60598817e-02, -1.53595564e-05],
        [ 4.41766069e-02,  1.42324615e-01,  6.49909127e-02]]]), 'success': False, 'fractional_success': 0.868188118460042}
2025-07-16 15:46:34,942 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08398463883952115, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.26806883e-02,  3.34232742e-02, -1.33134824e-05],
        [ 5.92429727e-02,  1.25346999e-01,  6.49895289e-02]]]), 'success': True, 'fractional_success': 0.999838906236343}
2025-07-16 15:46:36,029 3218693 INFO [EPISODE END] Episode ended. Reward: 0.006073660825489107, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.44233429e-02,  4.66259328e-02, -1.19734379e-05],
        [ 3.21930591e-02,  1.33242345e-01,  6.49894596e-02]]]), 'success': True, 'fractional_success': 0.9951164386080568}
2025-07-16 15:46:37,139 3218693 INFO [EPISODE END] Episode ended. Reward: 0.04042378771726644, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.84603238e-02,  1.60352817e-02, -1.28181496e-05],
        [ 1.66575555e-03,  9.61594842e-02,  6.49895451e-02]]]), 'success': False, 'fractional_success': 0.39342500376007866}
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 6          |
|    time_elapsed         | 107        |
|    total_timesteps      | 5070848    |
| train/                  |            |
|    approx_kl            | 0.46430597 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.541     |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 2.73       |
|    value_loss           | 0.0205     |
----------------------------------------
2025-07-16 15:46:49,049 3218693 INFO [EPISODE END] Episode ended. Reward: -0.012233467415436149, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.98556049e-02,  6.07181998e-02, -1.61123709e-05],
        [ 4.86066313e-02,  1.39175535e-01,  6.49900093e-02]]]), 'success': False, 'fractional_success': 0.7641157840722448}
2025-07-16 15:46:50,169 3218693 INFO [EPISODE END] Episode ended. Reward: -0.017714090411020587, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.14910837e-02,  2.73292367e-02, -1.21839816e-05],
        [ 1.19796410e-02,  1.00799277e-01,  6.49893270e-02]]]), 'success': False, 'fractional_success': 0.5610280292462062}
2025-07-16 15:46:51,243 3218693 INFO [EPISODE END] Episode ended. Reward: 0.005687453402430273, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.49981782e-02,  6.13082733e-02, -1.27574157e-05],
        [ 1.16478302e-02,  1.27952511e-01,  6.49891362e-02]]]), 'success': False, 'fractional_success': 0.5348227053486533}
2025-07-16 15:46:52,307 3218693 INFO [EPISODE END] Episode ended. Reward: 0.07268795122799719, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.85415380e-02,  4.72907634e-02, -1.28060050e-05],
        [ 6.15890405e-02,  1.37420134e-01,  6.49895935e-02]]]), 'success': True, 'fractional_success': 0.9389502345243477}
2025-07-16 15:46:53,396 3218693 INFO [EPISODE END] Episode ended. Reward: 0.07966768250700589, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.59611672e-02,  2.88017704e-03, -1.53922634e-05],
        [ 1.34656547e-02,  9.23097117e-02,  6.49900180e-02]]]), 'success': False, 'fractional_success': 0.4874299239680301}
2025-07-16 15:46:54,485 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08377590934651977, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.15644658e-02,  7.10035548e-02, -1.18649018e-05],
        [ 3.02724200e-02,  1.42840924e-01,  6.49894024e-02]]]), 'success': False, 'fractional_success': 0.6164278337099248}
2025-07-16 15:46:55,588 3218693 INFO [EPISODE END] Episode ended. Reward: 0.036418888391066384, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.67020143e-02,  4.44488563e-02, -1.31185146e-05],
        [ 4.52201206e-02,  1.36369780e-01,  6.49902084e-02]]]), 'success': True, 'fractional_success': 0.9998493592501241}
2025-07-16 15:46:56,673 3218693 INFO [EPISODE END] Episode ended. Reward: -0.10595410912712849, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.85140077e-02,  4.03899632e-02, -1.19629086e-05],
        [ 3.98038601e-02,  1.28706879e-01,  6.49894825e-02]]]), 'success': True, 'fractional_success': 0.9998381922624787}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 7           |
|    time_elapsed         | 126         |
|    total_timesteps      | 5074944     |
| train/                  |             |
|    approx_kl            | 0.038512163 |
|    clip_fraction        | 0.453       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.54       |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.0966     |
|    std                  | 2.75        |
|    value_loss           | 0.023       |
-----------------------------------------
2025-07-16 15:47:07,479 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03990783417599808, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.63174217e-01,  7.36493730e-02, -1.36656461e-05],
        [-8.17202792e-02,  1.55101379e-01,  6.49896891e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:47:08,559 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03760874243189541, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.44340589e-02,  5.01842032e-02, -1.74684479e-05],
        [ 4.13303684e-02,  1.25947674e-01,  6.49899074e-02]]]), 'success': True, 'fractional_success': 0.9585557079568295}
2025-07-16 15:47:09,637 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03427247761774863, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.53925066e-02,  1.38610458e-02, -1.34233435e-05],
        [ 4.65196943e-02,  1.05773873e-01,  6.49896085e-02]]]), 'success': False, 'fractional_success': 0.8963777902806952}
2025-07-16 15:47:10,720 3218693 INFO [EPISODE END] Episode ended. Reward: -0.020121151653763967, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.29026315e-02,  5.01615303e-02, -1.60426305e-05],
        [ 2.67087327e-02,  1.39776710e-01,  6.49897958e-02]]]), 'success': False, 'fractional_success': 0.8734680108108529}
2025-07-16 15:47:11,788 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08000049593628003, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.70944411e-02,  7.96481414e-02, -1.35410512e-05],
        [ 4.45011924e-02,  1.61245586e-01,  6.49897090e-02]]]), 'success': False, 'fractional_success': 0.5053331916072501}
2025-07-16 15:47:12,850 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03195803271232676, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.76136132e-02,  1.32870562e-02, -1.35299061e-05],
        [ 4.95628035e-02,  1.00462897e-01,  6.49895316e-02]]]), 'success': False, 'fractional_success': 0.8146825650292584}
2025-07-16 15:47:13,938 3218693 INFO [EPISODE END] Episode ended. Reward: 0.1157787410919514, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.76415635e-02,  7.26077129e-03, -1.35360194e-05],
        [ 4.42837287e-02,  9.91867418e-02,  6.49896210e-02]]]), 'success': False, 'fractional_success': 0.7950536711395562}
2025-07-16 15:47:15,007 3218693 INFO [EPISODE END] Episode ended. Reward: 0.015193352174364195, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.98639651e-02,  5.00367237e-02, -2.62318643e-05],
        [ 4.17202257e-02,  1.31615747e-01,  6.49838996e-02]]]), 'success': True, 'fractional_success': 0.96073544958834}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 8           |
|    time_elapsed         | 145         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.038204994 |
|    clip_fraction        | 0.439       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.539      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0913     |
|    std                  | 2.77        |
|    value_loss           | 0.0357      |
-----------------------------------------
2025-07-16 15:47:27,131 3218693 INFO [EPISODE END] Episode ended. Reward: -0.019083489070087722, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.04732448,  0.05268111, -0.00205657],
        [ 0.02374342,  0.12425972,  0.06397234]]]), 'success': False, 'fractional_success': 0.7837224277972136}
2025-07-16 15:47:28,213 3218693 INFO [EPISODE END] Episode ended. Reward: 0.051514917049987466, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.56259554e-02,  2.17565376e-02, -1.20628391e-05],
        [ 3.88232055e-02,  1.06204934e-01,  6.49890526e-02]]]), 'success': True, 'fractional_success': 0.9030007143891724}
2025-07-16 15:47:29,280 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0416177650265214, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.47819949e-02,  5.76923933e-02, -1.31318450e-05],
        [ 4.49339682e-02,  1.47406524e-01,  6.49900270e-02]]]), 'success': False, 'fractional_success': 0.8430645771922587}
2025-07-16 15:47:30,349 3218693 INFO [EPISODE END] Episode ended. Reward: -0.06777879002999275, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.89000315e-02,  2.62182730e-02, -1.25659220e-05],
        [ 4.25832191e-02,  1.07703073e-01,  6.49901340e-02]]]), 'success': True, 'fractional_success': 0.9260605400973879}
2025-07-16 15:47:31,421 3218693 INFO [EPISODE END] Episode ended. Reward: -0.028405483304773424, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.67245209e-02,  2.15927027e-02, -1.30181611e-05],
        [ 4.25167608e-02,  1.10831928e-01,  6.49901728e-02]]]), 'success': True, 'fractional_success': 0.9741900532306382}
2025-07-16 15:47:32,477 3218693 INFO [EPISODE END] Episode ended. Reward: 0.09050065828382603, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.17580037e-02,  4.99281234e-02, -1.45512315e-05],
        [ 4.64005660e-02,  1.18087742e-01,  6.49894286e-02]]]), 'success': False, 'fractional_success': 0.8034255514280446}
2025-07-16 15:47:33,582 3218693 INFO [EPISODE END] Episode ended. Reward: -0.020515193053297562, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.18331218e-02,  3.63329390e-02, -1.62128905e-05],
        [ 2.45419499e-02,  1.02712768e-01,  6.49900003e-02]]]), 'success': False, 'fractional_success': 0.7453158419909005}
2025-07-16 15:47:34,649 3218693 INFO [EPISODE END] Episode ended. Reward: -0.036239118178334945, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.54527040e-02,  7.68378938e-02, -1.38284366e-05],
        [ 3.71417369e-02,  1.59429938e-01,  6.49895595e-02]]]), 'success': False, 'fractional_success': 0.5485596619304299}
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 9          |
|    time_elapsed         | 165        |
|    total_timesteps      | 5083136    |
| train/                  |            |
|    approx_kl            | 0.04584127 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.528     |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.0931    |
|    std                  | 2.81       |
|    value_loss           | 0.0415     |
----------------------------------------
2025-07-16 15:47:46,743 3218693 INFO [EPISODE END] Episode ended. Reward: 0.06944802022231387, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.13587656e-02,  7.74193937e-02, -1.32732025e-05],
        [ 2.35225967e-02,  1.62298782e-01,  6.49899812e-02]]]), 'success': False, 'fractional_success': 0.4650896413664711}
2025-07-16 15:47:47,845 3218693 INFO [EPISODE END] Episode ended. Reward: -0.018438034318123308, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.63437034e-02,  6.33047130e-02, -1.20640115e-05],
        [ 6.44291769e-03,  1.36090607e-01,  6.49893530e-02]]]), 'success': False, 'fractional_success': 0.45337140890310135}
2025-07-16 15:47:48,930 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05577690513412855, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.35394100e-02,  4.41085660e-02, -1.20574642e-05],
        [ 3.54237553e-02,  1.23072860e-01,  6.49894401e-02]]]), 'success': True, 'fractional_success': 0.9998375397151481}
2025-07-16 15:47:50,007 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0006670438822633062, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.69510375e-02,  5.67122784e-02, -1.41196798e-05],
        [-5.83729966e-03,  1.37826639e-01,  6.49902765e-02]]]), 'success': False, 'fractional_success': 0.35200681504599457}
2025-07-16 15:47:51,085 3218693 INFO [EPISODE END] Episode ended. Reward: -0.05127364766337623, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.56208893e-02,  7.39287633e-02, -1.31087104e-05],
        [ 2.24151328e-02,  1.61964861e-01,  6.49899394e-02]]]), 'success': False, 'fractional_success': 0.501258399717915}
2025-07-16 15:47:52,152 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08711556780230781, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.25851890e-02,  2.62478085e-02, -1.33914717e-05],
        [ 4.90716744e-02,  1.17904785e-01,  6.49898930e-02]]]), 'success': True, 'fractional_success': 0.9998445076707531}
2025-07-16 15:47:53,222 3218693 INFO [EPISODE END] Episode ended. Reward: -0.055325709625504524, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.83299343e-02,  2.99827073e-02, -1.59239623e-05],
        [ 4.18353805e-02,  1.20150665e-01,  6.49900297e-02]]]), 'success': True, 'fractional_success': 0.9998466105678736}
2025-07-16 15:47:54,291 3218693 INFO [EPISODE END] Episode ended. Reward: -0.012200812697060612, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.03370692e-02,  2.56007208e-02, -1.31383982e-05],
        [ 4.85126768e-02,  1.14448614e-01,  6.49899351e-02]]]), 'success': True, 'fractional_success': 0.9998451554130344}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 10          |
|    time_elapsed         | 184         |
|    total_timesteps      | 5087232     |
| train/                  |             |
|    approx_kl            | 0.041341044 |
|    clip_fraction        | 0.452       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.556      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.1        |
|    std                  | 2.83        |
|    value_loss           | 0.0267      |
-----------------------------------------
2025-07-16 15:48:06,316 3218693 INFO [EPISODE END] Episode ended. Reward: -0.10660470208043829, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.82090839e-02,  3.36835302e-02, -1.26235546e-05],
        [ 4.11596188e-02,  1.13051108e-01,  6.49894740e-02]]]), 'success': True, 'fractional_success': 0.9998380610951606}
2025-07-16 15:48:07,400 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02862120244251864, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.07936737e-02,  2.88218839e-02, -1.36127585e-05],
        [ 3.05689433e-02,  1.20184876e-01,  6.49896843e-02]]]), 'success': True, 'fractional_success': 0.9701374473012814}
2025-07-16 15:48:08,465 3218693 INFO [EPISODE END] Episode ended. Reward: 0.06628120710477049, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.91362510e-02,  5.91545275e-02, -1.19023445e-05],
        [ 5.04372701e-02,  1.28728720e-01,  6.49892425e-02]]]), 'success': False, 'fractional_success': 0.6518590112942327}
2025-07-16 15:48:09,541 3218693 INFO [EPISODE END] Episode ended. Reward: 0.028644327440542773, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.92966640e-02,  4.80741106e-02, -1.42767274e-05],
        [ 2.27313326e-02,  1.20099428e-01,  6.49894238e-02]]]), 'success': False, 'fractional_success': 0.8420707103026221}
2025-07-16 15:48:10,602 3218693 INFO [EPISODE END] Episode ended. Reward: -0.07171051754969834, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.06227658,  0.05160634, -0.00034858],
        [ 0.03394984,  0.14879371,  0.07233249]]]), 'success': True, 'fractional_success': 0.9368255669303109}
2025-07-16 15:48:11,667 3218693 INFO [EPISODE END] Episode ended. Reward: 0.042020025572291175, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.38594489e-02,  3.96824168e-02, -1.19067833e-05],
        [ 4.74052082e-02,  1.20946900e-01,  6.49894577e-02]]]), 'success': True, 'fractional_success': 0.9998378109108247}
2025-07-16 15:48:12,738 3218693 INFO [EPISODE END] Episode ended. Reward: -0.021931992727456807, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.30876427e-02,  6.87011566e-02, -1.36091304e-05],
        [ 8.59726489e-03,  1.60385526e-01,  6.49896645e-02]]]), 'success': False, 'fractional_success': 0.4259707584087056}
2025-07-16 15:48:13,801 3218693 INFO [EPISODE END] Episode ended. Reward: 0.07400734843381129, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.92217058e-02,  3.88054058e-02, -1.30692325e-05],
        [ 5.25299179e-02,  1.30555873e-01,  6.49899137e-02]]]), 'success': True, 'fractional_success': 0.9998448255508464}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 11          |
|    time_elapsed         | 204         |
|    total_timesteps      | 5091328     |
| train/                  |             |
|    approx_kl            | 0.046489157 |
|    clip_fraction        | 0.513       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.543      |
|    n_updates            | 1305        |
|    policy_gradient_loss | -0.0971     |
|    std                  | 2.85        |
|    value_loss           | 0.0291      |
-----------------------------------------
2025-07-16 15:48:25,965 3218693 INFO [EPISODE END] Episode ended. Reward: 0.07187621020369067, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.51861396e-02,  4.91084433e-02, -1.65277978e-05],
        [ 1.84899878e-02,  1.32786403e-01,  6.49897777e-02]]]), 'success': False, 'fractional_success': 0.7649293184302374}
2025-07-16 15:48:27,040 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08110749186298238, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.86409898e-02,  1.68394466e-02, -1.26511603e-05],
        [ 5.32414610e-02,  1.08721021e-01,  6.49897965e-02]]]), 'success': True, 'fractional_success': 0.9417140153484096}
2025-07-16 15:48:28,127 3218693 INFO [EPISODE END] Episode ended. Reward: -0.08285514845440167, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.71191564e-02,  2.02126368e-02, -1.22910727e-05],
        [ 3.11812184e-02,  1.08513959e-01,  6.49898819e-02]]]), 'success': True, 'fractional_success': 0.9194883860657616}
2025-07-16 15:48:29,188 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02336495452335723, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.85714651e-02,  6.40332339e-02, -1.35867232e-05],
        [ 2.31745062e-02,  1.45780468e-01,  6.49894945e-02]]]), 'success': False, 'fractional_success': 0.6385626390045628}
2025-07-16 15:48:30,254 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03136501862402466, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.71938824e-02,  2.70643604e-02, -1.18904199e-05],
        [ 3.43771523e-02,  1.08636372e-01,  6.49894470e-02]]]), 'success': True, 'fractional_success': 0.9404068735273915}
2025-07-16 15:48:31,331 3218693 INFO [EPISODE END] Episode ended. Reward: -0.01269245795425543, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.20436855e-02,  4.67128209e-02, -1.17157092e-05],
        [ 5.11829412e-02,  1.19938933e-01,  6.49891629e-02]]]), 'success': False, 'fractional_success': 0.8389937193344859}
2025-07-16 15:48:32,413 3218693 INFO [EPISODE END] Episode ended. Reward: 0.004552975860104052, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.61914271e-02,  3.66400342e-02, -1.34963176e-05],
        [ 3.17232697e-02,  1.24553002e-01,  6.49895714e-02]]]), 'success': True, 'fractional_success': 0.9878917791413255}
2025-07-16 15:48:33,477 3218693 INFO [EPISODE END] Episode ended. Reward: 0.036784145018765, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.06888996e-02,  6.37461418e-02, -1.29365828e-05],
        [ 3.20703548e-02,  1.36506469e-01,  6.49898732e-02]]]), 'success': False, 'fractional_success': 0.7449854354202042}
2025-07-16 15:48:34,546 3218693 INFO [EPISODE END] Episode ended. Reward: -0.10382924780540931, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.72901583e-02,  5.82716335e-02, -1.33652077e-05],
        [ 2.61655797e-02,  1.41725251e-01,  6.49896741e-02]]]), 'success': False, 'fractional_success': 0.7528599221643053}
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 12         |
|    time_elapsed         | 224        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.04609365 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.561     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.87       |
|    value_loss           | 0.0208     |
----------------------------------------
2025-07-16 15:48:46,600 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03392194648866365, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.14194273e-02,  3.30968814e-02, -1.26392259e-05],
        [ 3.95166568e-02,  1.24034180e-01,  6.49896681e-02]]]), 'success': True, 'fractional_success': 0.9998410469441403}
2025-07-16 15:48:47,672 3218693 INFO [EPISODE END] Episode ended. Reward: -0.021196429328284885, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.55251879e-02,  3.54287147e-02, -1.65535707e-05],
        [ 5.60928841e-02,  1.07044047e-01,  6.49882619e-02]]]), 'success': False, 'fractional_success': 0.6767094973982808}
2025-07-16 15:48:48,746 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03332144434337967, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.26507650e-02,  4.21287756e-02, -2.81403759e-05],
        [ 4.47955626e-02,  1.19585205e-01,  6.49893374e-02]]]), 'success': True, 'fractional_success': 0.9998359597133605}
2025-07-16 15:48:49,855 3218693 INFO [EPISODE END] Episode ended. Reward: -0.08080350443042865, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.13108761e-02,  1.52858085e-02, -1.21500930e-05],
        [ 1.43372000e-03,  8.80297157e-02,  6.49893546e-02]]]), 'success': False, 'fractional_success': 0.3254671666865909}
2025-07-16 15:48:50,933 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04194053367477442, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.02654026e-02,  7.35999511e-02, -1.61162979e-05],
        [ 1.62433412e-02,  1.40104032e-01,  6.49900045e-02]]]), 'success': False, 'fractional_success': 0.44871639674767133}
2025-07-16 15:48:52,026 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03250600458711506, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.73911997e-02,  4.52703616e-02, -1.38128422e-05],
        [ 2.20773996e-02,  1.24741583e-01,  6.49895599e-02]]]), 'success': False, 'fractional_success': 0.8395174395519883}
2025-07-16 15:48:53,160 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08981267328337704, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.41808008e-01, -1.34953674e-01, -1.47164545e-05],
        [ 2.28889040e-01, -4.78733605e-02,  6.49903467e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:48:54,216 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03309450723339109, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.02666694,  0.03660934, -0.00016151],
        [ 0.06956028,  0.12847378,  0.07477292]]]), 'success': True, 'fractional_success': 0.9102606721808872}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 13          |
|    time_elapsed         | 244         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.052456357 |
|    clip_fraction        | 0.523       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.3       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.563      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.103      |
|    std                  | 2.89        |
|    value_loss           | 0.0206      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_0.zip

evaluating student performance (5 episodes)...
episode 1: length=28, reward=0.201, success=True
2025-07-16 15:49:05,638 3218693 INFO [EVAL EPISODE END] Episode 1 ended. Total reward: 0.20070304391478416, Length: 28, Success: True
episode 2: length=28, reward=0.201, success=True
2025-07-16 15:49:05,734 3218693 INFO [EVAL EPISODE END] Episode 2 ended. Total reward: 0.20070304391478416, Length: 28, Success: True
episode 3: length=28, reward=0.201, success=True
2025-07-16 15:49:05,829 3218693 INFO [EVAL EPISODE END] Episode 3 ended. Total reward: 0.20070304391478416, Length: 28, Success: True
2025-07-16 15:49:05,923 3218693 INFO [EVAL EPISODE END] Episode 4 ended. Total reward: 0.20070304391478416, Length: 28, Success: True
2025-07-16 15:49:06,018 3218693 INFO [EVAL EPISODE END] Episode 5 ended. Total reward: 0.20070304391478416, Length: 28, Success: True
performance summary:
success rate: 1.000 (5/5)
average reward: 0.201
average episode length: 28.0
2025-07-16 15:49:06,019 3218693 INFO [EVAL SUMMARY] Success rate: 1.0, Avg reward: 0.20070304391478416, Avg length: 28.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
2025-07-16 15:49:07,073 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0023306343573782154, Info: {'desired_goal': array([[[-0.03246735,  0.01336478,  0.        ],
        [ 0.05866552,  0.10449766,  0.065     ]]]), 'achieved_goal': array([[[-6.63755881e-02,  7.35739859e-02, -1.35444287e-05],
        [ 2.10499108e-02,  1.60998936e-01,  6.49895320e-02]]]), 'success': False, 'fractional_success': 0.19923463590162638}
episode 1: 501 steps, reward: 0.546
Reset #2: goal intervention applied (success: True)
2025-07-16 15:49:08,006 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03423314229584739, Info: {'desired_goal': array([[[-0.07526177, -0.00756685,  0.        ],
        [ 0.0004061 ,  0.06810102,  0.065     ]]]), 'achieved_goal': array([[[-4.42760947e-02,  3.38377790e-02, -1.19132688e-05],
        [ 4.33082988e-02,  1.21422426e-01,  6.49891795e-02]]]), 'success': False, 'fractional_success': 0.2673422352856907}
episode 2: 501 steps, reward: 1.251
Reset #3: goal intervention applied (success: True)
2025-07-16 15:49:08,977 3218693 INFO [EPISODE END] Episode ended. Reward: -0.010648180261120153, Info: {'desired_goal': array([[[-1.00122459e-01, -1.61834099e-02,  6.93889390e-18],
        [-1.14383566e-02,  7.25006921e-02,  6.50000000e-02]]]), 'achieved_goal': array([[[-4.46348651e-02,  6.74208545e-02, -1.47584090e-05],
        [ 4.59533506e-02,  1.58006787e-01,  6.49900832e-02]]]), 'success': False, 'fractional_success': 0.021438008099876945}
episode 3: 501 steps, reward: 1.432
2025-07-16 15:49:09,928 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0022063043644349636, Info: {'desired_goal': array([[[-9.67000227e-02,  2.42933163e-02, -6.93889390e-18],
        [-2.05001920e-02,  1.00493147e-01,  6.50000000e-02]]]), 'achieved_goal': array([[[-4.36498014e-02,  5.48324942e-02, -1.57591513e-05],
        [ 4.01751084e-02,  1.38655597e-01,  6.49899640e-02]]]), 'success': False, 'fractional_success': 0.18201648026185843}
2025-07-16 15:49:10,977 3218693 INFO [EPISODE END] Episode ended. Reward: -0.004517012030429978, Info: {'desired_goal': array([[[-0.03624416, -0.03452174,  0.        ],
        [ 0.05284012,  0.05456255,  0.065     ]]]), 'achieved_goal': array([[[-4.53302524e-02,  2.07800941e-02, -1.10693937e-05],
        [ 4.65026911e-02,  1.12613077e-01,  6.49890686e-02]]]), 'success': False, 'fractional_success': 0.3521822316968322}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.668
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6600', '8.3693', '7.3452', '7.7547', '8.1663']
Training reward models...
Reward model losses: ['0.0050', '0.2329', '0.0286', '1.3615', '1.7798']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6447', '1.6117', '1.4343', '1.3699', '1.4455']
Training action VAE models...
Action VAE losses: ['1.3829', '1.3433', '1.4357', '1.4565', '1.4456']
CM score components:
transition disagreement: 0.4419
reward disagreement: 0.4586
state disagreement: 0.5240
action disagreement: 0.5415
total CM score: 1.9659
goal is complete. CM score: 1.9659
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
2025-07-16 15:49:13,189 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01194395344464418, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.13063836e-02,  3.42766663e-02, -1.09788635e-05],
        [ 3.21021802e-02,  1.17685230e-01,  6.49890211e-02]]]), 'success': True, 'fractional_success': 0.9937118229863028}
episode 1: 501 steps, reward: 5.938
Reset #2: mass intervention applied (success: True)
2025-07-16 15:49:14,320 3218693 INFO [EPISODE END] Episode ended. Reward: 0.011947331841664424, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.37955663e-02,  3.02924486e-02, -1.09801740e-05],
        [ 4.07201474e-02,  1.14808161e-01,  6.49890218e-02]]]), 'success': True, 'fractional_success': 0.9998311043081318}
episode 2: 501 steps, reward: 6.075
Reset #3: mass intervention applied (success: True)
2025-07-16 15:49:15,465 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01781056298767828, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.21950028e-02,  3.08132249e-02, -1.11547548e-05],
        [ 3.45135008e-02,  1.17521812e-01,  6.49890239e-02]]]), 'success': True, 'fractional_success': 0.9998311367956371}
episode 3: 501 steps, reward: 5.977
2025-07-16 15:49:16,454 3218693 INFO [EPISODE END] Episode ended. Reward: 0.011997505899384881, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.20801954e-02,  3.25891532e-02, -1.09789254e-05],
        [ 3.38593900e-02,  1.18528739e-01,  6.49890212e-02]]]), 'success': True, 'fractional_success': 0.9998310954777089}
2025-07-16 15:49:17,570 3218693 INFO [EPISODE END] Episode ended. Reward: 0.027237683511630785, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.50092566e-02,  3.06969576e-02, -1.09788580e-05],
        [ 3.91504224e-02,  1.14856637e-01,  6.49890211e-02]]]), 'success': True, 'fractional_success': 0.9998310945152344}
total data points collected: 2505
average episode length: 501.0
average episode reward: 6.119
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6791', '7.3496', '7.7814', '7.5427', '6.8157']
Training reward models...
Reward model losses: ['0.6807', '0.4189', '0.0087', '0.1139', '0.2720']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7148', '1.4406', '1.6725', '1.6333', '1.9517']
Training action VAE models...
Action VAE losses: ['1.3630', '1.3957', '1.2601', '1.3735', '1.4298']
CM score components:
transition disagreement: 0.4359
reward disagreement: 0.3343
state disagreement: 0.5148
action disagreement: 0.5221
total CM score: 1.8071
mass is complete. CM score: 1.8071
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
2025-07-16 15:49:19,628 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009187464851867423, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.78972665e-02,  5.99705196e-02, -1.19623076e-05],
        [ 2.93673656e-02,  1.47234163e-01,  6.49894727e-02]]]), 'success': False, 'fractional_success': 0.7690731885914757}
episode 1: 501 steps, reward: 4.318
Reset #2: friction intervention applied (success: True)
2025-07-16 15:49:20,636 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009506365339448, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.89251329e-02,  5.86535443e-02, -1.37062524e-05],
        [ 3.02708927e-02,  1.47851155e-01,  6.49895414e-02]]]), 'success': False, 'fractional_success': 0.7998689341353139}
episode 2: 501 steps, reward: 3.134
Reset #3: friction intervention applied (success: True)
2025-07-16 15:49:21,648 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009404182257032769, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.79952151e-02,  5.91289924e-02, -1.35230432e-05],
        [ 3.01660639e-02,  1.47289803e-01,  6.49895068e-02]]]), 'success': False, 'fractional_success': 0.7914819060914438}
episode 3: 501 steps, reward: 3.139
2025-07-16 15:49:22,619 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009218356077654961, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.79062162e-02,  5.99699196e-02, -1.19624427e-05],
        [ 2.93647744e-02,  1.47239921e-01,  6.49894727e-02]]]), 'success': False, 'fractional_success': 0.769049762342571}
2025-07-16 15:49:23,627 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009427471516556781, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.92927391e-02,  5.88877447e-02, -1.57224731e-05],
        [ 3.02204622e-02,  1.48401372e-01,  6.49899530e-02]]]), 'success': False, 'fractional_success': 0.7957551899052466}
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.597
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4755', '7.3441', '7.2535', '6.7952', '8.0813']
Training reward models...
Reward model losses: ['0.7373', '0.0394', '0.0240', '0.0404', '0.2485']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9378', '1.8523', '1.5238', '2.0189', '1.6490']
Training action VAE models...
Action VAE losses: ['1.3120', '1.3508', '1.2820', '1.3409', '1.3273']
CM score components:
transition disagreement: 0.4153
reward disagreement: 0.2785
state disagreement: 0.5258
action disagreement: 0.5445
total CM score: 1.7640
friction is complete. CM score: 1.7640
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
2025-07-16 15:49:25,721 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 1: 501 steps, reward: 3.126
Reset #2: visual intervention applied (success: True)
2025-07-16 15:49:26,738 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 2: 501 steps, reward: 3.126
Reset #3: visual intervention applied (success: True)
2025-07-16 15:49:27,758 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 3: 501 steps, reward: 3.126
2025-07-16 15:49:28,775 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
2025-07-16 15:49:29,788 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.126
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6767', '7.5017', '7.6931', '7.6225', '7.3385']
Training reward models...
Reward model losses: ['0.5405', '0.6468', '0.0058', '0.0185', '0.1961']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6627', '1.9627', '1.9322', '1.8297', '1.6372']
Training action VAE models...
Action VAE losses: ['1.1746', '1.2557', '1.2616', '1.2763', '1.2600']
CM score components:
transition disagreement: 0.4114
reward disagreement: 0.2354
state disagreement: 0.5457
action disagreement: 0.5718
total CM score: 1.7644
visual is complete. CM score: 1.7644
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
2025-07-16 15:49:31,916 3218693 INFO [EPISODE END] Episode ended. Reward: 0.010518816401534408, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.25163377e-02,  1.33600134e-02, -1.24174303e-05],
        [ 5.86071759e-02,  1.04483330e-01,  6.49895631e-02]]]), 'success': False, 'fractional_success': 0.8765258474020129}
episode 1: 501 steps, reward: 3.881
Reset #2: pose intervention applied (success: True)
2025-07-16 15:49:33,163 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0019360821550786585, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.52524770e-02, -7.56432965e-03, -1.09788578e-05],
        [ 4.12448300e-04,  6.81005956e-02,  6.49890211e-02]]]), 'success': False, 'fractional_success': 0.160450063488142}
episode 2: 501 steps, reward: 0.120
Reset #3: pose intervention applied (success: True)
2025-07-16 15:49:34,438 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0009020215407914532, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.08777879e-01, -1.23763077e-02, -1.09788769e-05],
        [-2.11383501e-02,  7.52632212e-02,  6.49890211e-02]]]), 'success': False, 'fractional_success': 0.07464679796292509}
episode 3: 501 steps, reward: -2.114
2025-07-16 15:49:35,684 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0018096747301780416, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.66956530e-02,  2.42916584e-02, -1.16804831e-05],
        [-2.04949048e-02,  1.00492857e-01,  6.49896151e-02]]]), 'success': False, 'fractional_success': 0.15055210676290764}
2025-07-16 15:49:36,826 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0012914077001499216, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.62443136e-02, -3.45234116e-02, -1.36058646e-05],
        [ 5.28457852e-02,  5.45654079e-02,  6.49893455e-02]]]), 'success': False, 'fractional_success': 0.10868076505400061}
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9545', '7.1566', '8.1219', '7.9200', '6.5696']
Training reward models...
Reward model losses: ['0.4341', '0.0121', '0.1875', '0.0097', '0.0297']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1615', '2.0734', '1.8914', '1.7520', '1.9400']
Training action VAE models...
Action VAE losses: ['1.3324', '1.3158', '1.1466', '1.4115', '1.3567']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.1159
state disagreement: 0.5785
action disagreement: 0.5103
total CM score: 1.6304
pose is complete. CM score: 1.6304
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
2025-07-16 15:49:39,153 3218693 INFO [EPISODE END] Episode ended. Reward: -8.584481176693304e-06, Info: {'desired_goal': array([[[-0.03754987,  0.04276486,  0.        ],
        [ 0.03754987,  0.11723514,  0.0716524 ]]]), 'achieved_goal': array([[[-1.93874985e-03, -2.68609005e-02, -1.33862359e-05],
        [ 5.98590694e-02,  3.37309526e-02,  7.16417937e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 1: 501 steps, reward: -0.151
Reset #2: random intervention applied (success: True)
2025-07-16 15:49:40,145 3218693 INFO [EPISODE END] Episode ended. Reward: 0.021758726656735353, Info: {'desired_goal': array([[[ 0.00321706, -0.13235515,  0.0033262 ],
        [ 0.08216464, -0.05340757,  0.0683262 ]]]), 'achieved_goal': array([[[-3.74958674e-02, -5.47114247e-02, -1.62800660e-05],
        [ 4.83630951e-02,  3.11516531e-02,  6.49896680e-02]]]), 'success': False, 'fractional_success': 0.008959531286577433}
episode 2: 501 steps, reward: -0.077
Reset #3: random intervention applied (success: True)
2025-07-16 15:49:41,174 3218693 INFO [EPISODE END] Episode ended. Reward: 0.00015350760171860408, Info: {'desired_goal': array([[[-0.04068463, -0.0424988 ,  0.0033262 ],
        [ 0.04232706,  0.04051289,  0.0683262 ]]]), 'achieved_goal': array([[[ 4.01506157e-02, -3.36794577e-03, -1.35238503e-05],
        [ 1.21135895e-01,  7.76142261e-02,  6.49906621e-02]]]), 'success': False, 'fractional_success': 0.013148190678320798}
episode 3: 501 steps, reward: -0.817
2025-07-16 15:49:42,208 3218693 INFO [EPISODE END] Episode ended. Reward: 0.017393316966958106, Info: {'desired_goal': array([[[-0.04260181, -0.03139132,  0.0033262 ],
        [ 0.04860821,  0.05981869,  0.0683262 ]]]), 'achieved_goal': array([[[-5.14627928e-02, -4.30242589e-03, -1.20023384e-05],
        [ 2.99560890e-02,  7.71164587e-02,  6.49896902e-02]]]), 'success': False, 'fractional_success': 0.5305367490512606}
2025-07-16 15:49:43,375 3218693 INFO [EPISODE END] Episode ended. Reward: -5.285691243769875e-06, Info: {'desired_goal': array([[[-3.89573263e-02,  4.12792138e-02,  6.93889390e-18],
        [ 3.89573263e-02,  1.18720786e-01,  6.41228113e-02]]]), 'achieved_goal': array([[[-8.40801789e-03, -5.85436588e-02, -1.41233782e-05],
        [ 7.73489076e-02,  2.71985151e-02,  6.41124123e-02]]]), 'success': False, 'fractional_success': 0.0}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.071
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6803', '7.5753', '6.9102', '7.8302', '7.8223']
Training reward models...
Reward model losses: ['0.9204', '0.4246', '0.0650', '0.9631', '0.0098']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3381', '1.3779', '1.4317', '1.3415', '1.4005']
Training action VAE models...
Action VAE losses: ['1.4035', '1.5343', '1.4418', '1.3387', '1.3607']
CM score components:
transition disagreement: 0.4190
reward disagreement: 0.4805
state disagreement: 0.5000
action disagreement: 0.5489
total CM score: 1.9485
random is complete. CM score: 1.9485
2025-07-16 15:49:44,445 3218693 INFO Meta-Episode 1/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_0.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
2025-07-16 15:49:45,537 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0023306343573782154, Info: {'desired_goal': array([[[-0.03246735,  0.01336478,  0.        ],
        [ 0.05866552,  0.10449766,  0.065     ]]]), 'achieved_goal': array([[[-6.63755881e-02,  7.35739859e-02, -1.35444287e-05],
        [ 2.10499108e-02,  1.60998936e-01,  6.49895320e-02]]]), 'success': False, 'fractional_success': 0.19923463590162638}
episode 1: 501 steps, reward: 0.546
Reset #2: goal intervention applied (success: True)
2025-07-16 15:49:46,476 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03423314229584739, Info: {'desired_goal': array([[[-0.07526177, -0.00756685,  0.        ],
        [ 0.0004061 ,  0.06810102,  0.065     ]]]), 'achieved_goal': array([[[-4.42760947e-02,  3.38377790e-02, -1.19132688e-05],
        [ 4.33082988e-02,  1.21422426e-01,  6.49891795e-02]]]), 'success': False, 'fractional_success': 0.2673422352856907}
episode 2: 501 steps, reward: 1.251
Reset #3: goal intervention applied (success: True)
2025-07-16 15:49:47,480 3218693 INFO [EPISODE END] Episode ended. Reward: -0.010648180261120153, Info: {'desired_goal': array([[[-1.00122459e-01, -1.61834099e-02,  6.93889390e-18],
        [-1.14383566e-02,  7.25006921e-02,  6.50000000e-02]]]), 'achieved_goal': array([[[-4.46348651e-02,  6.74208545e-02, -1.47584090e-05],
        [ 4.59533506e-02,  1.58006787e-01,  6.49900832e-02]]]), 'success': False, 'fractional_success': 0.021438008099876945}
episode 3: 501 steps, reward: 1.432
2025-07-16 15:49:48,455 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0022063043644349636, Info: {'desired_goal': array([[[-9.67000227e-02,  2.42933163e-02, -6.93889390e-18],
        [-2.05001920e-02,  1.00493147e-01,  6.50000000e-02]]]), 'achieved_goal': array([[[-4.36498014e-02,  5.48324942e-02, -1.57591513e-05],
        [ 4.01751084e-02,  1.38655597e-01,  6.49899640e-02]]]), 'success': False, 'fractional_success': 0.18201648026185843}
2025-07-16 15:49:49,528 3218693 INFO [EPISODE END] Episode ended. Reward: -0.004517012030429978, Info: {'desired_goal': array([[[-0.03624416, -0.03452174,  0.        ],
        [ 0.05284012,  0.05456255,  0.065     ]]]), 'achieved_goal': array([[[-4.53302524e-02,  2.07800941e-02, -1.10693937e-05],
        [ 4.65026911e-02,  1.12613077e-01,  6.49890686e-02]]]), 'success': False, 'fractional_success': 0.3521822316968322}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.668
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3073', '7.0841', '6.7828', '7.1555', '7.6099']
Training reward models...
Reward model losses: ['0.2346', '0.2040', '0.2570', '0.0035', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4635', '1.3450', '1.3792', '1.5967', '1.5780']
Training action VAE models...
Action VAE losses: ['1.5102', '1.3749', '1.5696', '1.3820', '1.3583']
CM score components:
transition disagreement: 0.4061
reward disagreement: 0.0979
state disagreement: 0.4921
action disagreement: 0.5593
total CM score: 1.5555
goal is complete. CM score: 1.5555
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
2025-07-16 15:49:51,791 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01194395344464418, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.13063836e-02,  3.42766663e-02, -1.09788635e-05],
        [ 3.21021802e-02,  1.17685230e-01,  6.49890211e-02]]]), 'success': True, 'fractional_success': 0.9937118229863028}
episode 1: 501 steps, reward: 5.938
Reset #2: mass intervention applied (success: True)
2025-07-16 15:49:52,935 3218693 INFO [EPISODE END] Episode ended. Reward: 0.011947331841664424, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.37955663e-02,  3.02924486e-02, -1.09801740e-05],
        [ 4.07201474e-02,  1.14808161e-01,  6.49890218e-02]]]), 'success': True, 'fractional_success': 0.9998311043081318}
episode 2: 501 steps, reward: 6.075
Reset #3: mass intervention applied (success: True)
2025-07-16 15:49:54,086 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01781056298767828, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.21950028e-02,  3.08132249e-02, -1.11547548e-05],
        [ 3.45135008e-02,  1.17521812e-01,  6.49890239e-02]]]), 'success': True, 'fractional_success': 0.9998311367956371}
episode 3: 501 steps, reward: 5.977
2025-07-16 15:49:55,085 3218693 INFO [EPISODE END] Episode ended. Reward: 0.011997505899384881, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.20801954e-02,  3.25891532e-02, -1.09789254e-05],
        [ 3.38593900e-02,  1.18528739e-01,  6.49890212e-02]]]), 'success': True, 'fractional_success': 0.9998310954777089}
2025-07-16 15:49:56,219 3218693 INFO [EPISODE END] Episode ended. Reward: 0.027237683511630785, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.50092566e-02,  3.06969576e-02, -1.09788580e-05],
        [ 3.91504224e-02,  1.14856637e-01,  6.49890211e-02]]]), 'success': True, 'fractional_success': 0.9998310945152344}
total data points collected: 2505
average episode length: 501.0
average episode reward: 6.119
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0583', '7.6339', '7.0574', '6.8824', '8.1730']
Training reward models...
Reward model losses: ['0.0035', '0.2961', '0.0056', '0.2835', '0.5608']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6419', '1.6767', '1.7777', '1.7878', '2.2758']
Training action VAE models...
Action VAE losses: ['1.3816', '1.3672', '1.3023', '1.3880', '1.3313']
CM score components:
transition disagreement: 0.4158
reward disagreement: 0.1750
state disagreement: 0.5891
action disagreement: 0.5499
total CM score: 1.7297
mass is complete. CM score: 1.7297
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
2025-07-16 15:49:58,305 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009187464851867423, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.78972665e-02,  5.99705196e-02, -1.19623076e-05],
        [ 2.93673656e-02,  1.47234163e-01,  6.49894727e-02]]]), 'success': False, 'fractional_success': 0.7690731885914757}
episode 1: 501 steps, reward: 4.318
Reset #2: friction intervention applied (success: True)
2025-07-16 15:49:59,320 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009506365339448, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.89251329e-02,  5.86535443e-02, -1.37062524e-05],
        [ 3.02708927e-02,  1.47851155e-01,  6.49895414e-02]]]), 'success': False, 'fractional_success': 0.7998689341353139}
episode 2: 501 steps, reward: 3.134
Reset #3: friction intervention applied (success: True)
2025-07-16 15:50:00,336 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009404182257032769, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.79952151e-02,  5.91289924e-02, -1.35230432e-05],
        [ 3.01660639e-02,  1.47289803e-01,  6.49895068e-02]]]), 'success': False, 'fractional_success': 0.7914819060914438}
episode 3: 501 steps, reward: 3.139
2025-07-16 15:50:01,309 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009218356077654961, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.79062162e-02,  5.99699196e-02, -1.19624427e-05],
        [ 2.93647744e-02,  1.47239921e-01,  6.49894727e-02]]]), 'success': False, 'fractional_success': 0.769049762342571}
2025-07-16 15:50:02,323 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009427471516556781, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.92927391e-02,  5.88877447e-02, -1.57224731e-05],
        [ 3.02204622e-02,  1.48401372e-01,  6.49899530e-02]]]), 'success': False, 'fractional_success': 0.7957551899052466}
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.597
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8746', '7.2395', '6.8932', '7.6096', '7.0196']
Training reward models...
Reward model losses: ['0.0935', '1.0181', '0.3828', '0.0191', '0.1947']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7405', '1.6567', '1.7768', '1.7537', '1.8029']
Training action VAE models...
Action VAE losses: ['1.1313', '1.3008', '1.1347', '1.1367', '1.2485']
CM score components:
transition disagreement: 0.4148
reward disagreement: 0.3827
state disagreement: 0.5453
action disagreement: 0.5217
total CM score: 1.8645
friction is complete. CM score: 1.8645
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
2025-07-16 15:50:04,437 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 1: 501 steps, reward: 3.126
Reset #2: visual intervention applied (success: True)
2025-07-16 15:50:05,452 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 2: 501 steps, reward: 3.126
Reset #3: visual intervention applied (success: True)
2025-07-16 15:50:06,470 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
episode 3: 501 steps, reward: 3.126
2025-07-16 15:50:07,484 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
2025-07-16 15:50:08,501 3218693 INFO [EPISODE END] Episode ended. Reward: 0.009488109656411378, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.90220463e-02,  5.86475130e-02, -1.37070328e-05],
        [ 3.02670294e-02,  1.47938161e-01,  6.49895424e-02]]]), 'success': False, 'fractional_success': 0.7999093059222384}
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.126
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1384', '7.7649', '7.1259', '7.2592', '7.4961']
Training reward models...
Reward model losses: ['0.0401', '0.2021', '0.0631', '0.0064', '0.0406']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5378', '1.4899', '1.6868', '1.5812', '1.7192']
Training action VAE models...
Action VAE losses: ['1.1634', '1.0966', '1.1731', '1.2336', '1.2827']
CM score components:
transition disagreement: 0.4388
reward disagreement: 0.1126
state disagreement: 0.4834
action disagreement: 0.5264
total CM score: 1.5613
visual is complete. CM score: 1.5613
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
2025-07-16 15:50:10,630 3218693 INFO [EPISODE END] Episode ended. Reward: 0.010518816401534408, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.25163377e-02,  1.33600134e-02, -1.24174303e-05],
        [ 5.86071759e-02,  1.04483330e-01,  6.49895631e-02]]]), 'success': False, 'fractional_success': 0.8765258474020129}
episode 1: 501 steps, reward: 3.881
Reset #2: pose intervention applied (success: True)
2025-07-16 15:50:11,890 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0019360821550786585, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.52524770e-02, -7.56432965e-03, -1.09788578e-05],
        [ 4.12448300e-04,  6.81005956e-02,  6.49890211e-02]]]), 'success': False, 'fractional_success': 0.160450063488142}
episode 2: 501 steps, reward: 0.120
Reset #3: pose intervention applied (success: True)
2025-07-16 15:50:13,163 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0009020215407914532, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.08777879e-01, -1.23763077e-02, -1.09788769e-05],
        [-2.11383501e-02,  7.52632212e-02,  6.49890211e-02]]]), 'success': False, 'fractional_success': 0.07464679796292509}
episode 3: 501 steps, reward: -2.114
2025-07-16 15:50:14,405 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0018096747301780416, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.66956530e-02,  2.42916584e-02, -1.16804831e-05],
        [-2.04949048e-02,  1.00492857e-01,  6.49896151e-02]]]), 'success': False, 'fractional_success': 0.15055210676290764}
2025-07-16 15:50:15,542 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0012914077001499216, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.62443136e-02, -3.45234116e-02, -1.36058646e-05],
        [ 5.28457852e-02,  5.45654079e-02,  6.49893455e-02]]]), 'success': False, 'fractional_success': 0.10868076505400061}
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7899', '7.7074', '7.7299', '6.6138', '7.3974']
Training reward models...
Reward model losses: ['3.2546', '1.1052', '0.1599', '0.0143', '0.0239']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.2591', '1.7126', '1.7209', '1.7544', '1.6301']
Training action VAE models...
Action VAE losses: ['1.2068', '1.3910', '1.3606', '1.3686', '1.1990']
CM score components:
transition disagreement: 0.4404
reward disagreement: 0.7524
state disagreement: 0.5660
action disagreement: 0.5547
total CM score: 2.3135
pose is complete. CM score: 2.3135
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
2025-07-16 15:50:17,879 3218693 INFO [EPISODE END] Episode ended. Reward: -8.584481176693304e-06, Info: {'desired_goal': array([[[-0.03754987,  0.04276486,  0.        ],
        [ 0.03754987,  0.11723514,  0.0716524 ]]]), 'achieved_goal': array([[[-1.93874985e-03, -2.68609005e-02, -1.33862359e-05],
        [ 5.98590694e-02,  3.37309526e-02,  7.16417937e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 1: 501 steps, reward: -0.151
Reset #2: random intervention applied (success: True)
2025-07-16 15:50:18,866 3218693 INFO [EPISODE END] Episode ended. Reward: 0.021758726656735353, Info: {'desired_goal': array([[[ 0.00321706, -0.13235515,  0.0033262 ],
        [ 0.08216464, -0.05340757,  0.0683262 ]]]), 'achieved_goal': array([[[-3.74958674e-02, -5.47114247e-02, -1.62800660e-05],
        [ 4.83630951e-02,  3.11516531e-02,  6.49896680e-02]]]), 'success': False, 'fractional_success': 0.008959531286577433}
episode 2: 501 steps, reward: -0.077
Reset #3: random intervention applied (success: True)
2025-07-16 15:50:19,890 3218693 INFO [EPISODE END] Episode ended. Reward: 0.00015350760171860408, Info: {'desired_goal': array([[[-0.04068463, -0.0424988 ,  0.0033262 ],
        [ 0.04232706,  0.04051289,  0.0683262 ]]]), 'achieved_goal': array([[[ 4.01506157e-02, -3.36794577e-03, -1.35238503e-05],
        [ 1.21135895e-01,  7.76142261e-02,  6.49906621e-02]]]), 'success': False, 'fractional_success': 0.013148190678320798}
episode 3: 501 steps, reward: -0.817
2025-07-16 15:50:20,923 3218693 INFO [EPISODE END] Episode ended. Reward: 0.017393316966958106, Info: {'desired_goal': array([[[-0.04260181, -0.03139132,  0.0033262 ],
        [ 0.04860821,  0.05981869,  0.0683262 ]]]), 'achieved_goal': array([[[-5.14627928e-02, -4.30242589e-03, -1.20023384e-05],
        [ 2.99560890e-02,  7.71164587e-02,  6.49896902e-02]]]), 'success': False, 'fractional_success': 0.5305367490512606}
2025-07-16 15:50:22,093 3218693 INFO [EPISODE END] Episode ended. Reward: -5.285691243769875e-06, Info: {'desired_goal': array([[[-3.89573263e-02,  4.12792138e-02,  6.93889390e-18],
        [ 3.89573263e-02,  1.18720786e-01,  6.41228113e-02]]]), 'achieved_goal': array([[[-8.40801789e-03, -5.85436588e-02, -1.41233782e-05],
        [ 7.73489076e-02,  2.71985151e-02,  6.41124123e-02]]]), 'success': False, 'fractional_success': 0.0}
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.071
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5452', '7.3277', '8.2951', '7.4403', '7.3352']
Training reward models...
Reward model losses: ['0.1064', '0.0257', '0.0201', '0.2511', '0.1674']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4047', '1.3330', '1.4363', '1.3700', '1.4021']
Training action VAE models...
Action VAE losses: ['1.4407', '1.3099', '1.3988', '1.4534', '1.5134']
CM score components:
transition disagreement: 0.3832
reward disagreement: 0.1022
state disagreement: 0.5004
action disagreement: 0.5783
total CM score: 1.5641
random is complete. CM score: 1.5641
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
2025-07-16 15:50:24,252 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0597075781511001, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.33439709e-02,  3.64759266e-02, -1.57497381e-05],
        [ 2.20942453e-02,  1.11910320e-01,  6.49901632e-02]]]), 'success': False, 'fractional_success': 0.8321658272553466}
Reset #2: visual intervention applied (success: True)
2025-07-16 15:50:25,358 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03859026041759073, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.20061679e-02,  5.24996295e-02, -1.46670532e-05],
        [ 1.75822478e-02,  1.42085482e-01,  6.49900872e-02]]]), 'success': False, 'fractional_success': 0.7111231141055353}
Reset #3: visual intervention applied (success: True)
2025-07-16 15:50:26,411 3218693 INFO [EPISODE END] Episode ended. Reward: 0.028034997157658836, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.28508043e-01,  1.43573779e-02, -1.29280062e-05],
        [ 2.03468467e-01,  8.93159054e-02,  6.49898604e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:50:27,477 3218693 INFO [EPISODE END] Episode ended. Reward: 0.037662105917508, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.21243695e-02,  9.03806115e-02, -1.62132503e-05],
        [ 7.03990925e-03,  1.59539875e-01,  6.49900084e-02]]]), 'success': False, 'fractional_success': 0.20697376879232937}
2025-07-16 15:50:28,557 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0169652028443837, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.65974954e-02,  5.12489146e-02, -1.58344623e-05],
        [ 4.53167844e-02,  1.43161792e-01,  6.49900331e-02]]]), 'success': True, 'fractional_success': 0.9421798981005326}
2025-07-16 15:50:29,650 3218693 INFO [EPISODE END] Episode ended. Reward: 0.042029626414228395, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.55938842e-02,  4.92971594e-02, -1.59658908e-05],
        [ 3.87433203e-03,  1.38768142e-01,  6.49902266e-02]]]), 'success': False, 'fractional_success': 0.5440509910610782}
2025-07-16 15:50:30,734 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05867098921038351, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 8.03318747e-03,  1.45128108e-01, -1.60336026e-05],
        [ 9.52942647e-02,  2.32387042e-01,  6.49900220e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:50:31,808 3218693 INFO [EPISODE END] Episode ended. Reward: 0.06791139155836183, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.00086332e-02,  7.00840564e-02, -1.19037615e-05],
        [ 4.92514641e-02,  1.49343907e-01,  6.49894526e-02]]]), 'success': False, 'fractional_success': 0.6274396297569343}
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 462       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5103616   |
----------------------------------
2025-07-16 15:50:33,609 3218693 INFO [EPISODE END] Episode ended. Reward: 0.021028876156254175, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.12362983e-02,  4.60589222e-02, -1.28042188e-05],
        [ 3.95871941e-02,  1.36882139e-01,  6.49896063e-02]]]), 'success': True, 'fractional_success': 0.9998400966099584}
2025-07-16 15:50:34,685 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03523783982853841, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.72883142e-02,  3.93786876e-02, -1.27875990e-05],
        [ 4.31338627e-02,  1.29800801e-01,  6.49894196e-02]]]), 'success': True, 'fractional_success': 0.9998372241922318}
2025-07-16 15:50:35,751 3218693 INFO [EPISODE END] Episode ended. Reward: -0.01095228894978699, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.44211641e-02,  7.68513078e-02, -1.18800695e-05],
        [ 3.64124359e-02,  1.67684153e-01,  6.49898364e-02]]]), 'success': False, 'fractional_success': 0.5483556626707715}
2025-07-16 15:50:36,803 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02403792752773591, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.71907984e-02,  6.17609433e-02, -1.29090229e-05],
        [ 6.03800375e-02,  1.39333769e-01,  6.49898668e-02]]]), 'success': False, 'fractional_success': 0.5966559015822611}
2025-07-16 15:50:37,899 3218693 INFO [EPISODE END] Episode ended. Reward: 0.08922360733829736, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.82340276e-02,  4.95661529e-02, -1.21238162e-05],
        [ 1.73032537e-02,  1.25102777e-01,  6.49892889e-02]]]), 'success': False, 'fractional_success': 0.7417263569194168}
2025-07-16 15:50:38,971 3218693 INFO [EPISODE END] Episode ended. Reward: 0.02433944157041896, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.34389431e-02,  8.65407950e-02, -1.24409839e-05],
        [ 7.22872811e-03,  1.57209256e-01,  6.49897778e-02]]]), 'success': False, 'fractional_success': 0.2440624868596947}
2025-07-16 15:50:40,024 3218693 INFO [EPISODE END] Episode ended. Reward: -0.005473461950247346, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.70364908e-02,  7.98417663e-02, -1.29896184e-05],
        [ 3.84132719e-02,  1.45290195e-01,  6.49894086e-02]]]), 'success': False, 'fractional_success': 0.4601277647565847}
2025-07-16 15:50:41,101 3218693 INFO [EPISODE END] Episode ended. Reward: 0.029001596239523263, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.99786743e-02,  5.26349342e-02, -1.33156673e-05],
        [ 4.03909892e-02,  1.43003430e-01,  6.49899541e-02]]]), 'success': True, 'fractional_success': 0.9208586685605407}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 446         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5107712     |
| train/                  |             |
|    approx_kl            | 0.045231037 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.504      |
|    n_updates            | 1365        |
|    policy_gradient_loss | -0.0508     |
|    std                  | 2.93        |
|    value_loss           | 0.0515      |
-----------------------------------------
2025-07-16 15:50:42,901 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0954142155628732, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.53856627e-02,  5.23713370e-02, -1.17666144e-05],
        [ 2.13757312e-02,  1.19133310e-01,  6.49891691e-02]]]), 'success': False, 'fractional_success': 0.7666120452161805}
2025-07-16 15:50:44,007 3218693 INFO [EPISODE END] Episode ended. Reward: 0.04382566554069558, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.57501719e-02,  8.58047419e-02, -1.30263373e-05],
        [-8.58551163e-03,  1.62970179e-01,  6.49894687e-02]]]), 'success': False, 'fractional_success': 0.1510769241962167}
2025-07-16 15:50:45,061 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0033496962667508338, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.30272965e-02,  5.96191655e-02, -1.61211947e-05],
        [ 5.30847670e-02,  1.35726115e-01,  6.49899601e-02]]]), 'success': False, 'fractional_success': 0.6948819478430953}
2025-07-16 15:50:46,134 3218693 INFO [EPISODE END] Episode ended. Reward: -0.07255599692825244, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.61549954e-02,  6.60216043e-02, -1.29385730e-05],
        [-2.56841108e-02,  1.36493687e-01,  6.49898700e-02]]]), 'success': False, 'fractional_success': 0.07496857371277209}
2025-07-16 15:50:47,214 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02218461909470332, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.71313069e-02,  3.92198429e-02, -1.45112101e-05],
        [ 1.36409857e-02,  1.29992733e-01,  6.49892278e-02]]]), 'success': False, 'fractional_success': 0.7097436758814046}
2025-07-16 15:50:48,288 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0033907763968483067, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.18435506e-02,  4.59985485e-02, -1.15936414e-05],
        [ 2.00746344e-02,  1.17917165e-01,  6.49891338e-02]]]), 'success': False, 'fractional_success': 0.8087053130379963}
2025-07-16 15:50:49,395 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03983708856510495, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.08450774e-02,  6.17292767e-02, -1.31440595e-05],
        [-7.71098209e-03,  1.44863660e-01,  6.49902075e-02]]]), 'success': False, 'fractional_success': 0.2978382873289781}
2025-07-16 15:50:50,463 3218693 INFO [EPISODE END] Episode ended. Reward: -0.03764321027870462, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.74308983e-02,  8.10003896e-02, -1.16305925e-04],
        [ 3.01441038e-02,  1.76990578e-01,  7.30607992e-02]]]), 'success': False, 'fractional_success': 0.4670449378677619}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.030364338 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0418     |
|    std                  | 2.97        |
|    value_loss           | 0.0392      |
-----------------------------------------
2025-07-16 15:50:52,288 3218693 INFO [EPISODE END] Episode ended. Reward: 0.018814936021683503, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.32201389e-01,  7.83495281e-02, -1.91549924e-05],
        [-5.37404346e-02,  1.56810986e-01,  6.49899886e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:50:53,355 3218693 INFO [EPISODE END] Episode ended. Reward: -0.010707842008575415, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.13271462e-02,  5.29477206e-02, -1.35670607e-05],
        [ 3.80872140e-02,  1.42362336e-01,  6.49895256e-02]]]), 'success': True, 'fractional_success': 0.9160412749307352}
2025-07-16 15:50:54,415 3218693 INFO [EPISODE END] Episode ended. Reward: 0.016681049079516536, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24175034e-02,  6.80770792e-02, -1.48627427e-05],
        [ 5.51352412e-02,  1.55629673e-01,  6.49910165e-02]]]), 'success': False, 'fractional_success': 0.6824678226911236}
2025-07-16 15:50:55,483 3218693 INFO [EPISODE END] Episode ended. Reward: 0.031811411834679164, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.70813550e-02,  3.39099721e-02, -1.58998190e-05],
        [ 3.88223183e-02,  1.19817337e-01,  6.49900146e-02]]]), 'success': True, 'fractional_success': 0.9998463780637948}
2025-07-16 15:50:56,545 3218693 INFO [EPISODE END] Episode ended. Reward: -0.027860763257000883, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.50727549e-01, -3.42985609e-02, -1.37212840e-05],
        [ 2.40056713e-01,  5.50290039e-02,  6.49895634e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:50:57,609 3218693 INFO [EPISODE END] Episode ended. Reward: -0.00493401163901888, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.42672416e-02,  5.32844215e-02, -1.23372264e-05],
        [ 6.42028394e-02,  1.41754731e-01,  6.49892743e-02]]]), 'success': False, 'fractional_success': 0.7954912109892174}
2025-07-16 15:50:58,721 3218693 INFO [EPISODE END] Episode ended. Reward: -0.16577109166918907, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.20133761,  0.06478684, -0.0005945 ],
        [-0.10264113,  0.15544411,  0.07929082]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:50:59,791 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0076823087715850846, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.25879269e-02,  4.97407545e-02, -1.28907262e-05],
        [ 2.64571147e-02,  1.38785593e-01,  6.49898808e-02]]]), 'success': False, 'fractional_success': 0.8756279311499294}
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 438        |
|    iterations           | 4          |
|    time_elapsed         | 37         |
|    total_timesteps      | 5115904    |
| train/                  |            |
|    approx_kl            | 0.03203421 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.493     |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.0445    |
|    std                  | 3          |
|    value_loss           | 0.0632     |
----------------------------------------
2025-07-16 15:51:01,584 3218693 INFO [EPISODE END] Episode ended. Reward: 0.04583855955449218, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.16146728e-02,  3.42803718e-02, -1.29340809e-05],
        [ 1.43789344e-02,  1.00272568e-01,  6.49898662e-02]]]), 'success': False, 'fractional_success': 0.5854523255764261}
2025-07-16 15:51:02,654 3218693 INFO [EPISODE END] Episode ended. Reward: -0.019194115166761474, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.95404839e-02,  9.18984512e-02, -1.29458260e-05],
        [-1.54292744e-02,  1.66011394e-01,  6.49900425e-02]]]), 'success': False, 'fractional_success': 0.08322591968977854}
2025-07-16 15:51:03,719 3218693 INFO [EPISODE END] Episode ended. Reward: 0.023624604681235006, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.35644140e-02,  7.96724484e-02, -1.50278562e-05],
        [ 3.31750193e-02,  1.66415547e-01,  6.49908014e-02]]]), 'success': False, 'fractional_success': 0.5049677833968351}
2025-07-16 15:51:04,784 3218693 INFO [EPISODE END] Episode ended. Reward: 0.004975678532819131, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.21178870e-02,  5.42935118e-02, -1.46905767e-05],
        [ 4.76004231e-02,  1.44009288e-01,  6.49900861e-02]]]), 'success': False, 'fractional_success': 0.8953478528510868}
2025-07-16 15:51:05,842 3218693 INFO [EPISODE END] Episode ended. Reward: 0.04056557907378159, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.16803870e-02,  4.01717854e-02, -1.57710968e-05],
        [ 2.80270574e-02,  1.29879560e-01,  6.49899858e-02]]]), 'success': True, 'fractional_success': 0.9310420356121796}
2025-07-16 15:51:06,890 3218693 INFO [EPISODE END] Episode ended. Reward: 0.07280026030759316, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.94447656e-02,  1.30735413e-02, -1.18777580e-05],
        [ 4.73389122e-02,  9.98562527e-02,  6.49894691e-02]]]), 'success': False, 'fractional_success': 0.8053503114439697}
2025-07-16 15:51:07,961 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05939449092347213, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.33964219e-02,  4.96014267e-02, -1.28638468e-05],
        [ 1.79941917e-02,  1.40991052e-01,  6.49896078e-02]]]), 'success': False, 'fractional_success': 0.7515987782112404}
2025-07-16 15:51:09,033 3218693 INFO [EPISODE END] Episode ended. Reward: 0.006442583166532475, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.06234386e-01, -4.49065773e-02, -1.29431607e-05],
        [-2.59431266e-02,  3.53852964e-02,  6.49898719e-02]]]), 'success': False, 'fractional_success': 0.0}
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 251       |
|    ep_rew_mean          | 2.2542775 |
| time/                   |           |
|    fps                  | 437       |
|    iterations           | 5         |
|    time_elapsed         | 46        |
|    total_timesteps      | 5120000   |
| train/                  |           |
|    approx_kl            | 0.0432564 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.48     |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0306   |
|    std                  | 3.03      |
|    value_loss           | 0.0993    |
---------------------------------------
2025-07-16 15:51:10,818 3218693 INFO [EPISODE END] Episode ended. Reward: 0.019452809194664335, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.41339740e-02,  8.68609782e-02, -1.18952731e-05],
        [ 2.48404435e-02,  1.55835933e-01,  6.49894403e-02]]]), 'success': False, 'fractional_success': 0.3479086494301778}
2025-07-16 15:51:11,901 3218693 INFO [EPISODE END] Episode ended. Reward: -0.004138884092017172, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.10264128e-02,  4.83135050e-02, -1.29200813e-05],
        [ 3.49750528e-02,  1.24313039e-01,  6.49898646e-02]]]), 'success': True, 'fractional_success': 0.9873305608751503}
2025-07-16 15:51:12,960 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04384811810004799, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.78457779e-02,  9.09496651e-02, -1.29494672e-05],
        [ 5.59381036e-02,  1.64735412e-01,  6.49898493e-02]]]), 'success': False, 'fractional_success': 0.25675714623934043}
2025-07-16 15:51:14,017 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0013351181305420931, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.32628856e-02,  3.86418349e-02, -1.58700411e-05],
        [ 2.81304063e-02,  1.30034637e-01,  6.49900174e-02]]]), 'success': True, 'fractional_success': 0.9326322273066217}
2025-07-16 15:51:15,056 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04684220016518134, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.98824612e-02,  5.97391147e-02, -1.66346618e-05],
        [ 4.53674545e-02,  1.24986811e-01,  6.49899520e-02]]]), 'success': False, 'fractional_success': 0.654039714954747}
2025-07-16 15:51:16,117 3218693 INFO [EPISODE END] Episode ended. Reward: -0.06840913157236735, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.50363787e-03,  5.46626151e-02, -1.72153224e-05],
        [ 6.24527948e-02,  1.24614235e-01,  6.49914927e-02]]]), 'success': False, 'fractional_success': 0.5475510007334812}
2025-07-16 15:51:17,180 3218693 INFO [EPISODE END] Episode ended. Reward: 0.10258213252355551, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.01292276e-01,  5.54112516e-03, -1.60479727e-05],
        [-1.42582975e-02,  9.25775425e-02,  6.49902533e-02]]]), 'success': False, 'fractional_success': 0.1945959330643589}
2025-07-16 15:51:18,242 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012762510802737947, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 2.63021384e-02,  5.13711640e-02, -1.88859091e-05],
        [ 1.17350089e-01,  1.42416458e-01,  6.49922110e-02]]]), 'success': False, 'fractional_success': 0.08966216868200994}
2025-07-16 15:51:19,315 3218693 INFO [EPISODE END] Episode ended. Reward: 0.00629286195645713, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 9.99385008e-02,  1.08874073e-01, -1.16483760e-05],
        [ 1.67535574e-01,  1.76470627e-01,  6.49891779e-02]]]), 'success': False, 'fractional_success': 0.0}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 6           |
|    time_elapsed         | 56          |
|    total_timesteps      | 5124096     |
| train/                  |             |
|    approx_kl            | 0.025868267 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1425        |
|    policy_gradient_loss | -0.0478     |
|    std                  | 3.05        |
|    value_loss           | 0.0585      |
-----------------------------------------
2025-07-16 15:51:21,096 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0431683746110225, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.74428428e-02,  5.56409406e-02, -1.57204500e-05],
        [ 3.13707805e-02,  1.44453958e-01,  6.49899819e-02]]]), 'success': False, 'fractional_success': 0.8594255098753958}
2025-07-16 15:51:22,154 3218693 INFO [EPISODE END] Episode ended. Reward: 0.031622097801367995, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.75380885e-02,  7.63413457e-02, -1.26135664e-05],
        [ 1.62901009e-02,  1.60171333e-01,  6.49899467e-02]]]), 'success': False, 'fractional_success': 0.4174938540546373}
2025-07-16 15:51:23,237 3218693 INFO [EPISODE END] Episode ended. Reward: 0.056833811179968975, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.18440996e-02,  1.18935873e-02, -1.28277232e-05],
        [ 7.87686860e-03,  1.01613353e-01,  6.49897424e-02]]]), 'success': False, 'fractional_success': 0.5170610548429517}
2025-07-16 15:51:24,304 3218693 INFO [EPISODE END] Episode ended. Reward: 0.044036145351555346, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.50096799e-02,  6.37922883e-02, -1.36859319e-05],
        [ 2.03474118e-02,  1.39152150e-01,  6.49895527e-02]]]), 'success': False, 'fractional_success': 0.609150951648088}
2025-07-16 15:51:25,351 3218693 INFO [EPISODE END] Episode ended. Reward: 0.023661396660833313, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 9.18592506e-02, -6.62089269e-03, -1.59835589e-05],
        [ 1.80062199e-01,  8.15788104e-02,  6.49900220e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:51:26,407 3218693 INFO [EPISODE END] Episode ended. Reward: 0.046681964737019986, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.75517650e-02,  5.54350927e-02, -1.29298838e-05],
        [ 4.55838594e-02,  1.28569658e-01,  6.49898620e-02]]]), 'success': False, 'fractional_success': 0.8109618745330412}
2025-07-16 15:51:27,506 3218693 INFO [EPISODE END] Episode ended. Reward: -0.027575176820449444, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.05261204e-01,  6.74549820e-02, -1.41651845e-05],
        [-2.42771128e-02,  1.48439815e-01,  6.49902425e-02]]]), 'success': False, 'fractional_success': 0.08765550276304197}
2025-07-16 15:51:28,592 3218693 INFO [EPISODE END] Episode ended. Reward: -0.13510811815028725, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.07817374,  0.01141134, -0.00025311],
        [ 0.01284372,  0.10240355,  0.06485096]]]), 'success': False, 'fractional_success': 0.5878871168513918}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5128192     |
| train/                  |             |
|    approx_kl            | 0.031373758 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.48       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0521     |
|    std                  | 3.07        |
|    value_loss           | 0.103       |
-----------------------------------------
2025-07-16 15:51:30,363 3218693 INFO [EPISODE END] Episode ended. Reward: 0.050097266473661854, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.87424911e-02,  2.96244769e-02, -1.64155206e-05],
        [-1.55515914e-02,  1.12811211e-01,  6.49898536e-02]]]), 'success': False, 'fractional_success': 0.26070404585725204}
2025-07-16 15:51:31,420 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04201494233521707, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.68144340e-01, -2.34106884e-02, -1.22530342e-05],
        [-7.62606638e-02,  6.84719439e-02,  6.49897379e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:51:32,473 3218693 INFO [EPISODE END] Episode ended. Reward: -0.12788548843974706, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.21473638e-02,  5.52074746e-02, -1.36376165e-05],
        [ 1.02192923e-01,  1.45253150e-01,  6.49895533e-02]]]), 'success': False, 'fractional_success': 0.27594473765699346}
2025-07-16 15:51:33,516 3218693 INFO [EPISODE END] Episode ended. Reward: 0.019726586560832603, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.03333851e-01,  7.91349683e-02, -1.20358430e-05],
        [-1.44188957e-02,  1.68050084e-01,  6.49894976e-02]]]), 'success': False, 'fractional_success': 0.14276429429661144}
2025-07-16 15:51:34,569 3218693 INFO [EPISODE END] Episode ended. Reward: 0.03401862714143058, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.53657774e-02,  6.48907675e-02, -1.37844934e-05],
        [ 5.16057218e-02,  1.51864565e-01,  6.49896919e-02]]]), 'success': False, 'fractional_success': 0.7323335739995068}
2025-07-16 15:51:35,632 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04096381559757064, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.24586454e-02,  5.98247035e-02, -1.60369762e-05],
        [-1.10087092e-02,  1.41270169e-01,  6.49900111e-02]]]), 'success': False, 'fractional_success': 0.26790204653870886}
2025-07-16 15:51:36,688 3218693 INFO [EPISODE END] Episode ended. Reward: 0.11020422517080941, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.22345438e-02,  5.14573787e-02, -1.38177405e-05],
        [ 3.69094863e-02,  1.30604056e-01,  6.49895597e-02]]]), 'success': True, 'fractional_success': 0.938966409200555}
2025-07-16 15:51:37,743 3218693 INFO [EPISODE END] Episode ended. Reward: -0.09765963025250037, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.76931381e-02,  5.35118508e-02, -1.22073368e-05],
        [ 5.83257382e-03,  1.27036851e-01,  6.49893481e-02]]]), 'success': False, 'fractional_success': 0.5350998894066333}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 8           |
|    time_elapsed         | 75          |
|    total_timesteps      | 5132288     |
| train/                  |             |
|    approx_kl            | 0.025940482 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.9       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.482      |
|    n_updates            | 1455        |
|    policy_gradient_loss | -0.042      |
|    std                  | 3.1         |
|    value_loss           | 0.0899      |
-----------------------------------------
2025-07-16 15:51:39,523 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02097037639551824, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.51580980e-02,  6.09587184e-02, -7.76879967e-05],
        [-1.08963213e-02,  1.45225984e-01,  6.49553708e-02]]]), 'success': False, 'fractional_success': 0.2633649159526019}
2025-07-16 15:51:40,573 3218693 INFO [EPISODE END] Episode ended. Reward: 0.06654446948283063, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.80191992e-02,  6.48848758e-02, -1.26523340e-05],
        [ 4.20081472e-02,  1.34913183e-01,  6.49898586e-02]]]), 'success': False, 'fractional_success': 0.6819359975302497}
2025-07-16 15:51:41,638 3218693 INFO [EPISODE END] Episode ended. Reward: 0.024887978649015593, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.31680345e-02,  4.09176739e-02, -1.48533213e-05],
        [ 2.85278635e-02,  1.32611752e-01,  6.49901123e-02]]]), 'success': True, 'fractional_success': 0.9387473841788248}
2025-07-16 15:51:42,702 3218693 INFO [EPISODE END] Episode ended. Reward: -0.07533573619872204, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.83282671e-02,  2.69085834e-02, -1.59990595e-05],
        [ 3.46826709e-02,  1.09915298e-01,  6.49900118e-02]]]), 'success': True, 'fractional_success': 0.9600877949641061}
2025-07-16 15:51:43,770 3218693 INFO [EPISODE END] Episode ended. Reward: -0.04603631966895777, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.44747838e-02,  4.88375513e-02, -1.22359418e-05],
        [ 2.73888666e-02,  1.40701713e-01,  6.49895285e-02]]]), 'success': True, 'fractional_success': 0.9022621707821735}
2025-07-16 15:51:44,823 3218693 INFO [EPISODE END] Episode ended. Reward: 0.033279393184750115, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.31155639e-01,  2.46894282e-02, -1.59345864e-05],
        [ 2.22363008e-01,  1.15898926e-01,  6.49900075e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:51:45,874 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01334412382744723, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.16531953e-02,  6.19114616e-02, -1.20726704e-05],
        [ 4.51076504e-02,  1.38671636e-01,  6.49892777e-02]]]), 'success': False, 'fractional_success': 0.7680191784656497}
2025-07-16 15:51:46,921 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02806206832610068, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.37546392e-02,  4.28925430e-02, -1.57001232e-05],
        [ 4.23275272e-02,  1.28976051e-01,  6.49899431e-02]]]), 'success': True, 'fractional_success': 0.9998452779106517}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 9           |
|    time_elapsed         | 84          |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.019549722 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23         |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.502      |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0473     |
|    std                  | 3.12        |
|    value_loss           | 0.0556      |
-----------------------------------------
2025-07-16 15:51:48,738 3218693 INFO [EPISODE END] Episode ended. Reward: 0.02260679449734049, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.26865270e-02,  3.45382592e-02, -1.16637245e-05],
        [-2.47395941e-02,  1.02484811e-01,  6.49894666e-02]]]), 'success': False, 'fractional_success': 0.10097876917355231}
2025-07-16 15:51:49,819 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02603241131256559, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.22042405e-02,  1.96537279e-02, -1.55844719e-05],
        [ 2.08661079e-02,  9.27219914e-02,  6.49899601e-02]]]), 'success': False, 'fractional_success': 0.5711121681036904}
2025-07-16 15:51:50,878 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05441135504033488, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.76050414e-04,  2.75884823e-02, -1.71251511e-05],
        [ 6.96823515e-02,  9.78470786e-02,  6.49895908e-02]]]), 'success': False, 'fractional_success': 0.39408658624471565}
2025-07-16 15:51:51,954 3218693 INFO [EPISODE END] Episode ended. Reward: 0.030274564389781166, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.31158132e-01,  3.00910579e-02, -1.35467446e-05],
        [-3.92437144e-02,  1.22006116e-01,  6.49896363e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:51:53,032 3218693 INFO [EPISODE END] Episode ended. Reward: -0.14293312853416593, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.84696043e-02,  4.71844578e-02, -1.22655720e-05],
        [ 1.31147728e-02,  1.18768095e-01,  6.49893593e-02]]]), 'success': False, 'fractional_success': 0.7016508550459591}
2025-07-16 15:51:54,085 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05235867201910698, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.80683164e-02,  4.95293145e-02, -1.56590089e-05],
        [ 2.74256362e-02,  1.35021850e-01,  6.49899640e-02]]]), 'success': False, 'fractional_success': 0.8930120121439384}
2025-07-16 15:51:55,150 3218693 INFO [EPISODE END] Episode ended. Reward: -0.05599477308719936, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.65706231e-02,  4.53498942e-02, -1.27407615e-05],
        [ 2.82373213e-02,  1.20159591e-01,  6.49893451e-02]]]), 'success': True, 'fractional_success': 0.9342671561278358}
2025-07-16 15:51:56,216 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02222031967999734, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.10875937e-02,  2.83582693e-02, -1.19161846e-05],
        [ 2.37301835e-02,  1.13177086e-01,  6.49894596e-02]]]), 'success': False, 'fractional_success': 0.8649394653979503}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 5140480     |
| train/                  |             |
|    approx_kl            | 0.025581973 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.1       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.493      |
|    n_updates            | 1485        |
|    policy_gradient_loss | -0.0429     |
|    std                  | 3.15        |
|    value_loss           | 0.0517      |
-----------------------------------------
2025-07-16 15:51:58,054 3218693 INFO [EPISODE END] Episode ended. Reward: -0.06802738041548699, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.18733533e-02,  6.94162999e-02, -1.38899621e-05],
        [-1.16327709e-02,  1.39653972e-01,  6.49895490e-02]]]), 'success': False, 'fractional_success': 0.21275571341904725}
2025-07-16 15:51:59,132 3218693 INFO [EPISODE END] Episode ended. Reward: -0.007207148808944071, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-8.94792761e-02,  9.38132159e-03, -1.35048244e-05],
        [ 2.04155089e-03,  1.00903101e-01,  6.49895914e-02]]]), 'success': False, 'fractional_success': 0.43652793828183767}
2025-07-16 15:52:00,203 3218693 INFO [EPISODE END] Episode ended. Reward: 0.05301748714516097, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.61400504e-02,  6.61515771e-02, -1.26909002e-05],
        [ 8.30036632e-03,  1.40593419e-01,  6.49898963e-02]]]), 'success': False, 'fractional_success': 0.4475121151191081}
2025-07-16 15:52:01,288 3218693 INFO [EPISODE END] Episode ended. Reward: -0.028498325766522097, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.69786464e-02,  1.54569636e-01, -1.69971944e-05],
        [ 5.80007561e-02,  2.39553301e-01,  6.49899924e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:52:02,341 3218693 INFO [EPISODE END] Episode ended. Reward: 0.04855553096749626, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.09842324e-02,  4.71544189e-02, -2.98318916e-05],
        [ 5.42816240e-02,  1.33029739e-01,  6.60951301e-02]]]), 'success': True, 'fractional_success': 0.9766804979008761}
2025-07-16 15:52:03,448 3218693 INFO [EPISODE END] Episode ended. Reward: -0.02398892894476613, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-9.51750639e-02,  1.51452067e-02, -7.01451605e-05],
        [-1.06400984e-02,  9.96895302e-02,  6.49435325e-02]]]), 'success': False, 'fractional_success': 0.2697909817095452}
2025-07-16 15:52:04,520 3218693 INFO [EPISODE END] Episode ended. Reward: -0.011308532367315915, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.86898115e-02,  2.30524295e-02, -1.56947315e-05],
        [ 1.73099107e-02,  1.09050827e-01,  6.49899663e-02]]]), 'success': False, 'fractional_success': 0.7255308676190222}
2025-07-16 15:52:05,576 3218693 INFO [EPISODE END] Episode ended. Reward: 0.040520893257682905, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-2.35465384e-02,  4.29099348e-02, -1.35585956e-05],
        [ 6.83646643e-02,  1.34821787e-01,  6.49896318e-02]]]), 'success': False, 'fractional_success': 0.8621168978583336}
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 435        |
|    iterations           | 11         |
|    time_elapsed         | 103        |
|    total_timesteps      | 5144576    |
| train/                  |            |
|    approx_kl            | 0.02706469 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.499     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0395    |
|    std                  | 3.17       |
|    value_loss           | 0.0563     |
----------------------------------------
2025-07-16 15:52:07,364 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0791961344701747, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.15181879e-02,  6.87548039e-02, -1.59198116e-05],
        [ 2.42033990e-02,  1.54472656e-01,  6.49900143e-02]]]), 'success': False, 'fractional_success': 0.5870107070227464}
2025-07-16 15:52:08,427 3218693 INFO [EPISODE END] Episode ended. Reward: 0.01035708231395078, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.69906731e-02,  3.80424324e-02, -1.84358217e-05],
        [ 1.41745126e-02,  1.29206305e-01,  6.49891369e-02]]]), 'success': False, 'fractional_success': 0.7179494176084783}
2025-07-16 15:52:09,490 3218693 INFO [EPISODE END] Episode ended. Reward: -0.008477821860127502, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.08836122,  0.01034625, -0.00027477],
        [ 0.00361004,  0.10229258,  0.06485167]]]), 'success': False, 'fractional_success': 0.4672300405515418}
2025-07-16 15:52:10,532 3218693 INFO [EPISODE END] Episode ended. Reward: -0.06599706966188121, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.72110099e-02,  6.10150647e-02, -1.47143441e-05],
        [ 4.45958121e-02,  1.52823591e-01,  6.49901096e-02]]]), 'success': False, 'fractional_success': 0.7919554056242811}
2025-07-16 15:52:11,577 3218693 INFO [EPISODE END] Episode ended. Reward: -0.038237189996738544, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-0.04305712,  0.0788685 , -0.00064693],
        [ 0.03977196,  0.16543021,  0.06984046]]]), 'success': False, 'fractional_success': 0.5174076613989005}
2025-07-16 15:52:12,623 3218693 INFO [EPISODE END] Episode ended. Reward: -0.039547430590022546, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.15862150e-02,  3.79922253e-02, -1.55440504e-05],
        [ 1.03478893e-01,  1.29885713e-01,  6.49896620e-02]]]), 'success': False, 'fractional_success': 0.32169936588777825}
2025-07-16 15:52:13,689 3218693 INFO [EPISODE END] Episode ended. Reward: 0.030091471155946656, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.70464337e-02,  3.58916214e-02, -1.28693893e-05],
        [ 1.43980114e-02,  1.27337040e-01,  6.49896085e-02]]]), 'success': False, 'fractional_success': 0.7213925203952616}
2025-07-16 15:52:14,733 3218693 INFO [EPISODE END] Episode ended. Reward: -0.043036144292905136, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-1.44978691e-01,  1.03515552e-01, -1.51563907e-05],
        [-7.86533068e-02,  1.69837491e-01,  6.49899860e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:52:15,790 3218693 INFO [EPISODE END] Episode ended. Reward: -0.08219004700468505, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.97506038e-02,  5.33969239e-02, -1.17308618e-05],
        [-6.02442234e-03,  1.27123581e-01,  6.49891154e-02]]]), 'success': False, 'fractional_success': 0.37030202302620013}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 12          |
|    time_elapsed         | 112         |
|    total_timesteps      | 5148672     |
| train/                  |             |
|    approx_kl            | 0.018755198 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.2       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.496      |
|    n_updates            | 1515        |
|    policy_gradient_loss | -0.0332     |
|    std                  | 3.19        |
|    value_loss           | 0.0653      |
-----------------------------------------
2025-07-16 15:52:17,559 3218693 INFO [EPISODE END] Episode ended. Reward: 0.0431147891851583, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-5.47973204e-02,  7.13915997e-02, -1.23099552e-05],
        [ 2.00229275e-02,  1.46212333e-01,  6.49897200e-02]]]), 'success': False, 'fractional_success': 0.5109566994725603}
2025-07-16 15:52:18,617 3218693 INFO [EPISODE END] Episode ended. Reward: -0.020357511907628422, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 9.59578213e-03,  1.63445691e-01, -1.29359237e-05],
        [ 7.72056496e-02,  2.31054221e-01,  6.49898675e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:52:19,669 3218693 INFO [EPISODE END] Episode ended. Reward: -0.007678052120815458, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.86975486e-02,  3.93578205e-02, -3.77409816e-05],
        [ 2.32206567e-02,  1.31295392e-01,  6.49899986e-02]]]), 'success': False, 'fractional_success': 0.8571089698284249}
2025-07-16 15:52:20,724 3218693 INFO [EPISODE END] Episode ended. Reward: -0.00020260705493477404, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-7.48117804e-02,  6.09327917e-02, -1.21665945e-05],
        [ 1.58584245e-02,  1.51602711e-01,  6.49895169e-02]]]), 'success': False, 'fractional_success': 0.5901317784973401}
2025-07-16 15:52:21,796 3218693 INFO [EPISODE END] Episode ended. Reward: -0.0715537247577534, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-6.84259206e-02,  5.83746618e-02, -2.34990250e-05],
        [ 6.53541224e-03,  1.33333258e-01,  6.49846609e-02]]]), 'success': False, 'fractional_success': 0.49995415279166383}
2025-07-16 15:52:22,850 3218693 INFO [EPISODE END] Episode ended. Reward: 0.012755208452036303, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-4.16247045e-02,  7.28013867e-02, -1.56946141e-05],
        [ 4.95838229e-02,  1.64009521e-01,  6.49900112e-02]]]), 'success': False, 'fractional_success': 0.6106540405855024}
2025-07-16 15:52:23,909 3218693 INFO [EPISODE END] Episode ended. Reward: 0.14300323128046064, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[ 1.15119775e-01,  6.96193224e-02, -1.59506072e-05],
        [ 2.06944482e-01,  1.61445602e-01,  6.49900032e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:52:24,971 3218693 INFO [EPISODE END] Episode ended. Reward: 0.026911643623128766, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.47335217e-02,  5.68340294e-02, -1.25545325e-05],
        [ 4.51819932e-02,  1.36750637e-01,  6.49893935e-02]]]), 'success': False, 'fractional_success': 0.8562598025106001}
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 13          |
|    time_elapsed         | 122         |
|    total_timesteps      | 5152768     |
| train/                  |             |
|    approx_kl            | 0.018374024 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.2       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.48       |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0376     |
|    std                  | 3.21        |
|    value_loss           | 0.0857      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_1.zip

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-0.814, success=False
2025-07-16 15:52:27,034 3218693 INFO [EVAL EPISODE END] Episode 1 ended. Total reward: -0.8142377938617781, Length: 501, Success: False
episode 2: length=501, reward=-0.814, success=False
2025-07-16 15:52:28,100 3218693 INFO [EVAL EPISODE END] Episode 2 ended. Total reward: -0.8142377938617781, Length: 501, Success: False
episode 3: length=501, reward=-0.814, success=False
2025-07-16 15:52:29,172 3218693 INFO [EVAL EPISODE END] Episode 3 ended. Total reward: -0.8142377938617781, Length: 501, Success: False
2025-07-16 15:52:30,238 3218693 INFO [EVAL EPISODE END] Episode 4 ended. Total reward: -0.8142377938617781, Length: 501, Success: False
2025-07-16 15:52:31,310 3218693 INFO [EVAL EPISODE END] Episode 5 ended. Total reward: -0.8142377938617781, Length: 501, Success: False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.814
average episode length: 501.0
2025-07-16 15:52:31,311 3218693 INFO [EVAL SUMMARY] Success rate: 0.0, Avg reward: -0.8142377938617781, Avg length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
2025-07-16 15:52:32,460 3218693 INFO [EPISODE END] Episode ended. Reward: -1.4306668450175808e-05, Info: {'desired_goal': array([[[-0.03246735,  0.01336478,  0.        ],
        [ 0.05866552,  0.10449766,  0.065     ]]]), 'achieved_goal': array([[[-3.24977777e-02, -1.12499604e-01, -1.09789753e-05],
        [ 3.25027719e-02, -4.74990541e-02,  6.49890212e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 1: 501 steps, reward: -0.739
Reset #2: goal intervention applied (success: True)
2025-07-16 15:52:33,531 3218693 INFO [EPISODE END] Episode ended. Reward: -2.7156903659775188e-05, Info: {'desired_goal': array([[[-0.07526177, -0.00756685,  0.        ],
        [ 0.0004061 ,  0.06810102,  0.065     ]]]), 'achieved_goal': array([[[-3.24892413e-02, -1.12503502e-01, -1.85184675e-05],
        [ 3.25230710e-02, -4.74921025e-02,  6.49960106e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 2: 501 steps, reward: -0.825
Reset #3: goal intervention applied (success: True)
2025-07-16 15:52:34,593 3218693 INFO [EPISODE END] Episode ended. Reward: -5.113697220472113e-05, Info: {'desired_goal': array([[[-1.00122459e-01, -1.61834099e-02,  6.93889390e-18],
        [-1.14383566e-02,  7.25006921e-02,  6.50000000e-02]]]), 'achieved_goal': array([[[-3.38790221e-02, -1.12903394e-01, -1.86549516e-05],
        [ 3.28986591e-02, -4.61256453e-02,  6.49966956e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 3: 501 steps, reward: -0.849
2025-07-16 15:52:35,652 3218693 INFO [EPISODE END] Episode ended. Reward: -2.712138144956855e-05, Info: {'desired_goal': array([[[-9.67000227e-02,  2.42933163e-02, -6.93889390e-18],
        [-2.05001920e-02,  1.00493147e-01,  6.50000000e-02]]]), 'achieved_goal': array([[[-3.32417484e-02, -1.12575082e-01, -1.68502219e-05],
        [ 3.27579496e-02, -4.65755057e-02,  6.49949100e-02]]]), 'success': False, 'fractional_success': 0.0}
2025-07-16 15:52:36,719 3218693 INFO [EPISODE END] Episode ended. Reward: -1.5230008956429453e-05, Info: {'desired_goal': array([[[-0.03624416, -0.03452174,  0.        ],
        [ 0.05284012,  0.05456255,  0.065     ]]]), 'achieved_goal': array([[[-3.24915090e-02, -1.12503904e-01, -1.85184673e-05],
        [ 3.25202363e-02, -4.74930720e-02,  6.49960106e-02]]]), 'success': False, 'fractional_success': 0.0}
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.813
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1702', '7.2273', '7.7917', '7.8239', '8.8966']
Training reward models...
Reward model losses: ['0.1361', '0.0151', '0.2222', '0.0023', '0.0080']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1660', '1.2042', '1.2539', '1.1526', '1.1983']
Training action VAE models...
Action VAE losses: ['0.7736', '0.8674', '0.7279', '0.8124', '0.7717']
CM score components:
transition disagreement: 0.4608
reward disagreement: 0.0971
state disagreement: 0.4741
action disagreement: 0.5403
total CM score: 1.5722
goal is complete. CM score: 1.5722
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
2025-07-16 15:52:38,883 3218693 INFO [EPISODE END] Episode ended. Reward: -2.5728690647789818e-05, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24898446e-02, -1.12503610e-01, -1.85184675e-05],
        [ 3.25223178e-02, -4.74923605e-02,  6.49960106e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 1: 501 steps, reward: -0.814
Reset #2: mass intervention applied (success: True)
2025-07-16 15:52:39,964 3218693 INFO [EPISODE END] Episode ended. Reward: -2.5728690647789818e-05, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24898446e-02, -1.12503610e-01, -1.85184675e-05],
        [ 3.25223178e-02, -4.74923605e-02,  6.49960106e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 2: 501 steps, reward: -0.814
Reset #3: mass intervention applied (success: True)
2025-07-16 15:52:41,041 3218693 INFO [EPISODE END] Episode ended. Reward: -2.572869064770655e-05, Info: {'desired_goal': array([[[-0.0325,  0.0475,  0.    ],
        [ 0.0325,  0.1125,  0.065 ]]]), 'achieved_goal': array([[[-3.24898446e-02, -1.12503610e-01, -1.85184675e-05],
        [ 3.25223178e-02, -4.74923605e-02,  6.49960106e-02]]]), 'success': False, 'fractional_success': 0.0}
episode 3: 501 steps, reward: -0.814
Traceback (most recent call last):
  File "meta_teacher_student.py", line 878, in <module>
  File "meta_teacher_student.py", line 840, in main
    student_performance_after, avg_reward_after = evaluate_student_performance(student_model, validation_env)
  File "meta_teacher_student.py", line 493, in get_teacher_state
    cm_score = evaluate_cm_score(intervened_env, student_model, device=device, intervention_type=intervention['type'])
  File "meta_teacher_student.py", line 220, in evaluate_cm_score
    next_obs, rew, done, info = env.step(act)
  File "meta_teacher_student.py", line 182, in step
    result = self.base_env.step(action)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/causalworld.py", line 268, in step
    self._robot.apply_action(action)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/robot/trifinger.py", line 350, in apply_action
    self.step_simulation()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/robot/trifinger.py", line 317, in step_simulation
    physicsClientId=self._pybullet_client_full_id)
KeyboardInterrupt
