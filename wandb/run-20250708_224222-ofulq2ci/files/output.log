[AutoCaLC-CPDRL] Evaluating intervention: goal
autocalc_cpdrl.py:126: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
  Reward: -0.3922, CM: 2.4702
[AutoCaLC-CPDRL] Evaluating intervention: mass
  Reward: -0.6950, CM: 2.1985
[AutoCaLC-CPDRL] Evaluating intervention: friction
  Reward: -0.3785, CM: 2.4816
[AutoCaLC-CPDRL] Evaluating intervention: visual
  Reward: -0.2587, CM: 2.5736
[AutoCaLC-CPDRL] Evaluating intervention: joints
WARNING:root:Applying intervention lead to infeasibility of the robot
WARNING:root:Invalid Intervention was just executed!
  Reward: -0.1179, CM: 2.4082
[AutoCaLC-CPDRL] Evaluating intervention: pose
  Reward: -0.2588, CM: 2.2928
[AutoCaLC-CPDRL] Evaluating intervention: random
  Reward: -0.1227, CM: 2.4036

[AutoCaLC-CPDRL] Adaptive weights: w_p=0.508, w_cm=0.492

[AutoCaLC-CPDRL] Intervention ranking (adaptive weights):
  1. visual | Score: 1.1350 | Reward: -0.2587 | CM: 2.5736
  2. joints | Score: 1.1251 | Reward: -0.1179 | CM: 2.4082
  3. random | Score: 1.1204 | Reward: -0.1227 | CM: 2.4036
  4. friction | Score: 1.0289 | Reward: -0.3785 | CM: 2.4816
  5. goal | Score: 1.0163 | Reward: -0.3922 | CM: 2.4702
  6. pose | Score: 0.9968 | Reward: -0.2588 | CM: 2.2928
  7. mass | Score: 0.7288 | Reward: -0.6950 | CM: 2.1985

[AutoCaLC-CPDRL] === Intervention 'visual' just got applied (rank 1) ===
[AutoCaLC-CPDRL] Initial CM score: 2.5441
[AutoCaLC-CPDRL] PPO training phase 1 for intervention 'visual'...
Logging to autocalc_cpdrl_logs/PPO_6
[AutoCaLC-CPDRL] Episode 1: reward=0.0505 (Intervention: visual, Rank: 1)
[34m[1mwandb[0m: [33mWARNING[0m Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
[AutoCaLC-CPDRL] Episode 2: reward=-0.0218 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 3: reward=0.0662 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 4: reward=0.0687 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 5: reward=0.0619 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 6: reward=0.0469 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 7: reward=0.0784 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 8: reward=0.0179 (Intervention: visual, Rank: 1)
-----------------------------
| time/              |      |
|    fps             | 356  |
|    iterations      | 1    |
|    time_elapsed    | 5    |
|    total_timesteps | 2048 |
-----------------------------
[AutoCaLC-CPDRL] Episode 9: reward=0.0804 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 10: reward=0.0133 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0463
[AutoCaLC-CPDRL] Episode 11: reward=0.0484 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 12: reward=0.0030 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 13: reward=-0.0081 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 14: reward=0.0579 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 15: reward=0.0842 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 16: reward=-0.0493 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.037801716 |
|    clip_fraction        | 0.482       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.554      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.116      |
|    std                  | 2.66        |
|    value_loss           | 0.0426      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 17: reward=0.1087 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 18: reward=0.0382 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 19: reward=0.0034 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 20: reward=0.0496 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0336
[AutoCaLC-CPDRL] Episode 21: reward=0.1006 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 22: reward=0.0472 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 23: reward=-0.0127 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 24: reward=-0.0063 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 3           |
|    time_elapsed         | 31          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.047504127 |
|    clip_fraction        | 0.537       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.556      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.113      |
|    std                  | 2.67        |
|    value_loss           | 0.0218      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 25: reward=0.0169 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 26: reward=0.0620 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 27: reward=0.0774 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 28: reward=0.1118 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 29: reward=0.0863 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 30: reward=-0.0146 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0469
[AutoCaLC-CPDRL] Episode 31: reward=-0.0069 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 32: reward=-0.0523 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 182         |
|    iterations           | 4           |
|    time_elapsed         | 44          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.043570876 |
|    clip_fraction        | 0.531       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.533      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.109      |
|    std                  | 2.69        |
|    value_loss           | 0.035       |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 33: reward=-0.0239 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 34: reward=0.0228 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 35: reward=0.0191 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 36: reward=-0.0259 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 37: reward=0.0306 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 38: reward=0.0196 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 39: reward=0.0158 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 40: reward=-0.0050 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0006
-----------------------------------------
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 5           |
|    time_elapsed         | 57          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.049657904 |
|    clip_fraction        | 0.512       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.525      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.107      |
|    std                  | 2.71        |
|    value_loss           | 0.0431      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 41: reward=-0.0123 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 42: reward=0.0128 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 43: reward=0.0873 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 44: reward=0.0877 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 45: reward=0.0573 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 46: reward=-0.0235 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 47: reward=0.0142 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 48: reward=0.0017 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 174         |
|    iterations           | 6           |
|    time_elapsed         | 70          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.042623565 |
|    clip_fraction        | 0.491       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.545      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0982     |
|    std                  | 2.75        |
|    value_loss           | 0.0266      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 49: reward=-0.0234 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 50: reward=-0.0628 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0139
[AutoCaLC-CPDRL] Episode 51: reward=0.0426 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 52: reward=0.0239 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 53: reward=0.0423 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 54: reward=0.0462 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 55: reward=-0.0636 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 56: reward=0.0517 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 57: reward=0.0403 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 7           |
|    time_elapsed         | 83          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.037964568 |
|    clip_fraction        | 0.453       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.512      |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 2.77        |
|    value_loss           | 0.0601      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 58: reward=-0.0368 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 59: reward=0.0310 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 60: reward=0.0400 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0218
[AutoCaLC-CPDRL] Episode 61: reward=0.0159 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 62: reward=-0.0601 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 63: reward=0.0181 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 64: reward=-0.0626 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 65: reward=0.0130 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 8           |
|    time_elapsed         | 95          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.050668206 |
|    clip_fraction        | 0.504       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.542      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.107      |
|    std                  | 2.8         |
|    value_loss           | 0.0406      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 66: reward=-0.0356 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 67: reward=0.0384 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 68: reward=0.0427 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 69: reward=0.0434 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 70: reward=-0.1096 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0096
[AutoCaLC-CPDRL] Episode 71: reward=0.0498 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 72: reward=-0.0360 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 73: reward=-0.0905 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 172        |
|    iterations           | 9          |
|    time_elapsed         | 106        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.05970875 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.567     |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.111     |
|    std                  | 2.81       |
|    value_loss           | 0.0214     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 74: reward=-0.0018 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 75: reward=-0.0486 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 76: reward=0.0320 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 77: reward=-0.0375 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 78: reward=0.0745 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 79: reward=0.0707 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 80: reward=-0.0984 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0086
[AutoCaLC-CPDRL] Episode 81: reward=-0.0085 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 172        |
|    iterations           | 10         |
|    time_elapsed         | 119        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.04228936 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.559     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.83       |
|    value_loss           | 0.0222     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 82: reward=0.0952 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 83: reward=-0.0044 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 84: reward=-0.0112 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 85: reward=-0.0026 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 86: reward=0.0136 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 87: reward=-0.0958 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 88: reward=-0.1131 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 89: reward=0.0355 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 172        |
|    iterations           | 11         |
|    time_elapsed         | 130        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05764263 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.533     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.086     |
|    std                  | 2.87       |
|    value_loss           | 0.0518     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 90: reward=-0.0962 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0188
[AutoCaLC-CPDRL] Episode 91: reward=0.0243 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 92: reward=-0.0116 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 93: reward=-0.1252 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 94: reward=0.0008 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 95: reward=0.0387 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 96: reward=0.0524 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 97: reward=-0.0394 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 173        |
|    iterations           | 12         |
|    time_elapsed         | 141        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04765893 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.552     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.88       |
|    value_loss           | 0.0274     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 98: reward=-0.0431 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 99: reward=-0.0256 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 100: reward=0.0348 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0094
[AutoCaLC-CPDRL] Episode 101: reward=-0.1070 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 102: reward=0.0592 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 103: reward=-0.0440 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 104: reward=-0.0603 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 105: reward=0.0192 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 106: reward=-0.0471 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 173         |
|    iterations           | 13          |
|    time_elapsed         | 153         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.048455507 |
|    clip_fraction        | 0.53        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.567      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.106      |
|    std                  | 2.93        |
|    value_loss           | 0.0275      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 107: reward=0.0313 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 108: reward=-0.0031 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 109: reward=-0.0097 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 110: reward=-0.0071 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0169
[AutoCaLC-CPDRL] Episode 111: reward=-0.0664 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 112: reward=0.0417 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 113: reward=0.0119 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 114: reward=0.0812 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 172        |
|    iterations           | 14         |
|    time_elapsed         | 166        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06950668 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.562     |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.105     |
|    std                  | 2.95       |
|    value_loss           | 0.0549     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 115: reward=0.0283 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 116: reward=0.1282 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 117: reward=0.0080 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 118: reward=-0.0436 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 119: reward=0.0217 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 120: reward=-0.0663 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0145
[AutoCaLC-CPDRL] Episode 121: reward=-0.0078 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 122: reward=-0.0151 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 171        |
|    iterations           | 15         |
|    time_elapsed         | 179        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.05555522 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.572     |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.101     |
|    std                  | 3          |
|    value_loss           | 0.0195     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 123: reward=-0.0053 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 124: reward=0.0112 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 125: reward=0.0088 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 126: reward=-0.1060 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 127: reward=-0.0657 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 128: reward=-0.0042 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 129: reward=-0.0290 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 130: reward=-0.0177 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0231
-----------------------------------------
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 16          |
|    time_elapsed         | 191         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.048671223 |
|    clip_fraction        | 0.542       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.566      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.106      |
|    std                  | 3.03        |
|    value_loss           | 0.0279      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 131: reward=0.0576 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 132: reward=0.0231 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 133: reward=-0.0114 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 134: reward=-0.0168 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 135: reward=-0.0109 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 136: reward=-0.0155 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 137: reward=-0.0632 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 138: reward=-0.0576 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 17         |
|    time_elapsed         | 204        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07863188 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.55      |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.101     |
|    std                  | 3.04       |
|    value_loss           | 0.0552     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 139: reward=0.0390 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 140: reward=-0.0221 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0078
[AutoCaLC-CPDRL] Episode 141: reward=0.0292 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 142: reward=0.0300 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 143: reward=0.0637 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 144: reward=-0.0199 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 145: reward=-0.0340 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 146: reward=0.0182 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 170         |
|    iterations           | 18          |
|    time_elapsed         | 216         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.050403178 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.9       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.562      |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.101      |
|    std                  | 3.1         |
|    value_loss           | 0.0419      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 147: reward=0.0323 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 148: reward=0.0759 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 149: reward=0.0074 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 150: reward=0.0375 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0240
[AutoCaLC-CPDRL] Episode 151: reward=-0.0332 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 152: reward=0.0330 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 153: reward=0.0278 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 154: reward=0.0699 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 155: reward=0.0855 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 19         |
|    time_elapsed         | 227        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.05946914 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.57      |
|    n_updates            | 1425       |
|    policy_gradient_loss | -0.107     |
|    std                  | 3.14       |
|    value_loss           | 0.039      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 156: reward=-0.0048 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 157: reward=0.0278 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 158: reward=0.0094 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 159: reward=-0.0044 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 160: reward=-0.0347 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0176
[AutoCaLC-CPDRL] Episode 161: reward=-0.0835 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 162: reward=0.0475 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 163: reward=0.0747 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 171        |
|    iterations           | 20         |
|    time_elapsed         | 239        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.06797266 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.583     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.111     |
|    std                  | 3.2        |
|    value_loss           | 0.0329     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 164: reward=-0.1041 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 165: reward=0.0031 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 166: reward=0.0153 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 167: reward=-0.0705 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 168: reward=-0.0613 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 169: reward=0.0157 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 170: reward=-0.0795 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0243
[AutoCaLC-CPDRL] Episode 171: reward=-0.0045 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 21         |
|    time_elapsed         | 252        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06874631 |
|    clip_fraction        | 0.639      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.3      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.575     |
|    n_updates            | 1455       |
|    policy_gradient_loss | -0.112     |
|    std                  | 3.26       |
|    value_loss           | 0.0398     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 172: reward=-0.0491 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 173: reward=0.0164 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 174: reward=0.0252 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 175: reward=0.0373 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 176: reward=0.0360 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 177: reward=-0.0661 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 178: reward=-0.0354 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 179: reward=0.0654 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 22         |
|    time_elapsed         | 264        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.05217287 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.4      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.576     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.112     |
|    std                  | 3.27       |
|    value_loss           | 0.0434     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 180: reward=0.0126 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0038
[AutoCaLC-CPDRL] Episode 181: reward=0.0341 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 182: reward=-0.1334 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 183: reward=0.0573 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 184: reward=0.0039 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 185: reward=0.0019 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 186: reward=-0.0053 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 187: reward=-0.0772 (Intervention: visual, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 169        |
|    iterations           | 23         |
|    time_elapsed         | 277        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06626767 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.561     |
|    n_updates            | 1485       |
|    policy_gradient_loss | -0.0941    |
|    std                  | 3.34       |
|    value_loss           | 0.0475     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 188: reward=0.0238 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 189: reward=0.0352 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 190: reward=-0.0560 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0116
[AutoCaLC-CPDRL] Episode 191: reward=-0.0156 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 192: reward=-0.0661 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 193: reward=0.0006 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 194: reward=0.0243 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 195: reward=0.0280 (Intervention: visual, Rank: 1)
--------------------------------------
| time/                   |          |
|    fps                  | 170      |
|    iterations           | 24       |
|    time_elapsed         | 288      |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.066824 |
|    clip_fraction        | 0.587    |
|    clip_range           | 0.2      |
|    entropy_loss         | -23.7    |
|    explained_variance   | 0.512    |
|    learning_rate        | 0.00025  |
|    loss                 | -0.584   |
|    n_updates            | 1500     |
|    policy_gradient_loss | -0.11    |
|    std                  | 3.4      |
|    value_loss           | 0.0472   |
--------------------------------------
[AutoCaLC-CPDRL] Episode 196: reward=-0.0458 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 197: reward=0.0685 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 198: reward=-0.0405 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 199: reward=-0.0959 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 200: reward=0.0266 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): -0.0116
[AutoCaLC-CPDRL] Episode 201: reward=-0.0018 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 202: reward=-0.0167 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 203: reward=0.0827 (Intervention: visual, Rank: 1)
---------------------------------------
| time/                   |           |
|    fps                  | 170       |
|    iterations           | 25        |
|    time_elapsed         | 300       |
|    total_timesteps      | 51200     |
| train/                  |           |
|    approx_kl            | 0.0606414 |
|    clip_fraction        | 0.572     |
|    clip_range           | 0.2       |
|    entropy_loss         | -23.9     |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.591    |
|    n_updates            | 1515      |
|    policy_gradient_loss | -0.106    |
|    std                  | 3.5       |
|    value_loss           | 0.0304    |
---------------------------------------
[AutoCaLC-CPDRL] Episode 204: reward=0.0811 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 205: reward=0.0966 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 206: reward=0.0659 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 207: reward=0.0242 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 208: reward=-0.0564 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 209: reward=0.0214 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 210: reward=-0.0074 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Mean reward (last 10): 0.0290
[AutoCaLC-CPDRL] Episode 211: reward=-0.0233 (Intervention: visual, Rank: 1)
[AutoCaLC-CPDRL] Episode 212: reward=-0.0051 (Intervention: visual, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 170         |
|    iterations           | 26          |
|    time_elapsed         | 313         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.048922792 |
|    clip_fraction        | 0.489       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.569      |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.088      |
|    std                  | 3.55        |
|    value_loss           | 0.0405      |
-----------------------------------------
Traceback (most recent call last):
  File "autocalc_cpdrl.py", line 473, in <module>
    main()
  File "autocalc_cpdrl.py", line 448, in main
    pretrained_path, env, args.timesteps, args.log_dir, args.seed, ppo_config, policy_kwargs, transfer_model=transfer_model, callback=callback
  File "autocalc_cpdrl.py", line 322, in ppo_train_on_intervention
    model.learn(total_timesteps=timesteps, callback=callback)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 270, in learn
    self.train()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 215, in train
    clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()
KeyboardInterrupt
