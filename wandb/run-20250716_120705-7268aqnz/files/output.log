
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 10
   student training steps: 10000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:227: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
State VAE losses: ['152221.0344', '117240.6141', '71164.7233', '270286.6962', '307002.3050']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.6897
action disagreement: 0.5153
total CM score: 1.9381
goal is complete. CM score: 1.9381
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
State VAE losses: ['344474.0588', '145299.3028', '107908.4731', '87360.4478', '96935.5528']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.6385
action disagreement: 0.5139
total CM score: 1.6871
mass is complete. CM score: 1.6871
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
State VAE losses: ['57567.4314', '149008.6075', '227217.8612', '158482.0663', '93930.0434']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.5950
action disagreement: 0.5360
total CM score: 1.6920
friction is complete. CM score: 1.6920
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
State VAE losses: ['114260.2831', '134223.6891', '63234.7337', '176502.9763', '108256.8953']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.6181
action disagreement: 0.5367
total CM score: 1.5618
visual is complete. CM score: 1.5618
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
State VAE losses: ['111178.3425', '146861.8856', '102894.9375', '106227.8913', '125547.7681']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5702
action disagreement: 0.5396
total CM score: 1.6458
pose is complete. CM score: 1.6458
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
State VAE losses: ['191008.5269', '186898.3337', '249006.9175', '205124.2450', '120893.2041']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.6935
action disagreement: 0.5467
total CM score: 2.0510
random is complete. CM score: 2.0510
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 278       |
|    iterations      | 1         |
|    time_elapsed    | 14        |
|    total_timesteps | 5050368   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 179        |
|    iterations           | 2          |
|    time_elapsed         | 45         |
|    total_timesteps      | 5054464    |
| train/                  |            |
|    approx_kl            | 0.07661824 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.391     |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 2.71       |
|    value_loss           | 0.967      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 161         |
|    iterations           | 3           |
|    time_elapsed         | 76          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.082165435 |
|    clip_fraction        | 0.497       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 2.74        |
|    value_loss           | 0.233       |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-0.529, success=False
episode 2: length=501, reward=-0.529, success=False
episode 3: length=501, reward=-0.529, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.529
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 1.211
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -4.597
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.321
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.653
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8139', '8.3840', '7.4632', '7.7983', '8.2479']
Training reward models...
Reward model losses: ['0.0070', '0.1873', '0.0226', '1.2905', '1.9432']
Training state VAE models...
State VAE losses: ['372878.1575', '371477.3613', '109559.7597', '194805.2956', '316313.2100']
Training action VAE models...
Action VAE losses: ['1.3664', '1.3491', '1.3878', '1.4321', '1.4796']
CM score components:
transition disagreement: 0.4527
reward disagreement: 0.4897
state disagreement: 0.7390
action disagreement: 0.5429
total CM score: 2.2243
goal is complete. CM score: 2.2243
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.776
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 0.960
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.335
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.646
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7901', '7.4058', '8.1565', '7.5496', '6.8763']
Training reward models...
Reward model losses: ['0.5048', '0.5136', '0.0171', '0.1977', '0.1992']
Training state VAE models...
State VAE losses: ['75847.7625', '101066.4122', '78914.4947', '193127.7894', '123186.9288']
Training action VAE models...
Action VAE losses: ['1.4007', '1.4121', '1.2986', '1.4541', '1.4689']
CM score components:
transition disagreement: 0.4510
reward disagreement: 0.2856
state disagreement: 0.6111
action disagreement: 0.5359
total CM score: 1.8836
mass is complete. CM score: 1.8836
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.428
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.145
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.456
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.422
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7255', '7.3530', '7.3551', '6.8776', '8.0426']
Training reward models...
Reward model losses: ['0.7098', '0.1099', '0.0327', '0.0529', '0.2626']
Training state VAE models...
State VAE losses: ['92008.9500', '131086.9675', '112566.0306', '233195.4075', '154031.4281']
Training action VAE models...
Action VAE losses: ['1.4191', '1.4343', '1.3859', '1.4457', '1.4730']
CM score components:
transition disagreement: 0.4217
reward disagreement: 0.2601
state disagreement: 0.6717
action disagreement: 0.5470
total CM score: 1.9005
friction is complete. CM score: 1.9005
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -0.529
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.529
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -0.529
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.529
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7703', '7.5384', '7.7411', '7.6892', '7.3529']
Training reward models...
Reward model losses: ['0.5774', '0.5690', '0.0079', '0.0047', '0.1671']
Training state VAE models...
State VAE losses: ['187429.6288', '121109.8297', '107861.2916', '78721.1113', '207618.6400']
Training action VAE models...
Action VAE losses: ['1.3947', '1.4263', '1.6450', '1.4297', '1.3746']
CM score components:
transition disagreement: 0.4084
reward disagreement: 0.2512
state disagreement: 0.5728
action disagreement: 0.5818
total CM score: 1.8143
visual is complete. CM score: 1.8143
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 2.330
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.572
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.536
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.425
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9353', '7.2144', '8.2229', '8.0715', '6.7258']
Training reward models...
Reward model losses: ['0.3855', '0.0125', '0.1418', '0.0131', '0.0222']
Training state VAE models...
State VAE losses: ['175709.6831', '148635.2575', '100637.2703', '149154.9781', '129333.8456']
Training action VAE models...
Action VAE losses: ['1.4474', '1.4105', '1.3476', '1.4441', '1.4399']
CM score components:
transition disagreement: 0.4287
reward disagreement: 0.0922
state disagreement: 0.6449
action disagreement: 0.5175
total CM score: 1.6833
pose is complete. CM score: 1.6833
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.720
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.726
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.087
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.581
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7537', '7.6662', '6.9746', '7.8864', '7.9396']
Training reward models...
Reward model losses: ['0.8278', '0.4002', '0.1158', '1.0617', '0.0392']
Training state VAE models...
State VAE losses: ['185237.3556', '129576.5688', '222931.1025', '197128.0850', '101757.8813']
Training action VAE models...
Action VAE losses: ['1.4451', '1.5636', '1.3979', '1.3912', '1.3494']
CM score components:
transition disagreement: 0.4212
reward disagreement: 0.4614
state disagreement: 0.6209
action disagreement: 0.5568
total CM score: 2.0603
random is complete. CM score: 2.0603
INFO:root:Meta-Episode 1/10: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 1.211
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -4.597
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.321
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.653
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8475', '7.1836', '7.4591', '7.3193', '7.2293']
Training reward models...
Reward model losses: ['0.2449', '0.0999', '0.7838', '0.4785', '0.1477']
Training state VAE models...
State VAE losses: ['88696.9803', '126741.0434', '190910.6713', '256338.0663', '94478.7225']
Training action VAE models...
Action VAE losses: ['1.2963', '1.3152', '1.3242', '1.4807', '1.3768']
CM score components:
transition disagreement: 0.3706
reward disagreement: 0.2902
state disagreement: 0.6409
action disagreement: 0.5397
total CM score: 1.8414
goal is complete. CM score: 1.8414
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.776
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 0.960
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.335
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.646
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4549', '7.5404', '7.5177', '7.1412', '7.3066']
Training reward models...
Reward model losses: ['0.2839', '0.2708', '0.0111', '0.2625', '0.0138']
Training state VAE models...
State VAE losses: ['126798.8344', '143578.0916', '149795.4838', '171415.1744', '121318.5625']
Training action VAE models...
Action VAE losses: ['1.3159', '1.4306', '1.4689', '1.4107', '1.6319']
CM score components:
transition disagreement: 0.4163
reward disagreement: 0.1257
state disagreement: 0.6734
action disagreement: 0.5866
total CM score: 1.8020
mass is complete. CM score: 1.8020
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.428
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.145
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.456
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.422
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1204', '7.2288', '7.2652', '7.1224', '7.6179']
Training reward models...
Reward model losses: ['0.1389', '0.2233', '0.0160', '0.0114', '0.0167']
Training state VAE models...
State VAE losses: ['75617.9372', '119252.1987', '162515.4275', '85889.7756', '119520.0716']
Training action VAE models...
Action VAE losses: ['1.3184', '1.4745', '1.4192', '1.6270', '1.3099']
CM score components:
transition disagreement: 0.4341
reward disagreement: 0.0860
state disagreement: 0.6038
action disagreement: 0.5620
total CM score: 1.6858
friction is complete. CM score: 1.6858
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -0.529
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.529
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -0.529
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.529
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9426', '7.2647', '7.3421', '6.9701', '7.3374']
Training reward models...
Reward model losses: ['0.6786', '0.0435', '0.0043', '0.0147', '0.0191']
Training state VAE models...
State VAE losses: ['97962.0437', '275695.3613', '145300.1181', '128990.6119', '115784.3666']
Training action VAE models...
Action VAE losses: ['1.4283', '1.3409', '1.4833', '1.4127', '1.3864']
CM score components:
transition disagreement: 0.4345
reward disagreement: 0.1680
state disagreement: 0.6474
action disagreement: 0.5610
total CM score: 1.8109
visual is complete. CM score: 1.8109
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 2.330
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.572
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.536
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.425
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.6942', '7.6484', '8.2433', '7.1545', '7.8850']
Training reward models...
Reward model losses: ['0.1817', '0.5282', '0.0138', '0.1903', '0.0682']
Training state VAE models...
State VAE losses: ['141796.2809', '103195.2653', '315681.0350', '74877.5058', '114858.2194']
Training action VAE models...
Action VAE losses: ['1.4781', '1.4613', '1.4160', '1.4317', '1.3990']
CM score components:
transition disagreement: 0.4232
reward disagreement: 0.1857
state disagreement: 0.6734
action disagreement: 0.5803
total CM score: 1.8626
pose is complete. CM score: 1.8626
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.720
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.726
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.087
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.581
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2973', '7.9420', '7.3424', '7.4756', '7.3790']
Training reward models...
Reward model losses: ['0.3690', '0.1566', '0.1307', '0.1815', '0.0492']
Training state VAE models...
State VAE losses: ['246518.2131', '111263.8847', '119761.1347', '122606.7266', '177574.6656']
Training action VAE models...
Action VAE losses: ['1.3467', '1.6113', '1.4854', '1.4525', '1.4976']
CM score components:
transition disagreement: 0.4248
reward disagreement: 0.1363
state disagreement: 0.6480
action disagreement: 0.6129
total CM score: 1.8219
random is complete. CM score: 1.8219
IntervenedCausalWorld created with pose intervention
Reset #1: pose intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: pose intervention applied (success: True)
Reset #3: pose intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 290       |
|    iterations      | 1         |
|    time_elapsed    | 14        |
|    total_timesteps | 5062656   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 182        |
|    iterations           | 2          |
|    time_elapsed         | 44         |
|    total_timesteps      | 5066752    |
| train/                  |            |
|    approx_kl            | 0.05378409 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.545     |
|    n_updates            | 1215       |
|    policy_gradient_loss | -0.0863    |
|    std                  | 2.78       |
|    value_loss           | 0.0614     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 164        |
|    iterations           | 3          |
|    time_elapsed         | 74         |
|    total_timesteps      | 5070848    |
| train/                  |            |
|    approx_kl            | 0.06323108 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.537     |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0963    |
|    std                  | 2.8        |
|    value_loss           | 0.0542     |
----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=0.759, success=False
episode 2: length=501, reward=0.759, success=False
episode 3: length=501, reward=0.759, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 0.759
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 2.683
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -1.786
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -1.814
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.194
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8343', '7.9657', '7.5724', '7.2337', '7.2312']
Training reward models...
Reward model losses: ['0.0527', '0.6340', '0.0774', '0.2398', '0.1646']
Training state VAE models...
State VAE losses: ['99963.6566', '139905.9153', '157456.5887', '189563.1919', '237214.0850']
Training action VAE models...
Action VAE losses: ['1.3143', '1.4721', '1.4056', '1.3745', '1.4726']
CM score components:
transition disagreement: 0.3892
reward disagreement: 0.1866
state disagreement: 0.6744
action disagreement: 0.5351
total CM score: 1.7853
goal is complete. CM score: 1.7853
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -2.742
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -2.336
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -2.656
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.877
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2882', '7.5695', '8.4670', '6.9760', '7.9984']
Training reward models...
Reward model losses: ['0.0172', '0.2491', '0.1596', '0.0302', '0.2811']
Training state VAE models...
State VAE losses: ['207636.9375', '175350.2181', '127625.5331', '156496.3469', '77628.0606']
Training action VAE models...
Action VAE losses: ['1.2358', '1.3502', '1.2871', '1.5016', '1.2379']
CM score components:
transition disagreement: 0.4136
reward disagreement: 0.1935
state disagreement: 0.6025
action disagreement: 0.5613
total CM score: 1.7709
mass is complete. CM score: 1.7709
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -2.608
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -4.437
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -3.041
total data points collected: 2505
average episode length: 501.0
average episode reward: -3.286
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1052', '7.4586', '7.2743', '7.4610', '7.3060']
Training reward models...
Reward model losses: ['0.0676', '0.0271', '0.0427', '0.0415', '0.0405']
Training state VAE models...
State VAE losses: ['194611.2475', '138190.8184', '113487.5725', '69935.4581', '172155.3419']
Training action VAE models...
Action VAE losses: ['1.4744', '1.3894', '1.4563', '1.3323', '1.4685']
CM score components:
transition disagreement: 0.4745
reward disagreement: 0.1386
state disagreement: 0.5912
action disagreement: 0.5354
total CM score: 1.7396
friction is complete. CM score: 1.7396
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 0.759
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 0.759
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.759
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2741', '8.0899', '6.8938', '7.4717', '6.8784']
Training reward models...
Reward model losses: ['0.0876', '0.6008', '0.0701', '0.1707', '0.1505']
Training state VAE models...
State VAE losses: ['197646.3875', '153239.0175', '180682.6175', '70073.9305', '96953.9041']
Training action VAE models...
Action VAE losses: ['1.3386', '1.4539', '1.3541', '1.2950', '1.3182']
CM score components:
transition disagreement: 0.4158
reward disagreement: 0.2133
state disagreement: 0.6453
action disagreement: 0.5379
total CM score: 1.8123
visual is complete. CM score: 1.8123
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: -2.721
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -1.195
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -3.780
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.934
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8520', '6.5245', '7.2483', '7.3516', '7.7474']
Training reward models...
Reward model losses: ['0.1149', '0.0624', '0.0080', '0.0120', '0.3871']
Training state VAE models...
State VAE losses: ['181507.6269', '156554.7775', '225721.7356', '240360.6812', '87891.6931']
Training action VAE models...
Action VAE losses: ['1.4425', '1.3935', '1.4365', '1.4426', '1.3186']
CM score components:
transition disagreement: 0.4707
reward disagreement: 0.1482
state disagreement: 0.6755
action disagreement: 0.5552
total CM score: 1.8495
pose is complete. CM score: 1.8495
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 0.056
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -6.020
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.391
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.964
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2887', '7.7085', '7.1523', '7.8989', '6.9161']
Training reward models...
Reward model losses: ['0.0320', '0.0401', '0.0285', '0.4288', '0.2381']
Training state VAE models...
State VAE losses: ['122326.0272', '159477.0438', '62077.1064', '123784.4116', '89908.6987']
Training action VAE models...
Action VAE losses: ['1.4739', '1.4134', '1.5060', '1.4984', '1.2336']
CM score components:
transition disagreement: 0.4260
reward disagreement: 0.2000
state disagreement: 0.6208
action disagreement: 0.5954
total CM score: 1.8422
random is complete. CM score: 1.8422
INFO:root:Meta-Episode 2/10: Teacher chose 'pose', Reward: 0.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 2.683
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -1.786
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -1.814
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.194
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2827', '7.8782', '7.6111', '6.9377', '7.0692']
Training reward models...
Reward model losses: ['1.0120', '0.0927', '0.2408', '0.0401', '0.4066']
Training state VAE models...
State VAE losses: ['125479.2453', '102857.8047', '205868.7787', '100302.2316', '228638.9769']
Training action VAE models...
Action VAE losses: ['1.3227', '1.4015', '1.6062', '1.6392', '1.4955']
CM score components:
transition disagreement: 0.4330
reward disagreement: 0.3372
state disagreement: 0.6932
action disagreement: 0.6164
total CM score: 2.0798
goal is complete. CM score: 2.0798
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -2.742
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -2.336
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -2.656
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.877
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2526', '7.5219', '8.1687', '8.0536', '7.0976']
Training reward models...
Reward model losses: ['1.0965', '0.0891', '0.0042', '0.1052', '0.0098']
Training state VAE models...
State VAE losses: ['104705.9072', '102508.8184', '221869.0981', '154444.5869', '160886.2981']
Training action VAE models...
Action VAE losses: ['1.2471', '1.4217', '1.4358', '1.3027', '1.3320']
CM score components:
transition disagreement: 0.4190
reward disagreement: 0.3143
state disagreement: 0.6747
action disagreement: 0.5393
total CM score: 1.9474
mass is complete. CM score: 1.9474
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -2.608
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -4.437
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -3.041
total data points collected: 2505
average episode length: 501.0
average episode reward: -3.286
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5552', '7.2052', '7.5667', '7.4123', '8.1564']
Training reward models...
Reward model losses: ['0.1439', '0.0504', '0.1120', '0.0577', '0.0160']
Training state VAE models...
State VAE losses: ['87651.1644', '135883.5403', '176832.6494', '96968.5619', '184170.3019']
Training action VAE models...
Action VAE losses: ['1.6081', '1.5384', '1.4870', '1.4218', '1.3846']
CM score components:
transition disagreement: 0.4126
reward disagreement: 0.1263
state disagreement: 0.6250
action disagreement: 0.5904
total CM score: 1.7543
friction is complete. CM score: 1.7543
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 0.759
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 0.759
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.759
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2148', '7.4052', '7.7730', '7.6254', '8.1604']
Training reward models...
Reward model losses: ['0.1212', '0.2132', '0.0299', '0.0957', '0.2191']
Training state VAE models...
State VAE losses: ['115821.4381', '92192.4722', '113192.9644', '185383.5150', '140577.5153']
Training action VAE models...
Action VAE losses: ['1.5287', '1.3706', '1.3886', '1.3442', '1.3880']
CM score components:
transition disagreement: 0.4063
reward disagreement: 0.1161
state disagreement: 0.5856
action disagreement: 0.5565
total CM score: 1.6645
visual is complete. CM score: 1.6645
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: -2.721
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -1.195
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -3.780
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.934
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4757', '7.4160', '7.9068', '6.8418', '7.0371']
Training reward models...
Reward model losses: ['0.0109', '0.1544', '0.5146', '0.0776', '0.3752']
Training state VAE models...
State VAE losses: ['169538.0619', '196410.8344', '81264.1559', '104173.9628', '164506.8831']
Training action VAE models...
Action VAE losses: ['1.3943', '1.3604', '1.2527', '1.5782', '1.4558']
CM score components:
transition disagreement: 0.4330
reward disagreement: 0.1716
state disagreement: 0.6084
action disagreement: 0.6097
total CM score: 1.8227
pose is complete. CM score: 1.8227
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 0.056
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -6.020
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.391
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.964
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9591', '7.1990', '7.8155', '8.0115', '7.3175']
Training reward models...
Reward model losses: ['0.0413', '0.0473', '0.0970', '0.0564', '0.9279']
Training state VAE models...
State VAE losses: ['126914.2556', '77600.1572', '134751.1584', '141806.5138', '99144.8400']
Training action VAE models...
Action VAE losses: ['1.4100', '1.2103', '1.3783', '1.3758', '1.4359']
CM score components:
transition disagreement: 0.4050
reward disagreement: 0.2736
state disagreement: 0.5803
action disagreement: 0.5558
total CM score: 1.8148
random is complete. CM score: 1.8148
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: friction intervention applied (success: True)
Reset #3: friction intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 313       |
|    iterations      | 1         |
|    time_elapsed    | 13        |
|    total_timesteps | 5074944   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 188        |
|    iterations           | 2          |
|    time_elapsed         | 43         |
|    total_timesteps      | 5079040    |
| train/                  |            |
|    approx_kl            | 0.07508504 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.552     |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.101     |
|    std                  | 2.81       |
|    value_loss           | 0.0285     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 3          |
|    time_elapsed         | 73         |
|    total_timesteps      | 5083136    |
| train/                  |            |
|    approx_kl            | 0.06513667 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.551     |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.0952    |
|    std                  | 2.82       |
|    value_loss           | 0.0211     |
----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-2.811, success=False
episode 2: length=501, reward=-2.811, success=False
episode 3: length=501, reward=-2.811, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -2.811
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -1.569
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -6.018
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 0.676
total data points collected: 2505
average episode length: 501.0
average episode reward: -3.363
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5155', '8.3064', '7.3410', '7.0140', '7.3320']
Training reward models...
Reward model losses: ['0.2846', '0.2108', '0.0320', '1.0393', '0.0403']
Training state VAE models...
State VAE losses: ['211836.3156', '205939.8619', '92687.4228', '146064.7247', '206116.8594']
Training action VAE models...
Action VAE losses: ['1.4934', '1.5541', '1.4460', '1.4452', '1.6332']
CM score components:
transition disagreement: 0.4024
reward disagreement: 0.2928
state disagreement: 0.6842
action disagreement: 0.5970
total CM score: 1.9764
goal is complete. CM score: 1.9764
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -4.260
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -3.940
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.515
total data points collected: 2505
average episode length: 501.0
average episode reward: -4.223
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2795', '7.4975', '7.8099', '7.2134', '7.2000']
Training reward models...
Reward model losses: ['0.3374', '0.0409', '0.0414', '0.0463', '0.3986']
Training state VAE models...
State VAE losses: ['77471.6447', '131300.9094', '61167.6555', '126995.3691', '168724.3244']
Training action VAE models...
Action VAE losses: ['1.3920', '1.7077', '1.3590', '1.5625', '1.4394']
CM score components:
transition disagreement: 0.4007
reward disagreement: 0.1675
state disagreement: 0.6105
action disagreement: 0.6118
total CM score: 1.7905
mass is complete. CM score: 1.7905
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -2.127
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -3.052
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -2.351
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.899
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6420', '7.2996', '7.6811', '7.7085', '7.5131']
Training reward models...
Reward model losses: ['0.1854', '0.3222', '0.8117', '0.0082', '0.1284']
Training state VAE models...
State VAE losses: ['155589.3075', '58694.0909', '155651.0887', '229345.1313', '211361.2394']
Training action VAE models...
Action VAE losses: ['1.3966', '1.3279', '1.4912', '1.4619', '1.4023']
CM score components:
transition disagreement: 0.4374
reward disagreement: 0.2477
state disagreement: 0.6884
action disagreement: 0.5399
total CM score: 1.9134
friction is complete. CM score: 1.9134
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -2.811
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -2.811
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -2.811
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.811
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4958', '8.1977', '7.6649', '6.7526', '7.5266']
Training reward models...
Reward model losses: ['0.9819', '0.0112', '0.5112', '0.0349', '0.1170']
Training state VAE models...
State VAE losses: ['217718.8981', '96657.6844', '152237.2787', '116285.8053', '132220.7056']
Training action VAE models...
Action VAE losses: ['1.3419', '1.3434', '1.3845', '1.4554', '1.5801']
CM score components:
transition disagreement: 0.4110
reward disagreement: 0.3610
state disagreement: 0.6630
action disagreement: 0.5736
total CM score: 2.0086
visual is complete. CM score: 2.0086
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.018
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -3.846
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.679
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.157
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6316', '7.7578', '7.0955', '7.5566', '7.9489']
Training reward models...
Reward model losses: ['0.2969', '0.0215', '0.0165', '0.2257', '0.0196']
Training state VAE models...
State VAE losses: ['79068.2556', '72974.4312', '116787.0500', '82455.4012', '132825.5922']
Training action VAE models...
Action VAE losses: ['1.3909', '1.5095', '1.4250', '1.5309', '1.3955']
CM score components:
transition disagreement: 0.4212
reward disagreement: 0.1321
state disagreement: 0.5875
action disagreement: 0.5738
total CM score: 1.7146
pose is complete. CM score: 1.7146
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 0.294
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.337
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.783
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.764
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8853', '7.3705', '7.3943', '7.4137', '7.2783']
Training reward models...
Reward model losses: ['0.4115', '0.1476', '0.0262', '0.8396', '0.0424']
Training state VAE models...
State VAE losses: ['194020.1700', '102654.0894', '78198.2713', '200184.5269', '77153.2000']
Training action VAE models...
Action VAE losses: ['1.2881', '1.3284', '1.3078', '1.2863', '1.3464']
CM score components:
transition disagreement: 0.4236
reward disagreement: 0.2917
state disagreement: 0.6118
action disagreement: 0.5156
total CM score: 1.8427
random is complete. CM score: 1.8427
INFO:root:Meta-Episode 3/10: Teacher chose 'friction', Reward: 0.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -1.569
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -6.018
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 0.676
total data points collected: 2505
average episode length: 501.0
average episode reward: -3.363
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1832', '7.3333', '8.1003', '7.5951', '7.8860']
Training reward models...
Reward model losses: ['0.0273', '0.0515', '0.1987', '0.0302', '0.6658']
Training state VAE models...
State VAE losses: ['196161.7925', '137662.4300', '111024.5759', '200616.3519', '278138.9144']
Training action VAE models...
Action VAE losses: ['1.3705', '1.4565', '1.5278', '1.4021', '1.4460']
CM score components:
transition disagreement: 0.4283
reward disagreement: 0.2611
state disagreement: 0.7605
action disagreement: 0.5637
total CM score: 2.0135
goal is complete. CM score: 2.0135
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -4.260
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -3.940
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.515
total data points collected: 2505
average episode length: 501.0
average episode reward: -4.223
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6635', '7.0906', '6.2568', '7.6386', '7.2319']
Training reward models...
Reward model losses: ['0.2446', '0.4287', '0.0436', '0.4209', '0.0383']
Training state VAE models...
State VAE losses: ['123463.6131', '101999.5331', '95657.0591', '227595.1094', '100000.5441']
Training action VAE models...
Action VAE losses: ['1.5340', '1.4058', '1.4868', '1.5050', '1.4743']
CM score components:
transition disagreement: 0.4241
reward disagreement: 0.2328
state disagreement: 0.6387
action disagreement: 0.5750
total CM score: 1.8706
mass is complete. CM score: 1.8706
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -2.127
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -3.052
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -2.351
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.899
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.3955', '7.4530', '7.4010', '7.6585', '7.1530']
Training reward models...
Reward model losses: ['0.1407', '0.1709', '0.0429', '0.9034', '0.1630']
Training state VAE models...
State VAE losses: ['109640.3203', '113934.1150', '110109.5525', '124862.7319', '135157.4525']
Training action VAE models...
Action VAE losses: ['1.6093', '1.4918', '1.4681', '1.3496', '1.4102']
CM score components:
transition disagreement: 0.4175
reward disagreement: 0.3420
state disagreement: 0.5926
action disagreement: 0.5601
total CM score: 1.9121
friction is complete. CM score: 1.9121
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -2.811
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -2.811
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -2.811
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.811
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6075', '7.7030', '7.7232', '8.0260', '7.3975']
Training reward models...
Reward model losses: ['0.1123', '0.0309', '0.4091', '0.2396', '0.1571']
Training state VAE models...
State VAE losses: ['73272.2730', '155845.9806', '260371.6525', '120387.0150', '164097.9394']
Training action VAE models...
Action VAE losses: ['1.3743', '1.3715', '1.6954', '1.4086', '1.4565']
CM score components:
transition disagreement: 0.3944
reward disagreement: 0.1763
state disagreement: 0.6216
action disagreement: 0.5793
total CM score: 1.7717
visual is complete. CM score: 1.7717
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.018
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -3.846
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.679
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.157
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6910', '6.7286', '6.9544', '7.4299', '7.6440']
Training reward models...
Reward model losses: ['0.0589', '0.0598', '0.1545', '0.0165', '0.0268']
Training state VAE models...
State VAE losses: ['79355.7184', '237081.4950', '65037.8444', '156477.8006', '111961.3597']
Training action VAE models...
Action VAE losses: ['1.4101', '1.5219', '1.3414', '1.5057', '1.3544']
CM score components:
transition disagreement: 0.3961
reward disagreement: 0.0929
state disagreement: 0.6401
action disagreement: 0.5597
total CM score: 1.6889
pose is complete. CM score: 1.6889
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 0.294
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -4.337
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.783
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.764
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8442', '7.8557', '7.2808', '6.5653', '7.7838']
Training reward models...
Reward model losses: ['0.4313', '0.2580', '0.0177', '0.0399', '0.3624']
Training state VAE models...
State VAE losses: ['191806.2925', '340422.4050', '133811.7019', '182985.8681', '141733.5344']
Training action VAE models...
Action VAE losses: ['1.2080', '1.3197', '1.4258', '1.3994', '1.3144']
CM score components:
transition disagreement: 0.4112
reward disagreement: 0.2259
state disagreement: 0.7069
action disagreement: 0.5398
total CM score: 1.8838
random is complete. CM score: 1.8838
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 319       |
|    iterations      | 1         |
|    time_elapsed    | 12        |
|    total_timesteps | 5087232   |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 251       |
|    ep_rew_mean          | 2.2542775 |
| time/                   |           |
|    fps                  | 189       |
|    iterations           | 2         |
|    time_elapsed         | 43        |
|    total_timesteps      | 5091328   |
| train/                  |           |
|    approx_kl            | 0.079061  |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.00025   |
|    loss                 | -0.567    |
|    n_updates            | 1305      |
|    policy_gradient_loss | -0.105    |
|    std                  | 2.85      |
|    value_loss           | 0.021     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 251       |
|    ep_rew_mean          | 2.2542775 |
| time/                   |           |
|    fps                  | 167       |
|    iterations           | 3         |
|    time_elapsed         | 73        |
|    total_timesteps      | 5095424   |
| train/                  |           |
|    approx_kl            | 0.1078891 |
|    clip_fraction        | 0.576     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.00025   |
|    loss                 | -0.558    |
|    n_updates            | 1320      |
|    policy_gradient_loss | -0.106    |
|    std                  | 2.86      |
|    value_loss           | 0.0144    |
---------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=-1.595, success=False
episode 2: length=501, reward=-1.595, success=False
episode 3: length=501, reward=-1.595, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -1.595
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.361
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -2.243
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -2.249
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.771
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8190', '6.9543', '7.3028', '6.4750', '7.8054']
Training reward models...
Reward model losses: ['1.1318', '0.0342', '0.1155', '0.0768', '0.0329']
Training state VAE models...
State VAE losses: ['88826.9891', '106277.1906', '81390.5759', '67586.3817', '114469.6891']
Training action VAE models...
Action VAE losses: ['1.4588', '1.3449', '1.3699', '1.3786', '1.2184']
CM score components:
transition disagreement: 0.4210
reward disagreement: 0.3364
state disagreement: 0.5408
action disagreement: 0.5689
total CM score: 1.8671
goal is complete. CM score: 1.8671
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -3.439
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -0.304
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -2.349
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.081
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3807', '7.9222', '7.1677', '7.1836', '6.5816']
Training reward models...
Reward model losses: ['0.0496', '0.0674', '0.0271', '0.0444', '0.0745']
Training state VAE models...
State VAE losses: ['149777.9919', '79396.0772', '136821.6403', '124177.6072', '65380.8908']
Training action VAE models...
Action VAE losses: ['1.2239', '1.2276', '1.2928', '1.2644', '1.3565']
CM score components:
transition disagreement: 0.3777
reward disagreement: 0.1453
state disagreement: 0.6093
action disagreement: 0.5649
total CM score: 1.6973
mass is complete. CM score: 1.6973
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -4.228
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.308
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -2.569
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.360
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5512', '7.5259', '7.1401', '6.8755', '8.0614']
Training reward models...
Reward model losses: ['0.0094', '0.8370', '0.0958', '0.0112', '0.0105']
Training state VAE models...
State VAE losses: ['114009.5303', '172729.2375', '150164.4306', '104970.7281', '245396.4887']
Training action VAE models...
Action VAE losses: ['1.3555', '1.4100', '1.3093', '1.5279', '1.5469']
CM score components:
transition disagreement: 0.3872
reward disagreement: 0.2639
state disagreement: 0.6585
action disagreement: 0.5651
total CM score: 1.8747
friction is complete. CM score: 1.8747
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -1.595
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -1.595
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.595
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.595
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2180', '7.2089', '7.2792', '7.8843', '7.1575']
Training reward models...
Reward model losses: ['1.0880', '0.0724', '0.0047', '0.0215', '0.0076']
Training state VAE models...
State VAE losses: ['148313.3944', '174663.3094', '111706.3550', '124835.4022', '221795.7162']
Training action VAE models...
Action VAE losses: ['1.3521', '1.4223', '1.2498', '1.2622', '1.2383']
CM score components:
transition disagreement: 0.3778
reward disagreement: 0.2980
state disagreement: 0.6414
action disagreement: 0.5371
total CM score: 1.8544
visual is complete. CM score: 1.8544
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 3.167
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -0.449
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -3.142
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.772
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9258', '7.7060', '7.4326', '8.2240', '7.6814']
Training reward models...
Reward model losses: ['0.0249', '0.1522', '0.0210', '0.1047', '0.0777']
Training state VAE models...
State VAE losses: ['115703.4019', '224501.7550', '209862.9019', '109289.7975', '129349.8444']
Training action VAE models...
Action VAE losses: ['1.2297', '1.2979', '1.1934', '1.1739', '1.1811']
CM score components:
transition disagreement: 0.4593
reward disagreement: 0.0917
state disagreement: 0.5975
action disagreement: 0.5573
total CM score: 1.7058
pose is complete. CM score: 1.7058
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -4.087
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -2.442
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.854
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.059
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7939', '7.5220', '7.7842', '7.6351', '7.4125']
Training reward models...
Reward model losses: ['0.0291', '0.4135', '0.0559', '0.0292', '0.3601']
Training state VAE models...
State VAE losses: ['148167.8712', '139920.1666', '128503.9478', '78265.5269', '106824.5000']
Training action VAE models...
Action VAE losses: ['1.4306', '1.4221', '1.2632', '1.4534', '1.4587']
CM score components:
transition disagreement: 0.4267
reward disagreement: 0.2563
state disagreement: 0.5999
action disagreement: 0.5803
total CM score: 1.8632
random is complete. CM score: 1.8632
INFO:root:Meta-Episode 4/10: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.361
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -2.243
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -2.249
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.771
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8094', '7.9095', '6.9372', '7.4363', '7.4385']
Training reward models...
Reward model losses: ['0.1258', '0.1705', '0.2280', '0.8270', '0.4996']
Training state VAE models...
State VAE losses: ['148441.2944', '65669.1198', '157884.4769', '256525.0713', '161233.9813']
Training action VAE models...
Action VAE losses: ['1.3579', '1.2202', '1.2778', '1.6297', '1.6129']
CM score components:
transition disagreement: 0.4134
reward disagreement: 0.2205
state disagreement: 0.6817
action disagreement: 0.5749
total CM score: 1.8905
goal is complete. CM score: 1.8905
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -3.439
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -0.304
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -2.349
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.081
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.3959', '7.9524', '7.5634', '7.8672', '7.1019']
Training reward models...
Reward model losses: ['0.1184', '0.0146', '0.9979', '0.2111', '0.0069']
Training state VAE models...
State VAE losses: ['175489.5900', '143591.8797', '116477.4653', '136228.1894', '121516.7644']
Training action VAE models...
Action VAE losses: ['1.2530', '1.3101', '1.1876', '1.1194', '1.2399']
CM score components:
transition disagreement: 0.4025
reward disagreement: 0.2977
state disagreement: 0.6266
action disagreement: 0.5274
total CM score: 1.8543
mass is complete. CM score: 1.8543
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -4.228
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -1.308
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -2.569
total data points collected: 2505
average episode length: 501.0
average episode reward: -2.360
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.5471', '7.8190', '7.0007', '7.3424', '7.4147']
Training reward models...
Reward model losses: ['0.0718', '0.8069', '0.1277', '0.0906', '0.1048']
Training state VAE models...
State VAE losses: ['154131.5287', '67462.6155', '106071.6313', '102878.0206', '148477.8325']
Training action VAE models...
Action VAE losses: ['1.4088', '1.3431', '1.3661', '1.3956', '1.3771']
CM score components:
transition disagreement: 0.3797
reward disagreement: 0.2617
state disagreement: 0.6419
action disagreement: 0.5558
total CM score: 1.8391
friction is complete. CM score: 1.8391
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -1.595
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -1.595
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.595
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.595
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6487', '7.2542', '7.0787', '7.9476', '7.8824']
Training reward models...
Reward model losses: ['0.0748', '0.0042', '1.3657', '0.1890', '0.9526']
Training state VAE models...
State VAE losses: ['139923.1978', '84028.0453', '131268.8578', '181569.8337', '133797.3075']
Training action VAE models...
Action VAE losses: ['1.3003', '1.1708', '1.2924', '1.4335', '1.3605']
CM score components:
transition disagreement: 0.4573
reward disagreement: 0.4032
state disagreement: 0.5780
action disagreement: 0.5410
total CM score: 1.9795
visual is complete. CM score: 1.9795
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 3.167
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: -0.449
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -3.142
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.772
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9354', '7.7966', '6.8866', '7.2018', '7.4552']
Training reward models...
Reward model losses: ['0.2156', '0.0147', '0.0438', '0.1420', '0.0563']
Training state VAE models...
State VAE losses: ['135951.7131', '106018.2944', '144636.2328', '140454.6059', '197575.3456']
Training action VAE models...
Action VAE losses: ['1.2133', '1.1655', '1.1756', '1.4632', '1.3881']
CM score components:
transition disagreement: 0.3890
reward disagreement: 0.1282
state disagreement: 0.6399
action disagreement: 0.5808
total CM score: 1.7378
pose is complete. CM score: 1.7378
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -4.087
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -2.442
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.854
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.059
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.3030', '7.5079', '7.9055', '6.6070', '6.8940']
Training reward models...
Reward model losses: ['0.1588', '0.2276', '0.0234', '0.1046', '0.1138']
Training state VAE models...
State VAE losses: ['223848.5862', '274316.5037', '66481.1712', '91466.8525', '291298.9469']
Training action VAE models...
Action VAE losses: ['1.4860', '1.3142', '1.4623', '1.2948', '1.3747']
CM score components:
transition disagreement: 0.4395
reward disagreement: 0.1104
state disagreement: 0.6651
action disagreement: 0.5604
total CM score: 1.7753
random is complete. CM score: 1.7753
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: mass intervention applied (success: True)
Reset #3: mass intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 313       |
|    iterations      | 1         |
|    time_elapsed    | 13        |
|    total_timesteps | 5099520   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 2           |
|    time_elapsed         | 43          |
|    total_timesteps      | 5103616     |
| train/                  |             |
|    approx_kl            | 0.079531476 |
|    clip_fraction        | 0.551       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.567      |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.101      |
|    std                  | 2.91        |
|    value_loss           | 0.0127      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 168        |
|    iterations           | 3          |
|    time_elapsed         | 73         |
|    total_timesteps      | 5107712    |
| train/                  |            |
|    approx_kl            | 0.07596334 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.566     |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.105     |
|    std                  | 2.95       |
|    value_loss           | 0.0191     |
----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=0.713, success=False
episode 2: length=501, reward=0.713, success=False
episode 3: length=501, reward=0.713, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 0.713
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -1.469
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -2.862
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -2.261
total data points collected: 2505
average episode length: 501.0
average episode reward: -1.584
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5764', '8.0654', '7.2914', '7.9134', '7.6813']
Training reward models...
Reward model losses: ['0.0649', '0.1280', '0.0833', '0.0633', '0.7800']
Training state VAE models...
State VAE losses: ['189950.2819', '181581.6281', '167294.1506', '245740.3094', '148531.7844']
Training action VAE models...
Action VAE losses: ['1.2639', '1.3219', '1.5010', '1.4950', '1.3165']
CM score components:
transition disagreement: 0.4301
reward disagreement: 0.2665
state disagreement: 0.7474
action disagreement: 0.5504
total CM score: 1.9943
goal is complete. CM score: 1.9943
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: -2.297
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -3.155
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 1.308
Traceback (most recent call last):
  File "meta_teacher_student.py", line 695, in <module>
    main()
  File "meta_teacher_student.py", line 671, in main
    S_prime_teacher = get_teacher_state(student_model, args.task, INTERVENTIONS, device=device, seed=args.seed)
  File "meta_teacher_student.py", line 413, in get_teacher_state
    cm_score = evaluate_cm_score(intervened_env, student_model, device=device, intervention_type=intervention['type'])
  File "meta_teacher_student.py", line 200, in evaluate_cm_score
    next_obs, rew, done, info = env.step(act)
  File "meta_teacher_student.py", line 167, in step
    return self.base_env.step(action)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/causalworld.py", line 275, in step
    reward = self._task.get_reward()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/task_generators/base_task.py", line 690, in get_reward
    desired_goal=self._current_desired_goal)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/task_generators/pushing.py", line 225, in _calculate_dense_rewards
    current_angle_diff = 2 * np.arccos(np.clip(quat_diff[:, 3], -1., 1.))
  File "<__array_function__ internals>", line 6, in clip
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/numpy/core/fromnumeric.py", line 2115, in clip
    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/numpy/core/_methods.py", line 137, in _clip
    if _clip_dep_is_scalar_nan(max):
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/numpy/core/_methods.py", line 94, in _clip_dep_is_scalar_nan
    if ndim(a) != 0:
  File "<__array_function__ internals>", line 6, in ndim
KeyboardInterrupt
