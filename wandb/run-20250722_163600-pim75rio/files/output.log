2025-07-22 16:36:02,898 774412 INFO [PRETRAINED] Using pretrained model path: ppo_pushing_sb3/final_model.zip
2025-07-22 16:36:02,901 774412 INFO Starting with 7 interventions
2025-07-22 16:36:02,901 774412 INFO ===final evaluation===
2025-07-22 16:36:13,437 774412 INFO Final performance
2025-07-22 16:36:13,438 774412 INFO average reward: 3.394 +/- 0.000
2025-07-22 16:36:13,438 774412 INFO success rate: 1.000
2025-07-22 16:36:13,438 774412 INFO average episode length: 501.0
2025-07-22 16:36:13,438 774412 INFO initial performance: {'avg_reward': 3.393721938342554, 'reward_std': 4.440892098500626e-16, 'avg_length': 501.0, 'success_rate': 1.0, 'total_episodes': 10}
2025-07-22 16:36:13,439 774412 INFO CURRICULUM STAGE 1/7
2025-07-22 16:36:13,439 774412 INFO Remaining interventions: ['goal', 'mass', 'friction', 'visual', 'position', 'angle', 'random']
2025-07-22 16:36:13,439 774412 INFO
Testing intervention 1/7: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.176
total data points collected: 501
average episode length: 50.1
average episode reward: -0.018
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.340
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.216
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.679
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.149
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.008
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.141
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.376
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.420
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.463
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.595
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.421
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
baselines.py:200: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3214', '7.0977', '6.8085', '7.1494', '7.6413']
Training reward models...
Reward model losses: ['0.2433', '0.2068', '0.2358', '0.0093', '0.0194']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2406', '1.2135', '1.2528', '1.3185', '1.3497']
Training action VAE models...
Action VAE losses: ['1.5555', '1.4313', '1.4925', '1.3706', '1.3680']
CM score components:
transition disagreement: 0.4095
reward disagreement: 0.0940
state disagreement: 0.4722
action disagreement: 0.5613
total CM score: 1.5369
2025-07-22 16:36:24,205 774412 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 16:36:25,252 774412 INFO Episode 1: reward=-1.076, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 16:36:26,291 774412 INFO Episode 2: reward=1.394, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 16:36:26,387 774412 INFO Episode 3: reward=1.403, length=22, success=True
2025-07-22 16:36:31,665 774412 INFO Results: avg_reward=1.109, success_rate=0.300, avg_length=357.4
2025-07-22 16:36:31,667 774412 INFO
Testing intervention 2/7: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 33 steps, reward: -0.196
total data points collected: 33
average episode length: 3.3
average episode reward: -0.020
termination reasons: ['success']
success rate: 1/10
Reset #2: mass intervention applied (success: True)
episode 2: 33 steps, reward: -0.226
total data points collected: 66
average episode length: 6.6
average episode reward: -0.042
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.175
total data points collected: 567
average episode length: 56.7
average episode reward: 0.275
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 606
average episode length: 60.6
average episode reward: 0.171
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 869
average episode length: 86.9
average episode reward: 0.407
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 1370
average episode length: 137.0
average episode reward: 0.560
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 1871
average episode length: 187.1
average episode reward: 0.995
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 2372
average episode length: 237.2
average episode reward: 1.482
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 2410
average episode length: 241.0
average episode reward: 1.536
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 5/10
total data points collected: 2533
average episode length: 253.3
average episode reward: 1.517
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 6/10
tensor shapes - states: torch.Size([2533, 56]), actions: torch.Size([2533, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0618', '7.7143', '7.1078', '6.8695', '8.2497']
Training reward models...
Reward model losses: ['0.0085', '0.3513', '0.0077', '0.3001', '0.5900']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8094', '1.8174', '1.8916', '1.9281', '2.4819']
Training action VAE models...
Action VAE losses: ['1.4794', '1.4375', '1.3607', '1.4333', '1.3518']
CM score components:
transition disagreement: 0.4201
reward disagreement: 0.2101
state disagreement: 0.6020
action disagreement: 0.5512
total CM score: 1.7833
2025-07-22 16:36:37,896 774412 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 16:36:38,011 774412 INFO Episode 1: reward=-0.763, length=28, success=True
Reset #2: mass intervention applied (success: True)
2025-07-22 16:36:38,123 774412 INFO Episode 2: reward=-0.352, length=31, success=True
Reset #3: mass intervention applied (success: True)
2025-07-22 16:36:38,233 774412 INFO Episode 3: reward=-0.216, length=31, success=True
2025-07-22 16:36:39,938 774412 INFO Results: avg_reward=0.001, success_rate=0.900, avg_length=77.0
2025-07-22 16:36:39,940 774412 INFO
Testing intervention 3/7: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 40 steps, reward: 0.452
total data points collected: 40
average episode length: 4.0
average episode reward: 0.045
termination reasons: ['success']
success rate: 1/10
Reset #2: friction intervention applied (success: True)
episode 2: 40 steps, reward: -0.118
total data points collected: 80
average episode length: 8.0
average episode reward: 0.033
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.317
total data points collected: 581
average episode length: 58.1
average episode reward: 0.365
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 613
average episode length: 61.3
average episode reward: 0.354
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 877
average episode length: 87.7
average episode reward: 0.609
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 1378
average episode length: 137.8
average episode reward: 1.002
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 1879
average episode length: 187.9
average episode reward: 1.386
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 1961
average episode length: 196.1
average episode reward: 1.438
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success']
success rate: 5/10
total data points collected: 1996
average episode length: 199.6
average episode reward: 1.429
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success', 'success']
success rate: 6/10
total data points collected: 2497
average episode length: 249.7
average episode reward: 1.617
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 6/10
tensor shapes - states: torch.Size([2497, 56]), actions: torch.Size([2497, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9008', '7.2530', '6.8730', '7.7103', '7.0574']
Training reward models...
Reward model losses: ['0.0653', '1.1019', '0.5024', '0.0242', '0.2265']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7264', '1.5889', '1.5984', '1.8298', '1.7181']
Training action VAE models...
Action VAE losses: ['1.2996', '1.3674', '1.2929', '1.3067', '1.4268']
CM score components:
transition disagreement: 0.4215
reward disagreement: 0.4378
state disagreement: 0.5306
action disagreement: 0.5179
total CM score: 1.9078
2025-07-22 16:36:46,101 774412 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 16:36:46,219 774412 INFO Episode 1: reward=-0.879, length=28, success=True
Reset #2: friction intervention applied (success: True)
2025-07-22 16:36:46,324 774412 INFO Episode 2: reward=-0.777, length=27, success=True
Reset #3: friction intervention applied (success: True)
2025-07-22 16:36:46,428 774412 INFO Episode 3: reward=-0.775, length=27, success=True
2025-07-22 16:36:47,154 774412 INFO Results: avg_reward=-0.826, success_rate=1.000, avg_length=27.4
2025-07-22 16:36:47,156 774412 INFO
Testing intervention 4/7: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.799
total data points collected: 501
average episode length: 50.1
average episode reward: 0.380
termination reasons: ['max_length']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.047
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.685
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: visual intervention applied (success: True)
episode 3: 401 steps, reward: 4.829
total data points collected: 1403
average episode length: 140.3
average episode reward: 1.167
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1904
average episode length: 190.4
average episode reward: 1.487
termination reasons: ['max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 1.396
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 2/10
total data points collected: 1978
average episode length: 197.8
average episode reward: 1.337
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 2010
average episode length: 201.0
average episode reward: 1.159
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 2511
average episode length: 251.1
average episode reward: 1.386
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 3012
average episode length: 301.2
average episode reward: 1.803
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 3513
average episode length: 351.3
average episode reward: 2.134
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 4/10
tensor shapes - states: torch.Size([3513, 56]), actions: torch.Size([3513, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1150', '7.7619', '7.0914', '7.1919', '7.4913']
Training reward models...
Reward model losses: ['0.0551', '0.2885', '0.1062', '0.0198', '0.0581']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6096', '1.7050', '1.6790', '1.7611', '1.6296']
Training action VAE models...
Action VAE losses: ['1.4084', '1.3043', '1.3701', '1.4613', '1.5215']
CM score components:
transition disagreement: 0.4395
reward disagreement: 0.1506
state disagreement: 0.5061
action disagreement: 0.5374
total CM score: 1.6336
2025-07-22 16:36:55,116 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 16:36:55,231 774412 INFO Episode 1: reward=-0.773, length=27, success=True
Reset #2: visual intervention applied (success: True)
2025-07-22 16:36:55,336 774412 INFO Episode 2: reward=-0.773, length=27, success=True
Reset #3: visual intervention applied (success: True)
2025-07-22 16:36:55,440 774412 INFO Episode 3: reward=-0.773, length=27, success=True
2025-07-22 16:36:56,169 774412 INFO Results: avg_reward=-0.773, success_rate=1.000, avg_length=27.0
2025-07-22 16:36:56,171 774412 INFO
Testing intervention 5/7: position (CM score)
IntervenedCausalWorld created with position intervention
evaluating CM score for position intervention...
Reset #1: position intervention applied (success: True)
episode 1: 35 steps, reward: -1.696
total data points collected: 35
average episode length: 3.5
average episode reward: -0.170
termination reasons: ['success']
success rate: 1/10
Reset #2: position intervention applied (success: True)
episode 2: 501 steps, reward: -3.449
total data points collected: 536
average episode length: 53.6
average episode reward: -0.515
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: position intervention applied (success: True)
episode 3: 501 steps, reward: 2.055
total data points collected: 1037
average episode length: 103.7
average episode reward: -0.309
termination reasons: ['success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 1538
average episode length: 153.8
average episode reward: -0.190
termination reasons: ['success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2039
average episode length: 203.9
average episode reward: -0.029
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2540
average episode length: 254.0
average episode reward: 0.394
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3041
average episode length: 304.1
average episode reward: 0.628
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3542
average episode length: 354.2
average episode reward: 0.782
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 0.803
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 4078
average episode length: 407.8
average episode reward: 1.155
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4078, 56]), actions: torch.Size([4078, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7852', '7.7244', '7.6858', '6.6319', '7.3930']
Training reward models...
Reward model losses: ['3.0782', '1.1042', '0.1575', '0.0203', '0.0340']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6499', '1.7088', '1.5572', '1.6576', '1.4569']
Training action VAE models...
Action VAE losses: ['1.3798', '1.5109', '1.4191', '1.4644', '1.3449']
CM score components:
transition disagreement: 0.4450
reward disagreement: 0.7511
state disagreement: 0.5280
action disagreement: 0.5551
total CM score: 2.2792
2025-07-22 16:37:05,246 774412 INFO testing intervention: position
IntervenedCausalWorld created with position intervention
Reset #1: position intervention applied (success: True)
2025-07-22 16:37:06,297 774412 INFO Episode 1: reward=3.566, length=501, success=False
Reset #2: position intervention applied (success: True)
2025-07-22 16:37:06,405 774412 INFO Episode 2: reward=-1.829, length=28, success=True
Reset #3: position intervention applied (success: True)
2025-07-22 16:37:07,388 774412 INFO Episode 3: reward=2.727, length=501, success=False
2025-07-22 16:37:14,438 774412 INFO Results: avg_reward=-0.018, success_rate=0.100, avg_length=453.7
2025-07-22 16:37:14,440 774412 INFO
Testing intervention 6/7: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 338 steps, reward: 2.353
total data points collected: 338
average episode length: 33.8
average episode reward: 0.235
termination reasons: ['success']
success rate: 1/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: 0.355
total data points collected: 839
average episode length: 83.9
average episode reward: 0.271
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 33 steps, reward: -1.702
total data points collected: 872
average episode length: 87.2
average episode reward: 0.101
termination reasons: ['success', 'max_length', 'success']
success rate: 2/10
total data points collected: 1201
average episode length: 120.1
average episode reward: 0.305
termination reasons: ['success', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 1702
average episode length: 170.2
average episode reward: 0.311
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length']
success rate: 3/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 0.525
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 4/10
total data points collected: 1975
average episode length: 197.5
average episode reward: 0.541
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success']
success rate: 5/10
total data points collected: 2476
average episode length: 247.6
average episode reward: 0.835
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 5/10
total data points collected: 2516
average episode length: 251.6
average episode reward: 0.395
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 6/10
total data points collected: 3017
average episode length: 301.7
average episode reward: 0.351
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length']
success rate: 6/10
tensor shapes - states: torch.Size([3017, 56]), actions: torch.Size([3017, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4578', '7.3447', '8.3567', '7.3963', '7.3306']
Training reward models...
Reward model losses: ['0.1261', '0.0766', '0.0544', '0.3345', '0.1984']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4926', '1.4158', '1.3596', '1.4830', '1.5776']
Training action VAE models...
Action VAE losses: ['1.4831', '1.4249', '1.4275', '1.4818', '1.4880']
CM score components:
transition disagreement: 0.3766
reward disagreement: 0.1347
state disagreement: 0.5085
action disagreement: 0.5841
total CM score: 1.6039
2025-07-22 16:37:21,498 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 16:37:22,513 774412 INFO Episode 1: reward=0.846, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 16:37:23,548 774412 INFO Episode 2: reward=-2.998, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 16:37:24,555 774412 INFO Episode 3: reward=1.373, length=501, success=False
2025-07-22 16:37:30,635 774412 INFO Results: avg_reward=-0.090, success_rate=0.100, avg_length=453.9
2025-07-22 16:37:30,637 774412 INFO
Testing intervention 7/7: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 225 steps, reward: 4.185
total data points collected: 225
average episode length: 22.5
average episode reward: 0.419
termination reasons: ['success']
success rate: 1/10
Reset #2: random intervention applied (success: True)
episode 2: 46 steps, reward: 1.844
total data points collected: 271
average episode length: 27.1
average episode reward: 0.603
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.957
total data points collected: 772
average episode length: 77.2
average episode reward: 0.507
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 1273
average episode length: 127.3
average episode reward: 0.240
termination reasons: ['success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 1774
average episode length: 177.4
average episode reward: 0.578
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2275
average episode length: 227.5
average episode reward: 0.730
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2776
average episode length: 277.6
average episode reward: 0.988
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3038
average episode length: 303.8
average episode reward: 1.275
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3539
average episode length: 353.9
average episode reward: 1.057
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 1.048
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3577, 56]), actions: torch.Size([3577, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1869', '7.1633', '7.7889', '7.8317', '8.8476']
Training reward models...
Reward model losses: ['0.2678', '0.0461', '0.3041', '0.0354', '0.0399']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2985', '1.3248', '1.3389', '1.2574', '1.2420']
Training action VAE models...
Action VAE losses: ['1.4852', '1.5103', '1.3810', '1.4729', '1.4495']
CM score components:
transition disagreement: 0.4526
reward disagreement: 0.1635
state disagreement: 0.4906
action disagreement: 0.5830
total CM score: 1.6898
2025-07-22 16:37:38,733 774412 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 16:37:39,723 774412 INFO Episode 1: reward=1.516, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 16:37:40,776 774412 INFO Episode 2: reward=-1.410, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 16:37:41,843 774412 INFO Episode 3: reward=7.979, length=501, success=False
2025-07-22 16:37:48,914 774412 INFO Results: avg_reward=1.897, success_rate=0.000, avg_length=501.0
2025-07-22 16:37:48,915 774412 INFO Best intervention for stage 1: position (Unified score: 0.6484)
2025-07-22 16:37:48,916 774412 INFO === stage 1/7: training on position intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_1_position
IntervenedCausalWorld created with position intervention
Reset #1: position intervention applied (success: True)
Reset #2: position intervention applied (success: True)
Reset #3: position intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | position  |
|    stage             | 1         |
| rollout/             |           |
|    ep_len_mean       | 268       |
|    ep_rew_mean       | 2.0396163 |
| time/                |           |
|    fps               | 452       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5050368   |
------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.9497699   |
| time/                   |             |
|    fps                  | 276         |
|    iterations           | 2           |
|    time_elapsed         | 29          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.044647623 |
|    clip_fraction        | 0.493       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.108      |
|    std                  | 2.67        |
|    value_loss           | 0.0252      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 1.6769154   |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 3           |
|    time_elapsed         | 50          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.037631687 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.534      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0902     |
|    std                  | 2.7         |
|    value_loss           | 0.0208      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 1.5872107   |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 4           |
|    time_elapsed         | 70          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.065796316 |
|    clip_fraction        | 0.565       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.105      |
|    std                  | 2.71        |
|    value_loss           | 0.0299      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 339         |
|    ep_rew_mean          | 1.3940446   |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 5           |
|    time_elapsed         | 91          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.043427818 |
|    clip_fraction        | 0.434       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.547      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0917     |
|    std                  | 2.73        |
|    value_loss           | 0.0314      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 1.2106693   |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 6           |
|    time_elapsed         | 111         |
|    total_timesteps      | 5070848     |
| train/                  |             |
|    approx_kl            | 0.046077747 |
|    clip_fraction        | 0.47        |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.541      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 2.75        |
|    value_loss           | 0.0462      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 1.0472485   |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 7           |
|    time_elapsed         | 132         |
|    total_timesteps      | 5074944     |
| train/                  |             |
|    approx_kl            | 0.052667886 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.546      |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.102      |
|    std                  | 2.78        |
|    value_loss           | 0.0315      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 0.85194606  |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 8           |
|    time_elapsed         | 152         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.061902195 |
|    clip_fraction        | 0.549       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.56       |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.106      |
|    std                  | 2.81        |
|    value_loss           | 0.0199      |
-----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | position  |
|    stage                | 1         |
| rollout/                |           |
|    ep_len_mean          | 412       |
|    ep_rew_mean          | 0.7065226 |
| time/                   |           |
|    fps                  | 212       |
|    iterations           | 9         |
|    time_elapsed         | 173       |
|    total_timesteps      | 5083136   |
| train/                  |           |
|    approx_kl            | 0.3833355 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.558    |
|    n_updates            | 1275      |
|    policy_gradient_loss | -0.11     |
|    std                  | 2.83      |
|    value_loss           | 0.0188    |
---------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 0.3684297  |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 10         |
|    time_elapsed         | 194        |
|    total_timesteps      | 5087232    |
| train/                  |            |
|    approx_kl            | 0.04994037 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00025    |
|    loss                 | -0.565     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.11      |
|    std                  | 2.85       |
|    value_loss           | 0.0199     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 448        |
|    ep_rew_mean          | 0.24491644 |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 11         |
|    time_elapsed         | 214        |
|    total_timesteps      | 5091328    |
| train/                  |            |
|    approx_kl            | 0.06606398 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.557     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.104     |
|    std                  | 2.9        |
|    value_loss           | 0.028      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 0.30077672 |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 12         |
|    time_elapsed         | 234        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.06117303 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.578     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.115     |
|    std                  | 2.94       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 436         |
|    ep_rew_mean          | 0.3670481   |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 13          |
|    time_elapsed         | 255         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.078939214 |
|    clip_fraction        | 0.624       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.583      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.115      |
|    std                  | 2.99        |
|    value_loss           | 0.0146      |
-----------------------------------------
2025-07-22 16:42:15,900 774412 INFO model saved to cm_sequencing_logs/model_stage_1_position
2025-07-22 16:42:15,902 774412 INFO
Completed stage 1. Intervention 'position' removed from list.
2025-07-22 16:42:15,902 774412 INFO Remaining interventions: 6
2025-07-22 16:42:15,902 774412 INFO CURRICULUM STAGE 2/7
2025-07-22 16:42:15,902 774412 INFO Remaining interventions: ['goal', 'mass', 'friction', 'visual', 'angle', 'random']
2025-07-22 16:42:15,903 774412 INFO
Testing intervention 1/6: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 0.463
total data points collected: 501
average episode length: 50.1
average episode reward: 0.046
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 295 steps, reward: 2.686
total data points collected: 796
average episode length: 79.6
average episode reward: 0.315
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -1.758
total data points collected: 1297
average episode length: 129.7
average episode reward: 0.139
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1798
average episode length: 179.8
average episode reward: 0.152
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2299
average episode length: 229.9
average episode reward: 0.354
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2800
average episode length: 280.0
average episode reward: 0.337
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3301
average episode length: 330.1
average episode reward: 0.306
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3802
average episode length: 380.2
average episode reward: 0.438
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4303
average episode length: 430.3
average episode reward: 0.637
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4804
average episode length: 480.4
average episode reward: 0.248
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4804, 56]), actions: torch.Size([4804, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4697', '7.2781', '7.6518', '8.5692', '7.9831']
Training reward models...
Reward model losses: ['0.0168', '0.1472', '1.7272', '0.2074', '0.1726']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3364', '1.5354', '1.3675', '1.2900', '1.4034']
Training action VAE models...
Action VAE losses: ['1.3965', '1.4227', '1.3653', '1.3740', '1.3704']
CM score components:
transition disagreement: 0.4291
reward disagreement: 0.4628
state disagreement: 0.5345
action disagreement: 0.5425
total CM score: 1.9687
2025-07-22 16:42:26,334 774412 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 16:42:27,289 774412 INFO Episode 1: reward=0.922, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 16:42:28,244 774412 INFO Episode 2: reward=2.176, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 16:42:29,193 774412 INFO Episode 3: reward=7.417, length=501, success=False
2025-07-22 16:42:36,108 774412 INFO Results: avg_reward=3.192, success_rate=0.000, avg_length=501.0
2025-07-22 16:42:36,109 774412 INFO
Testing intervention 2/6: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.560
total data points collected: 501
average episode length: 50.1
average episode reward: 0.356
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 3.396
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.696
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.286
total data points collected: 1503
average episode length: 150.3
average episode reward: 1.124
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 1632
average episode length: 163.2
average episode reward: 1.243
termination reasons: ['max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1672
average episode length: 167.2
average episode reward: 1.273
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 2173
average episode length: 217.3
average episode reward: 1.398
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 2674
average episode length: 267.4
average episode reward: 1.412
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3175
average episode length: 317.5
average episode reward: 1.727
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3204
average episode length: 320.4
average episode reward: 1.798
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3242
average episode length: 324.2
average episode reward: 1.894
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3242, 56]), actions: torch.Size([3242, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2166', '7.6009', '7.0086', '7.6727', '6.4510']
Training reward models...
Reward model losses: ['0.0171', '0.0340', '0.1850', '0.0238', '0.2576']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9954', '1.8801', '2.0157', '1.9890', '1.9011']
Training action VAE models...
Action VAE losses: ['1.4605', '1.4743', '1.3381', '1.2991', '1.5320']
CM score components:
transition disagreement: 0.4383
reward disagreement: 0.1509
state disagreement: 0.5499
action disagreement: 0.5441
total CM score: 1.6832
2025-07-22 16:42:43,597 774412 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 16:42:44,563 774412 INFO Episode 1: reward=3.415, length=501, success=False
Reset #2: mass intervention applied (success: True)
2025-07-22 16:42:45,524 774412 INFO Episode 2: reward=4.401, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 16:42:46,601 774412 INFO Episode 3: reward=1.694, length=501, success=False
2025-07-22 16:42:53,326 774412 INFO Results: avg_reward=4.084, success_rate=0.000, avg_length=501.0
2025-07-22 16:42:53,327 774412 INFO
Testing intervention 3/6: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.782
total data points collected: 501
average episode length: 50.1
average episode reward: -0.078
termination reasons: ['max_length']
success rate: 0/10
Reset #2: friction intervention applied (success: True)
episode 2: 264 steps, reward: 1.648
total data points collected: 765
average episode length: 76.5
average episode reward: 0.087
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 1.547
total data points collected: 1266
average episode length: 126.6
average episode reward: 0.241
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1767
average episode length: 176.7
average episode reward: 0.212
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2268
average episode length: 226.8
average episode reward: 0.051
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2769
average episode length: 276.9
average episode reward: -0.224
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2808
average episode length: 280.8
average episode reward: -0.281
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 2844
average episode length: 284.4
average episode reward: -0.179
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 2872
average episode length: 287.2
average episode reward: -0.166
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 3373
average episode length: 337.3
average episode reward: 0.124
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 4/10
tensor shapes - states: torch.Size([3373, 56]), actions: torch.Size([3373, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9254', '7.5901', '7.2962', '6.8289', '7.5785']
Training reward models...
Reward model losses: ['0.0088', '0.0502', '0.2647', '0.2440', '0.2046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6881', '1.5163', '1.6501', '1.9112', '1.5810']
Training action VAE models...
Action VAE losses: ['1.5875', '1.4398', '1.4791', '1.4553', '1.4522']
CM score components:
transition disagreement: 0.4272
reward disagreement: 0.1618
state disagreement: 0.5529
action disagreement: 0.5811
total CM score: 1.7229
2025-07-22 16:43:01,003 774412 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 16:43:01,950 774412 INFO Episode 1: reward=4.724, length=501, success=False
Reset #2: friction intervention applied (success: True)
2025-07-22 16:43:02,889 774412 INFO Episode 2: reward=4.681, length=501, success=False
Reset #3: friction intervention applied (success: True)
2025-07-22 16:43:03,831 774412 INFO Episode 3: reward=4.625, length=501, success=False
2025-07-22 16:43:10,423 774412 INFO Results: avg_reward=4.632, success_rate=0.000, avg_length=501.0
2025-07-22 16:43:10,425 774412 INFO
Testing intervention 4/6: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 55 steps, reward: 1.031
total data points collected: 55
average episode length: 5.5
average episode reward: 0.103
termination reasons: ['success']
success rate: 1/10
Reset #2: visual intervention applied (success: True)
episode 2: 31 steps, reward: 0.100
total data points collected: 86
average episode length: 8.6
average episode reward: 0.113
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.161
total data points collected: 587
average episode length: 58.7
average episode reward: -0.003
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 621
average episode length: 62.1
average episode reward: 0.027
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 676
average episode length: 67.6
average episode reward: 0.123
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 712
average episode length: 71.2
average episode reward: 0.223
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success']
success rate: 5/10
total data points collected: 1213
average episode length: 121.3
average episode reward: 0.760
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 5/10
total data points collected: 1243
average episode length: 124.3
average episode reward: 0.854
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success']
success rate: 6/10
total data points collected: 1277
average episode length: 127.7
average episode reward: 0.749
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success', 'success']
success rate: 7/10
total data points collected: 1317
average episode length: 131.7
average episode reward: 0.662
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success', 'success', 'success']
success rate: 8/10
tensor shapes - states: torch.Size([1317, 56]), actions: torch.Size([1317, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4718', '7.3469', '7.5401', '7.3774', '7.3294']
Training reward models...
Reward model losses: ['0.2178', '0.0375', '0.3594', '0.0220', '0.0177']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2681', '1.3573', '1.3982', '1.4295', '1.3115']
Training action VAE models...
Action VAE losses: ['1.2959', '1.3916', '1.3979', '1.4953', '1.4806']
CM score components:
transition disagreement: 0.4281
reward disagreement: 0.1849
state disagreement: 0.5227
action disagreement: 0.5245
total CM score: 1.6602
2025-07-22 16:43:14,383 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 16:43:15,336 774412 INFO Episode 1: reward=4.688, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 16:43:16,278 774412 INFO Episode 2: reward=4.688, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 16:43:17,223 774412 INFO Episode 3: reward=4.688, length=501, success=False
2025-07-22 16:43:23,804 774412 INFO Results: avg_reward=4.688, success_rate=0.000, avg_length=501.0
2025-07-22 16:43:23,805 774412 INFO
Testing intervention 5/6: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: 2.023
total data points collected: 501
average episode length: 50.1
average episode reward: 0.202
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 33 steps, reward: -2.310
total data points collected: 534
average episode length: 53.4
average episode reward: -0.029
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -1.060
total data points collected: 1035
average episode length: 103.5
average episode reward: -0.135
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1536
average episode length: 153.6
average episode reward: -0.409
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2037
average episode length: 203.7
average episode reward: -0.541
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2538
average episode length: 253.8
average episode reward: -0.275
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3039
average episode length: 303.9
average episode reward: -0.700
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3540
average episode length: 354.0
average episode reward: -0.767
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4041
average episode length: 404.1
average episode reward: -1.085
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4542
average episode length: 454.2
average episode reward: -1.394
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4542, 56]), actions: torch.Size([4542, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7739', '7.6519', '8.3817', '7.8143', '7.4538']
Training reward models...
Reward model losses: ['0.0479', '0.0413', '0.3454', '0.0342', '0.1802']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0288', '1.1060', '1.1069', '1.1002', '1.0163']
Training action VAE models...
Action VAE losses: ['1.4539', '1.5299', '1.4185', '1.4049', '1.5202']
CM score components:
transition disagreement: 0.4231
reward disagreement: 0.1640
state disagreement: 0.4724
action disagreement: 0.5676
total CM score: 1.6271
2025-07-22 16:43:33,545 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 16:43:34,561 774412 INFO Episode 1: reward=3.165, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 16:43:34,666 774412 INFO Episode 2: reward=0.477, length=29, success=True
Reset #3: angle intervention applied (success: True)
2025-07-22 16:43:35,799 774412 INFO Episode 3: reward=-2.879, length=501, success=False
2025-07-22 16:43:42,858 774412 INFO Results: avg_reward=0.677, success_rate=0.100, avg_length=453.8
2025-07-22 16:43:42,860 774412 INFO
Testing intervention 6/6: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.807
total data points collected: 501
average episode length: 50.1
average episode reward: -0.081
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: 3.933
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.313
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.839
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.129
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.259
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.351
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.324
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.371
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.441
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.550
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.576
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6651', '7.1368', '6.6226', '7.3791', '7.0876']
Training reward models...
Reward model losses: ['0.2963', '0.0148', '0.5422', '0.0197', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5245', '1.3919', '1.5092', '1.5745', '1.4365']
Training action VAE models...
Action VAE losses: ['1.4914', '1.4027', '1.4766', '1.3909', '1.3687']
CM score components:
transition disagreement: 0.4195
reward disagreement: 0.2221
state disagreement: 0.5067
action disagreement: 0.5399
total CM score: 1.6882
2025-07-22 16:43:53,663 774412 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 16:43:53,768 774412 INFO Episode 1: reward=-1.081, length=21, success=True
Reset #2: random intervention applied (success: True)
2025-07-22 16:43:54,776 774412 INFO Episode 2: reward=-2.379, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 16:43:55,677 774412 INFO Episode 3: reward=0.730, length=501, success=False
2025-07-22 16:44:02,912 774412 INFO Results: avg_reward=0.084, success_rate=0.100, avg_length=453.0
2025-07-22 16:44:02,914 774412 INFO Best intervention for stage 2: goal (Unified score: 0.8376)
2025-07-22 16:44:02,914 774412 INFO === stage 2/7: training on goal intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_2_goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
Reset #2: goal intervention applied (success: True)
Reset #3: goal intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | goal      |
|    stage             | 2         |
| rollout/             |           |
|    ep_len_mean       | 427       |
|    ep_rew_mean       | 0.4322988 |
| time/                |           |
|    fps               | 451       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5103616   |
------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 0.43533164 |
| time/                   |            |
|    fps                  | 273        |
|    iterations           | 2          |
|    time_elapsed         | 29         |
|    total_timesteps      | 5107712    |
| train/                  |            |
|    approx_kl            | 0.05867845 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.161      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.563     |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.0918    |
|    std                  | 3.11       |
|    value_loss           | 0.024      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 419         |
|    ep_rew_mean          | 0.46213177  |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 3           |
|    time_elapsed         | 50          |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.062795706 |
|    clip_fraction        | 0.584       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.1       |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.578      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.105      |
|    std                  | 3.16        |
|    value_loss           | 0.0212      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 419        |
|    ep_rew_mean          | 0.51755047 |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 4          |
|    time_elapsed         | 70         |
|    total_timesteps      | 5115904    |
| train/                  |            |
|    approx_kl            | 0.07124545 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.3      |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.528     |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.0704    |
|    std                  | 3.24       |
|    value_loss           | 0.0498     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 423         |
|    ep_rew_mean          | 0.55656815  |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 5           |
|    time_elapsed         | 91          |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.053791218 |
|    clip_fraction        | 0.541       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.00025     |
|    loss                 | -0.57       |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0949     |
|    std                  | 3.28        |
|    value_loss           | 0.0282      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 418         |
|    ep_rew_mean          | 0.6525709   |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 6           |
|    time_elapsed         | 111         |
|    total_timesteps      | 5124096     |
| train/                  |             |
|    approx_kl            | 0.062448464 |
|    clip_fraction        | 0.523       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.6       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.571      |
|    n_updates            | 1425        |
|    policy_gradient_loss | -0.0886     |
|    std                  | 3.35        |
|    value_loss           | 0.038       |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 0.8838661  |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 7          |
|    time_elapsed         | 132        |
|    total_timesteps      | 5128192    |
| train/                  |            |
|    approx_kl            | 0.06620833 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.581     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.101     |
|    std                  | 3.4        |
|    value_loss           | 0.017      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | 1.0379033   |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 8           |
|    time_elapsed         | 153         |
|    total_timesteps      | 5132288     |
| train/                  |             |
|    approx_kl            | 0.059610263 |
|    clip_fraction        | 0.518       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.574      |
|    n_updates            | 1455        |
|    policy_gradient_loss | -0.0957     |
|    std                  | 3.44        |
|    value_loss           | 0.0226      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 400        |
|    ep_rew_mean          | 1.1911796  |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 9          |
|    time_elapsed         | 173        |
|    total_timesteps      | 5136384    |
| train/                  |            |
|    approx_kl            | 0.06736397 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24        |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.577     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0988    |
|    std                  | 3.5        |
|    value_loss           | 0.0191     |
----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | goal      |
|    stage                | 2         |
| rollout/                |           |
|    ep_len_mean          | 389       |
|    ep_rew_mean          | 1.1182109 |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 10        |
|    time_elapsed         | 194       |
|    total_timesteps      | 5140480   |
| train/                  |           |
|    approx_kl            | 0.0647219 |
|    clip_fraction        | 0.582     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.2     |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.589    |
|    n_updates            | 1485      |
|    policy_gradient_loss | -0.101    |
|    std                  | 3.57      |
|    value_loss           | 0.0176    |
---------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | goal      |
|    stage                | 2         |
| rollout/                |           |
|    ep_len_mean          | 394       |
|    ep_rew_mean          | 1.1810517 |
| time/                   |           |
|    fps                  | 209       |
|    iterations           | 11        |
|    time_elapsed         | 215       |
|    total_timesteps      | 5144576   |
| train/                  |           |
|    approx_kl            | 0.0645857 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.4     |
|    explained_variance   | 0.627     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.581    |
|    n_updates            | 1500      |
|    policy_gradient_loss | -0.0884   |
|    std                  | 3.64      |
|    value_loss           | 0.0262    |
---------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | goal      |
|    stage                | 2         |
| rollout/                |           |
|    ep_len_mean          | 408       |
|    ep_rew_mean          | 1.3633358 |
| time/                   |           |
|    fps                  | 208       |
|    iterations           | 12        |
|    time_elapsed         | 236       |
|    total_timesteps      | 5148672   |
| train/                  |           |
|    approx_kl            | 0.0737346 |
|    clip_fraction        | 0.629     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.6     |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.602    |
|    n_updates            | 1515      |
|    policy_gradient_loss | -0.101    |
|    std                  | 3.75      |
|    value_loss           | 0.0136    |
---------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 1.4861051  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 13         |
|    time_elapsed         | 257        |
|    total_timesteps      | 5152768    |
| train/                  |            |
|    approx_kl            | 0.06677362 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.8      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.00025    |
|    loss                 | -0.616     |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.102     |
|    std                  | 3.85       |
|    value_loss           | 0.0155     |
----------------------------------------
2025-07-22 16:48:31,887 774412 INFO model saved to cm_sequencing_logs/model_stage_2_goal
2025-07-22 16:48:31,888 774412 INFO
Completed stage 2. Intervention 'goal' removed from list.
2025-07-22 16:48:31,889 774412 INFO Remaining interventions: 5
2025-07-22 16:48:31,889 774412 INFO CURRICULUM STAGE 3/7
2025-07-22 16:48:31,889 774412 INFO Remaining interventions: ['mass', 'friction', 'visual', 'angle', 'random']
2025-07-22 16:48:31,889 774412 INFO
Testing intervention 1/5: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 2.298
total data points collected: 501
average episode length: 50.1
average episode reward: 0.230
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 0.861
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.316
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 137 steps, reward: 0.033
total data points collected: 1139
average episode length: 113.9
average episode reward: 0.319
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1640
average episode length: 164.0
average episode reward: 0.199
termination reasons: ['max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 2141
average episode length: 214.1
average episode reward: 0.192
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2178
average episode length: 217.8
average episode reward: 0.278
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 2679
average episode length: 267.9
average episode reward: 0.048
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 3180
average episode length: 318.0
average episode reward: 0.066
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3230
average episode length: 323.0
average episode reward: 0.073
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3263
average episode length: 326.3
average episode reward: -0.047
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3263, 56]), actions: torch.Size([3263, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8078', '7.5048', '6.9853', '7.3154', '7.1479']
Training reward models...
Reward model losses: ['0.1972', '0.0510', '0.0114', '0.0279', '0.0353']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7258', '1.7188', '1.6863', '1.6480', '1.7546']
Training action VAE models...
Action VAE losses: ['1.5602', '1.4099', '1.3791', '1.4324', '1.3701']
CM score components:
transition disagreement: 0.3913
reward disagreement: 0.1334
state disagreement: 0.5431
action disagreement: 0.5449
total CM score: 1.6127
2025-07-22 16:48:39,401 774412 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 16:48:40,534 774412 INFO Episode 1: reward=1.724, length=501, success=False
Reset #2: mass intervention applied (success: True)
2025-07-22 16:48:41,530 774412 INFO Episode 2: reward=-4.328, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 16:48:42,619 774412 INFO Episode 3: reward=-2.589, length=501, success=False
2025-07-22 16:48:49,893 774412 INFO Results: avg_reward=-2.599, success_rate=0.000, avg_length=501.0
2025-07-22 16:48:49,895 774412 INFO
Testing intervention 2/5: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 33 steps, reward: 1.431
total data points collected: 33
average episode length: 3.3
average episode reward: 0.143
termination reasons: ['success']
success rate: 1/10
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: 1.213
total data points collected: 65
average episode length: 6.5
average episode reward: 0.264
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 0.180
total data points collected: 566
average episode length: 56.6
average episode reward: 0.282
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 1067
average episode length: 106.7
average episode reward: 0.235
termination reasons: ['success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 1101
average episode length: 110.1
average episode reward: 0.339
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 1602
average episode length: 160.2
average episode reward: 0.455
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 1894
average episode length: 189.4
average episode reward: 0.707
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 4/10
total data points collected: 2072
average episode length: 207.2
average episode reward: 0.567
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success', 'success']
success rate: 5/10
total data points collected: 2406
average episode length: 240.6
average episode reward: 0.850
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success']
success rate: 6/10
total data points collected: 2907
average episode length: 290.7
average episode reward: 1.107
termination reasons: ['success', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 6/10
tensor shapes - states: torch.Size([2907, 56]), actions: torch.Size([2907, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6345', '7.8810', '6.6729', '7.1098', '7.2759']
Training reward models...
Reward model losses: ['0.0073', '0.1085', '0.5982', '0.0123', '0.0284']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9727', '1.8753', '1.7486', '1.7195', '1.8559']
Training action VAE models...
Action VAE losses: ['1.3429', '1.5780', '1.5158', '1.3321', '1.4979']
CM score components:
transition disagreement: 0.3896
reward disagreement: 0.2595
state disagreement: 0.5553
action disagreement: 0.5627
total CM score: 1.7671
2025-07-22 16:48:56,748 774412 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 16:48:57,917 774412 INFO Episode 1: reward=-1.917, length=501, success=False
Reset #2: friction intervention applied (success: True)
2025-07-22 16:48:59,070 774412 INFO Episode 2: reward=-1.306, length=501, success=False
Reset #3: friction intervention applied (success: True)
2025-07-22 16:49:00,069 774412 INFO Episode 3: reward=-1.631, length=501, success=False
2025-07-22 16:49:05,281 774412 INFO Results: avg_reward=-0.782, success_rate=0.200, avg_length=414.9
2025-07-22 16:49:05,283 774412 INFO
Testing intervention 3/5: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 1.893
total data points collected: 501
average episode length: 50.1
average episode reward: 0.189
termination reasons: ['max_length']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 28 steps, reward: 0.264
total data points collected: 529
average episode length: 52.9
average episode reward: 0.216
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: visual intervention applied (success: True)
episode 3: 31 steps, reward: -0.305
total data points collected: 560
average episode length: 56.0
average episode reward: 0.185
termination reasons: ['max_length', 'success', 'success']
success rate: 2/10
total data points collected: 1061
average episode length: 106.1
average episode reward: 0.352
termination reasons: ['max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 1562
average episode length: 156.2
average episode reward: 0.775
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2063
average episode length: 206.3
average episode reward: 0.274
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2564
average episode length: 256.4
average episode reward: 0.730
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2607
average episode length: 260.7
average episode reward: 0.854
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 2886
average episode length: 288.6
average episode reward: 1.073
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 3123
average episode length: 312.3
average episode reward: 1.279
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'success']
success rate: 5/10
tensor shapes - states: torch.Size([3123, 56]), actions: torch.Size([3123, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1159', '7.6219', '7.9595', '6.8562', '7.7069']
Training reward models...
Reward model losses: ['0.0470', '0.0152', '0.9044', '0.7981', '0.3142']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5941', '1.5950', '1.7786', '1.5258', '1.4271']
Training action VAE models...
Action VAE losses: ['1.4009', '1.3294', '1.5645', '1.6976', '1.5135']
CM score components:
transition disagreement: 0.4509
reward disagreement: 0.4229
state disagreement: 0.5059
action disagreement: 0.5893
total CM score: 1.9690
2025-07-22 16:49:12,531 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 16:49:13,584 774412 INFO Episode 1: reward=-1.482, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 16:49:14,628 774412 INFO Episode 2: reward=-1.482, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 16:49:15,669 774412 INFO Episode 3: reward=-1.482, length=501, success=False
2025-07-22 16:49:22,959 774412 INFO Results: avg_reward=-1.482, success_rate=0.000, avg_length=501.0
2025-07-22 16:49:22,961 774412 INFO
Testing intervention 4/5: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -1.209
total data points collected: 501
average episode length: 50.1
average episode reward: -0.121
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: -0.803
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.201
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -2.565
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.458
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.829
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -0.599
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -1.143
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: -1.312
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: -1.516
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4042
average episode length: 404.2
average episode reward: -1.560
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 4224
average episode length: 422.4
average episode reward: -1.541
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
tensor shapes - states: torch.Size([4224, 56]), actions: torch.Size([4224, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3905', '7.2262', '7.6831', '7.2309', '6.5842']
Training reward models...
Reward model losses: ['0.0644', '0.0696', '0.0581', '0.0291', '0.0267']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2693', '1.3935', '1.2095', '1.2839', '1.1602']
Training action VAE models...
Action VAE losses: ['1.3428', '1.3943', '1.5541', '1.4470', '1.4910']
CM score components:
transition disagreement: 0.4089
reward disagreement: 0.1153
state disagreement: 0.5005
action disagreement: 0.5653
total CM score: 1.5900
2025-07-22 16:49:32,224 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 16:49:33,134 774412 INFO Episode 1: reward=0.797, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 16:49:34,051 774412 INFO Episode 2: reward=-0.063, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 16:49:35,184 774412 INFO Episode 3: reward=-2.181, length=501, success=False
2025-07-22 16:49:41,775 774412 INFO Results: avg_reward=-1.115, success_rate=0.100, avg_length=465.6
2025-07-22 16:49:41,776 774412 INFO
Testing intervention 5/5: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.084
total data points collected: 501
average episode length: 50.1
average episode reward: 0.208
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: 4.070
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.615
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -2.561
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.359
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.347
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.556
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.625
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 1.186
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 1.612
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 1.816
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 1.790
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3938', '8.0474', '7.5019', '7.3821', '6.5030']
Training reward models...
Reward model losses: ['0.0363', '0.7533', '0.0399', '0.4781', '0.0747']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4951', '1.6169', '1.4745', '1.5542', '1.5649']
Training action VAE models...
Action VAE losses: ['1.4647', '1.3982', '1.4777', '1.3851', '1.4650']
CM score components:
transition disagreement: 0.4646
reward disagreement: 0.2529
state disagreement: 0.5278
action disagreement: 0.5816
total CM score: 1.8269
2025-07-22 16:49:52,521 774412 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 16:49:52,749 774412 INFO Episode 1: reward=-0.970, length=82, success=True
Reset #2: random intervention applied (success: True)
2025-07-22 16:49:53,726 774412 INFO Episode 2: reward=1.055, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 16:49:54,603 774412 INFO Episode 3: reward=-2.434, length=501, success=False
2025-07-22 16:50:01,376 774412 INFO Results: avg_reward=1.656, success_rate=0.100, avg_length=459.1
2025-07-22 16:50:01,378 774412 INFO Best intervention for stage 3: random (Unified score: 0.8126)
2025-07-22 16:50:01,378 774412 INFO === stage 3/7: training on random intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_3_random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
Reset #2: random intervention applied (success: True)
Reset #3: random intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | random    |
|    stage             | 3         |
| rollout/             |           |
|    ep_len_mean       | 413       |
|    ep_rew_mean       | 1.5244681 |
| time/                |           |
|    fps               | 449       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5156864   |
------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 418        |
|    ep_rew_mean          | 1.494007   |
| time/                   |            |
|    fps                  | 280        |
|    iterations           | 2          |
|    time_elapsed         | 29         |
|    total_timesteps      | 5160960    |
| train/                  |            |
|    approx_kl            | 0.07822046 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.2      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.578     |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0809    |
|    std                  | 4.04       |
|    value_loss           | 0.045      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 423        |
|    ep_rew_mean          | 1.2558143  |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 3          |
|    time_elapsed         | 50         |
|    total_timesteps      | 5165056    |
| train/                  |            |
|    approx_kl            | 0.06475336 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.5      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.624     |
|    n_updates            | 1575       |
|    policy_gradient_loss | -0.0946    |
|    std                  | 4.12       |
|    value_loss           | 0.0266     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 432        |
|    ep_rew_mean          | 1.2543646  |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 4          |
|    time_elapsed         | 70         |
|    total_timesteps      | 5169152    |
| train/                  |            |
|    approx_kl            | 0.10521562 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.7      |
|    explained_variance   | 0.501      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.611     |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0941    |
|    std                  | 4.25       |
|    value_loss           | 0.0409     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | random      |
|    stage                | 3           |
| rollout/                |             |
|    ep_len_mean          | 432         |
|    ep_rew_mean          | 1.157009    |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 5           |
|    time_elapsed         | 91          |
|    total_timesteps      | 5173248     |
| train/                  |             |
|    approx_kl            | 0.056256652 |
|    clip_fraction        | 0.589       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.607      |
|    n_updates            | 1605        |
|    policy_gradient_loss | -0.0915     |
|    std                  | 4.34        |
|    value_loss           | 0.0322      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 1.1972232  |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 6          |
|    time_elapsed         | 112        |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.08224226 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.2      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.615     |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 4.47       |
|    value_loss           | 0.0359     |
----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | random    |
|    stage                | 3         |
| rollout/                |           |
|    ep_len_mean          | 450       |
|    ep_rew_mean          | 1.1285158 |
| time/                   |           |
|    fps                  | 215       |
|    iterations           | 7         |
|    time_elapsed         | 133       |
|    total_timesteps      | 5181440   |
| train/                  |           |
|    approx_kl            | 0.1473015 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.2       |
|    entropy_loss         | -26.4     |
|    explained_variance   | 0.63      |
|    learning_rate        | 0.00025   |
|    loss                 | -0.617    |
|    n_updates            | 1635      |
|    policy_gradient_loss | -0.0963   |
|    std                  | 4.58      |
|    value_loss           | 0.0279    |
---------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 460        |
|    ep_rew_mean          | 1.1087179  |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 8          |
|    time_elapsed         | 153        |
|    total_timesteps      | 5185536    |
| train/                  |            |
|    approx_kl            | 0.08154991 |
|    clip_fraction        | 0.601      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.6      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.622     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0856    |
|    std                  | 4.72       |
|    value_loss           | 0.0366     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | random      |
|    stage                | 3           |
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | 1.0121946   |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 9           |
|    time_elapsed         | 174         |
|    total_timesteps      | 5189632     |
| train/                  |             |
|    approx_kl            | 0.082753465 |
|    clip_fraction        | 0.64        |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.00025     |
|    loss                 | -0.644      |
|    n_updates            | 1665        |
|    policy_gradient_loss | -0.0931     |
|    std                  | 4.89        |
|    value_loss           | 0.0327      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | 0.98885256 |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 10         |
|    time_elapsed         | 195        |
|    total_timesteps      | 5193728    |
| train/                  |            |
|    approx_kl            | 0.08446197 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.2      |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.61      |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0791    |
|    std                  | 5.07       |
|    value_loss           | 0.0984     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | 0.81088173 |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 11         |
|    time_elapsed         | 215        |
|    total_timesteps      | 5197824    |
| train/                  |            |
|    approx_kl            | 0.23544365 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.6      |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.617     |
|    n_updates            | 1695       |
|    policy_gradient_loss | -0.09      |
|    std                  | 5.25       |
|    value_loss           | 0.0836     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | 0.79401416 |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 12         |
|    time_elapsed         | 236        |
|    total_timesteps      | 5201920    |
| train/                  |            |
|    approx_kl            | 0.0797572  |
|    clip_fraction        | 0.636      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.66      |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.107     |
|    std                  | 5.38       |
|    value_loss           | 0.052      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | 0.8178701  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 13         |
|    time_elapsed         | 257        |
|    total_timesteps      | 5206016    |
| train/                  |            |
|    approx_kl            | 0.07247929 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28        |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.673     |
|    n_updates            | 1725       |
|    policy_gradient_loss | -0.114     |
|    std                  | 5.44       |
|    value_loss           | 0.0634     |
----------------------------------------
2025-07-22 16:54:30,261 774412 INFO model saved to cm_sequencing_logs/model_stage_3_random
2025-07-22 16:54:30,262 774412 INFO
Completed stage 3. Intervention 'random' removed from list.
2025-07-22 16:54:30,263 774412 INFO Remaining interventions: 4
2025-07-22 16:54:30,263 774412 INFO CURRICULUM STAGE 4/7
2025-07-22 16:54:30,263 774412 INFO Remaining interventions: ['mass', 'friction', 'visual', 'angle']
2025-07-22 16:54:30,263 774412 INFO
Testing intervention 1/4: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 1.983
total data points collected: 501
average episode length: 50.1
average episode reward: 0.198
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -1.981
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.000
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 204 steps, reward: 2.335
total data points collected: 1206
average episode length: 120.6
average episode reward: 0.234
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1237
average episode length: 123.7
average episode reward: 0.325
termination reasons: ['max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 1738
average episode length: 173.8
average episode reward: 0.683
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 1805
average episode length: 180.5
average episode reward: 0.759
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 2306
average episode length: 230.6
average episode reward: 0.812
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 2807
average episode length: 280.7
average episode reward: 0.941
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 3308
average episode length: 330.8
average episode reward: 0.711
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 3809
average episode length: 380.9
average episode reward: 0.577
termination reasons: ['max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 3/10
tensor shapes - states: torch.Size([3809, 56]), actions: torch.Size([3809, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0572', '7.7263', '7.5194', '7.5738', '7.7803']
Training reward models...
Reward model losses: ['0.0731', '0.0461', '0.0739', '0.0298', '0.0685']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7368', '1.6931', '1.6711', '1.7136', '1.9898']
Training action VAE models...
Action VAE losses: ['1.4813', '1.3501', '1.3705', '1.3832', '1.4550']
CM score components:
transition disagreement: 0.4110
reward disagreement: 0.1080
state disagreement: 0.5540
action disagreement: 0.5504
total CM score: 1.6235
2025-07-22 16:54:38,744 774412 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 16:54:39,674 774412 INFO Episode 1: reward=-2.886, length=501, success=False
Reset #2: mass intervention applied (success: True)
2025-07-22 16:54:40,571 774412 INFO Episode 2: reward=3.552, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 16:54:41,523 774412 INFO Episode 3: reward=-3.177, length=501, success=False
2025-07-22 16:54:48,008 774412 INFO Results: avg_reward=-1.491, success_rate=0.000, avg_length=501.0
2025-07-22 16:54:48,010 774412 INFO
Testing intervention 2/4: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.122
total data points collected: 501
average episode length: 50.1
average episode reward: 0.412
termination reasons: ['max_length']
success rate: 0/10
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: -0.322
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.380
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: -0.974
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.283
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 1531
average episode length: 153.1
average episode reward: 0.378
termination reasons: ['max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1557
average episode length: 155.7
average episode reward: 0.464
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 1739
average episode length: 173.9
average episode reward: 0.581
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'success']
success rate: 3/10
total data points collected: 2240
average episode length: 224.0
average episode reward: 0.422
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 3/10
total data points collected: 2741
average episode length: 274.1
average episode reward: 0.278
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 3242
average episode length: 324.2
average episode reward: 0.379
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 3743
average episode length: 374.3
average episode reward: 0.158
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 3/10
tensor shapes - states: torch.Size([3743, 56]), actions: torch.Size([3743, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9728', '7.2260', '7.0874', '7.0453', '8.3344']
Training reward models...
Reward model losses: ['1.5982', '0.1458', '0.0472', '0.1217', '0.0090']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9140', '2.4658', '1.9792', '2.0163', '2.0049']
Training action VAE models...
Action VAE losses: ['1.3993', '1.6035', '1.6435', '1.3281', '1.3713']
CM score components:
transition disagreement: 0.4282
reward disagreement: 0.4293
state disagreement: 0.5949
action disagreement: 0.5984
total CM score: 2.0509
2025-07-22 16:54:56,333 774412 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 16:54:57,275 774412 INFO Episode 1: reward=-1.147, length=501, success=False
Reset #2: friction intervention applied (success: True)
2025-07-22 16:54:58,214 774412 INFO Episode 2: reward=-1.822, length=501, success=False
Reset #3: friction intervention applied (success: True)
2025-07-22 16:54:59,150 774412 INFO Episode 3: reward=-1.918, length=501, success=False
2025-07-22 16:55:05,878 774412 INFO Results: avg_reward=-1.280, success_rate=0.000, avg_length=501.0
2025-07-22 16:55:05,879 774412 INFO
Testing intervention 3/4: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.425
total data points collected: 501
average episode length: 50.1
average episode reward: 0.343
termination reasons: ['max_length']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -2.957
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.047
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 2.871
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.334
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.314
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.171
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2554
average episode length: 255.4
average episode reward: 0.290
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 3055
average episode length: 305.5
average episode reward: 0.289
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 3556
average episode length: 355.6
average episode reward: 0.209
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4057
average episode length: 405.7
average episode reward: 0.330
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4558
average episode length: 455.8
average episode reward: 0.062
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4558, 56]), actions: torch.Size([4558, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9003', '7.9414', '7.2499', '7.7918', '7.2273']
Training reward models...
Reward model losses: ['0.1715', '0.0158', '0.0978', '0.2950', '0.0134']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0434', '1.0619', '1.0786', '1.0666', '1.1206']
Training action VAE models...
Action VAE losses: ['1.3255', '1.3701', '1.4536', '1.4515', '1.4845']
CM score components:
transition disagreement: 0.4201
reward disagreement: 0.1152
state disagreement: 0.4710
action disagreement: 0.5499
total CM score: 1.5561
2025-07-22 16:55:15,688 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 16:55:16,636 774412 INFO Episode 1: reward=-1.084, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 16:55:17,568 774412 INFO Episode 2: reward=-1.084, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 16:55:18,499 774412 INFO Episode 3: reward=-1.084, length=501, success=False
2025-07-22 16:55:24,999 774412 INFO Results: avg_reward=-1.084, success_rate=0.000, avg_length=501.0
2025-07-22 16:55:25,001 774412 INFO
Testing intervention 4/4: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -4.866
total data points collected: 501
average episode length: 50.1
average episode reward: -0.487
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 46 steps, reward: -2.310
total data points collected: 547
average episode length: 54.7
average episode reward: -0.718
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -3.314
total data points collected: 1048
average episode length: 104.8
average episode reward: -1.049
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1549
average episode length: 154.9
average episode reward: -1.088
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2050
average episode length: 205.0
average episode reward: -1.343
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2551
average episode length: 255.1
average episode reward: -1.767
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3052
average episode length: 305.2
average episode reward: -1.847
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3553
average episode length: 355.3
average episode reward: -1.920
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4054
average episode length: 405.4
average episode reward: -2.082
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4555
average episode length: 455.5
average episode reward: -2.138
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4555, 56]), actions: torch.Size([4555, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8842', '6.7629', '7.4696', '6.9244', '7.3193']
Training reward models...
Reward model losses: ['0.2001', '0.1462', '0.1609', '0.0341', '0.0309']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3435', '1.2549', '1.2251', '1.2374', '1.2912']
Training action VAE models...
Action VAE losses: ['1.3635', '1.3907', '1.3091', '1.3127', '1.3523']
CM score components:
transition disagreement: 0.4018
reward disagreement: 0.1020
state disagreement: 0.4895
action disagreement: 0.5152
total CM score: 1.5086
2025-07-22 16:55:34,818 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 16:55:35,789 774412 INFO Episode 1: reward=-1.203, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 16:55:36,771 774412 INFO Episode 2: reward=-2.878, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 16:55:37,654 774412 INFO Episode 3: reward=0.230, length=501, success=False
2025-07-22 16:55:44,363 774412 INFO Results: avg_reward=-2.987, success_rate=0.000, avg_length=501.0
2025-07-22 16:55:44,365 774412 INFO Best intervention for stage 4: friction (Unified score: 0.9485)
2025-07-22 16:55:44,365 774412 INFO === stage 4/7: training on friction intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_4_friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
Reset #2: friction intervention applied (success: True)
Reset #3: friction intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | friction  |
|    stage             | 4         |
| rollout/             |           |
|    ep_len_mean       | 483       |
|    ep_rew_mean       | 0.6640947 |
| time/                |           |
|    fps               | 456       |
|    iterations        | 1         |
|    time_elapsed      | 8         |
|    total_timesteps   | 5210112   |
------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 0.6826624  |
| time/                   |            |
|    fps                  | 273        |
|    iterations           | 2          |
|    time_elapsed         | 29         |
|    total_timesteps      | 5214208    |
| train/                  |            |
|    approx_kl            | 0.08136695 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.3      |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.659     |
|    n_updates            | 1755       |
|    policy_gradient_loss | -0.0924    |
|    std                  | 5.67       |
|    value_loss           | 0.0412     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 476         |
|    ep_rew_mean          | 0.6018536   |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 3           |
|    time_elapsed         | 50          |
|    total_timesteps      | 5218304     |
| train/                  |             |
|    approx_kl            | 0.082882814 |
|    clip_fraction        | 0.613       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.5       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.693      |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.111      |
|    std                  | 5.78        |
|    value_loss           | 0.0287      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 467         |
|    ep_rew_mean          | 0.3918567   |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 4           |
|    time_elapsed         | 71          |
|    total_timesteps      | 5222400     |
| train/                  |             |
|    approx_kl            | 0.070085704 |
|    clip_fraction        | 0.566       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.665      |
|    n_updates            | 1785        |
|    policy_gradient_loss | -0.0949     |
|    std                  | 5.86        |
|    value_loss           | 0.044       |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 458         |
|    ep_rew_mean          | -0.03636437 |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 5           |
|    time_elapsed         | 92          |
|    total_timesteps      | 5226496     |
| train/                  |             |
|    approx_kl            | 0.07464121  |
|    clip_fraction        | 0.558       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.8       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.685      |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0964     |
|    std                  | 5.99        |
|    value_loss           | 0.051       |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 454         |
|    ep_rew_mean          | -0.22167957 |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 6           |
|    time_elapsed         | 114         |
|    total_timesteps      | 5230592     |
| train/                  |             |
|    approx_kl            | 0.0910656   |
|    clip_fraction        | 0.601       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29         |
|    explained_variance   | 0.494       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.679      |
|    n_updates            | 1815        |
|    policy_gradient_loss | -0.1        |
|    std                  | 6.11        |
|    value_loss           | 0.0424      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 449         |
|    ep_rew_mean          | -0.3212486  |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 7           |
|    time_elapsed         | 134         |
|    total_timesteps      | 5234688     |
| train/                  |             |
|    approx_kl            | 0.070550054 |
|    clip_fraction        | 0.595       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.701      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.104      |
|    std                  | 6.22        |
|    value_loss           | 0.0417      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | -0.40324715 |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 8           |
|    time_elapsed         | 155         |
|    total_timesteps      | 5238784     |
| train/                  |             |
|    approx_kl            | 0.07775431  |
|    clip_fraction        | 0.578       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.687      |
|    n_updates            | 1845        |
|    policy_gradient_loss | -0.0965     |
|    std                  | 6.36        |
|    value_loss           | 0.0441      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | -0.49351177 |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 9           |
|    time_elapsed         | 176         |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.04833978  |
|    clip_fraction        | 0.478       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.69       |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0857     |
|    std                  | 6.48        |
|    value_loss           | 0.0412      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | -0.61381143 |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 10          |
|    time_elapsed         | 197         |
|    total_timesteps      | 5246976     |
| train/                  |             |
|    approx_kl            | 0.09971557  |
|    clip_fraction        | 0.63        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.706      |
|    n_updates            | 1875        |
|    policy_gradient_loss | -0.104      |
|    std                  | 6.64        |
|    value_loss           | 0.0216      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 417        |
|    ep_rew_mean          | -0.6215164 |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 11         |
|    time_elapsed         | 218        |
|    total_timesteps      | 5251072    |
| train/                  |            |
|    approx_kl            | 0.07566699 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.9      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.706     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0989    |
|    std                  | 6.79       |
|    value_loss           | 0.0209     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 409        |
|    ep_rew_mean          | -0.5551413 |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 12         |
|    time_elapsed         | 239        |
|    total_timesteps      | 5255168    |
| train/                  |            |
|    approx_kl            | 0.0803621  |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.715     |
|    n_updates            | 1905       |
|    policy_gradient_loss | -0.106     |
|    std                  | 6.99       |
|    value_loss           | 0.0273     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | -0.34688893 |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 13          |
|    time_elapsed         | 260         |
|    total_timesteps      | 5259264     |
| train/                  |             |
|    approx_kl            | 0.06489811  |
|    clip_fraction        | 0.569       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.724      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.102      |
|    std                  | 7.08        |
|    value_loss           | 0.0185      |
-----------------------------------------
2025-07-22 17:00:16,822 774412 INFO model saved to cm_sequencing_logs/model_stage_4_friction
2025-07-22 17:00:16,823 774412 INFO
Completed stage 4. Intervention 'friction' removed from list.
2025-07-22 17:00:16,823 774412 INFO Remaining interventions: 3
2025-07-22 17:00:16,824 774412 INFO CURRICULUM STAGE 5/7
2025-07-22 17:00:16,824 774412 INFO Remaining interventions: ['mass', 'visual', 'angle']
2025-07-22 17:00:16,824 774412 INFO
Testing intervention 1/3: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.066
total data points collected: 501
average episode length: 50.1
average episode reward: 0.007
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -1.737
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.167
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.725
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.105
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.175
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -0.170
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -0.504
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3035
average episode length: 303.5
average episode reward: -0.409
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 3062
average episode length: 306.2
average episode reward: -0.277
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 3563
average episode length: 356.3
average episode reward: -0.367
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 4064
average episode length: 406.4
average episode reward: -0.423
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4064, 56]), actions: torch.Size([4064, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1385', '7.9173', '7.9525', '8.0346', '7.5667']
Training reward models...
Reward model losses: ['0.0201', '0.0801', '0.5947', '0.2543', '0.0166']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7271', '1.8941', '1.5925', '1.8619', '1.7246']
Training action VAE models...
Action VAE losses: ['1.3910', '1.3588', '1.5471', '1.3829', '1.3776']
CM score components:
transition disagreement: 0.4369
reward disagreement: 0.1746
state disagreement: 0.5455
action disagreement: 0.5265
total CM score: 1.6834
2025-07-22 17:00:25,822 774412 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 17:00:25,930 774412 INFO Episode 1: reward=1.083, length=23, success=True
Reset #2: mass intervention applied (success: True)
2025-07-22 17:00:26,887 774412 INFO Episode 2: reward=-0.252, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 17:00:27,824 774412 INFO Episode 3: reward=-0.888, length=501, success=False
2025-07-22 17:00:32,952 774412 INFO Results: avg_reward=-0.537, success_rate=0.300, avg_length=357.7
2025-07-22 17:00:32,954 774412 INFO
Testing intervention 2/3: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 72 steps, reward: 2.328
total data points collected: 72
average episode length: 7.2
average episode reward: 0.233
termination reasons: ['success']
success rate: 1/10
Reset #2: visual intervention applied (success: True)
episode 2: 29 steps, reward: 1.773
total data points collected: 101
average episode length: 10.1
average episode reward: 0.410
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: visual intervention applied (success: True)
episode 3: 137 steps, reward: -0.262
total data points collected: 238
average episode length: 23.8
average episode reward: 0.384
termination reasons: ['success', 'success', 'success']
success rate: 3/10
total data points collected: 739
average episode length: 73.9
average episode reward: 0.378
termination reasons: ['success', 'success', 'success', 'max_length']
success rate: 3/10
total data points collected: 915
average episode length: 91.5
average episode reward: 0.426
termination reasons: ['success', 'success', 'success', 'max_length', 'success']
success rate: 4/10
total data points collected: 1416
average episode length: 141.6
average episode reward: 0.081
termination reasons: ['success', 'success', 'success', 'max_length', 'success', 'max_length']
success rate: 4/10
total data points collected: 1439
average episode length: 143.9
average episode reward: 0.143
termination reasons: ['success', 'success', 'success', 'max_length', 'success', 'max_length', 'success']
success rate: 5/10
total data points collected: 1940
average episode length: 194.0
average episode reward: -0.047
termination reasons: ['success', 'success', 'success', 'max_length', 'success', 'max_length', 'success', 'max_length']
success rate: 5/10
total data points collected: 2441
average episode length: 244.1
average episode reward: -0.366
termination reasons: ['success', 'success', 'success', 'max_length', 'success', 'max_length', 'success', 'max_length', 'max_length']
success rate: 5/10
total data points collected: 2942
average episode length: 294.2
average episode reward: -0.505
termination reasons: ['success', 'success', 'success', 'max_length', 'success', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 5/10
tensor shapes - states: torch.Size([2942, 56]), actions: torch.Size([2942, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7523', '7.3334', '7.3811', '7.4824', '7.4085']
Training reward models...
Reward model losses: ['0.0354', '1.2133', '0.0216', '0.0156', '0.0482']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8329', '1.5415', '1.5520', '1.7923', '1.7683']
Training action VAE models...
Action VAE losses: ['1.4343', '1.6023', '1.4180', '1.3560', '1.3445']
CM score components:
transition disagreement: 0.4077
reward disagreement: 0.3442
state disagreement: 0.5410
action disagreement: 0.5646
total CM score: 1.8576
2025-07-22 17:00:39,855 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 17:00:40,874 774412 INFO Episode 1: reward=-1.078, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 17:00:41,885 774412 INFO Episode 2: reward=-1.078, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 17:00:42,893 774412 INFO Episode 3: reward=-1.078, length=501, success=False
2025-07-22 17:00:49,935 774412 INFO Results: avg_reward=-1.078, success_rate=0.000, avg_length=501.0
2025-07-22 17:00:49,937 774412 INFO
Testing intervention 3/3: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 26 steps, reward: -0.738
total data points collected: 26
average episode length: 2.6
average episode reward: -0.074
termination reasons: ['success']
success rate: 1/10
Reset #2: angle intervention applied (success: True)
episode 2: 120 steps, reward: 1.258
total data points collected: 146
average episode length: 14.6
average episode reward: 0.052
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -4.776
total data points collected: 647
average episode length: 64.7
average episode reward: -0.426
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 683
average episode length: 68.3
average episode reward: -0.326
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 718
average episode length: 71.8
average episode reward: -0.255
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 1219
average episode length: 121.9
average episode reward: -0.273
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 1254
average episode length: 125.4
average episode reward: -0.106
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 5/10
total data points collected: 1303
average episode length: 130.3
average episode reward: -0.186
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'success', 'success']
success rate: 6/10
total data points collected: 1804
average episode length: 180.4
average episode reward: -0.465
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 6/10
total data points collected: 1838
average episode length: 183.8
average episode reward: -0.504
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 7/10
tensor shapes - states: torch.Size([1838, 56]), actions: torch.Size([1838, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6271', '7.5560', '7.1124', '7.4895', '6.8878']
Training reward models...
Reward model losses: ['1.3810', '0.3403', '0.0297', '1.9618', '0.2047']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3551', '1.7306', '1.8232', '1.6653', '1.4808']
Training action VAE models...
Action VAE losses: ['1.5008', '1.3794', '1.3961', '1.5518', '1.4494']
CM score components:
transition disagreement: 0.3945
reward disagreement: 0.6709
state disagreement: 0.5238
action disagreement: 0.5701
total CM score: 2.1594
2025-07-22 17:00:54,894 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 17:00:55,826 774412 INFO Episode 1: reward=-2.852, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 17:00:56,807 774412 INFO Episode 2: reward=-3.522, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 17:00:57,798 774412 INFO Episode 3: reward=-2.028, length=501, success=False
2025-07-22 17:01:02,904 774412 INFO Results: avg_reward=-1.761, success_rate=0.200, avg_length=408.0
2025-07-22 17:01:02,906 774412 INFO Best intervention for stage 5: mass (Unified score: 0.5000)
2025-07-22 17:01:02,906 774412 INFO === stage 5/7: training on mass intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_5_mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
Reset #2: mass intervention applied (success: True)
Reset #3: mass intervention applied (success: True)
-------------------------------------
| custom/              |            |
|    intervention_type | mass       |
|    stage             | 5          |
| rollout/             |            |
|    ep_len_mean       | 408        |
|    ep_rew_mean       | -0.4231371 |
| time/                |            |
|    fps               | 448        |
|    iterations        | 1          |
|    time_elapsed      | 9          |
|    total_timesteps   | 5263360    |
-------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 406        |
|    ep_rew_mean          | -0.1578906 |
| time/                   |            |
|    fps                  | 270        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 5267456    |
| train/                  |            |
|    approx_kl            | 0.06791937 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.7      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.738     |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.111     |
|    std                  | 7.39       |
|    value_loss           | 0.024      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 410         |
|    ep_rew_mean          | -0.1508578  |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 3           |
|    time_elapsed         | 51          |
|    total_timesteps      | 5271552     |
| train/                  |             |
|    approx_kl            | 0.052443907 |
|    clip_fraction        | 0.526       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.727      |
|    n_updates            | 1965        |
|    policy_gradient_loss | -0.104      |
|    std                  | 7.45        |
|    value_loss           | 0.0202      |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    intervention_type    | mass         |
|    stage                | 5            |
| rollout/                |              |
|    ep_len_mean          | 406          |
|    ep_rew_mean          | -0.020473475 |
| time/                   |              |
|    fps                  | 226          |
|    iterations           | 4            |
|    time_elapsed         | 72           |
|    total_timesteps      | 5275648      |
| train/                  |              |
|    approx_kl            | 0.050243825  |
|    clip_fraction        | 0.538        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.9        |
|    explained_variance   | 0.715        |
|    learning_rate        | 0.00025      |
|    loss                 | -0.711       |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.0903      |
|    std                  | 7.58         |
|    value_loss           | 0.0178       |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 0.02224626  |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 5           |
|    time_elapsed         | 93          |
|    total_timesteps      | 5279744     |
| train/                  |             |
|    approx_kl            | 0.063962996 |
|    clip_fraction        | 0.589       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.728      |
|    n_updates            | 1995        |
|    policy_gradient_loss | -0.101      |
|    std                  | 7.71        |
|    value_loss           | 0.0205      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 0.00223302 |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 6          |
|    time_elapsed         | 114        |
|    total_timesteps      | 5283840    |
| train/                  |            |
|    approx_kl            | 0.07063115 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.745     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.108     |
|    std                  | 7.84       |
|    value_loss           | 0.0152     |
----------------------------------------
------------------------------------------
| custom/                 |              |
|    intervention_type    | mass         |
|    stage                | 5            |
| rollout/                |              |
|    ep_len_mean          | 408          |
|    ep_rew_mean          | -0.035470992 |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 7            |
|    time_elapsed         | 135          |
|    total_timesteps      | 5287936      |
| train/                  |              |
|    approx_kl            | 0.06281921   |
|    clip_fraction        | 0.589        |
|    clip_range           | 0.2          |
|    entropy_loss         | -31.4        |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.00025      |
|    loss                 | -0.739       |
|    n_updates            | 2025         |
|    policy_gradient_loss | -0.102       |
|    std                  | 8            |
|    value_loss           | 0.0197       |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 410         |
|    ep_rew_mean          | -0.16099706 |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 8           |
|    time_elapsed         | 156         |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.071255    |
|    clip_fraction        | 0.566       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.6       |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.716      |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0884     |
|    std                  | 8.21        |
|    value_loss           | 0.0378      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | -0.24539396 |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 9           |
|    time_elapsed         | 177         |
|    total_timesteps      | 5296128     |
| train/                  |             |
|    approx_kl            | 0.093087286 |
|    clip_fraction        | 0.62        |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.8       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.739      |
|    n_updates            | 2055        |
|    policy_gradient_loss | -0.0985     |
|    std                  | 8.4         |
|    value_loss           | 0.0334      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 418        |
|    ep_rew_mean          | -0.3160859 |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 10         |
|    time_elapsed         | 199        |
|    total_timesteps      | 5300224    |
| train/                  |            |
|    approx_kl            | 0.07176401 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32        |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.748     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.103     |
|    std                  | 8.57       |
|    value_loss           | 0.0235     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | -0.32088816 |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 11          |
|    time_elapsed         | 220         |
|    total_timesteps      | 5304320     |
| train/                  |             |
|    approx_kl            | 0.071004674 |
|    clip_fraction        | 0.587       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.751      |
|    n_updates            | 2085        |
|    policy_gradient_loss | -0.105      |
|    std                  | 8.73        |
|    value_loss           | 0.035       |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 5           |
| rollout/                |             |
|    ep_len_mean          | 405         |
|    ep_rew_mean          | -0.29862925 |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 12          |
|    time_elapsed         | 241         |
|    total_timesteps      | 5308416     |
| train/                  |             |
|    approx_kl            | 0.08693662  |
|    clip_fraction        | 0.63        |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.3       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.746      |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.107      |
|    std                  | 8.95        |
|    value_loss           | 0.0363      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 405        |
|    ep_rew_mean          | -0.4569556 |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 13         |
|    time_elapsed         | 263        |
|    total_timesteps      | 5312512    |
| train/                  |            |
|    approx_kl            | 0.07838793 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.5      |
|    explained_variance   | 0.546      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.748     |
|    n_updates            | 2115       |
|    policy_gradient_loss | -0.104     |
|    std                  | 9.15       |
|    value_loss           | 0.0597     |
----------------------------------------
2025-07-22 17:05:38,112 774412 INFO model saved to cm_sequencing_logs/model_stage_5_mass
2025-07-22 17:05:38,113 774412 INFO
Completed stage 5. Intervention 'mass' removed from list.
2025-07-22 17:05:38,114 774412 INFO Remaining interventions: 2
2025-07-22 17:05:38,114 774412 INFO CURRICULUM STAGE 6/7
2025-07-22 17:05:38,114 774412 INFO Remaining interventions: ['visual', 'angle']
2025-07-22 17:05:38,114 774412 INFO
Testing intervention 1/2: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 36 steps, reward: 1.804
total data points collected: 36
average episode length: 3.6
average episode reward: 0.180
termination reasons: ['success']
success rate: 1/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -0.420
total data points collected: 537
average episode length: 53.7
average episode reward: 0.138
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.526
total data points collected: 1038
average episode length: 103.8
average episode reward: -0.014
termination reasons: ['success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 1539
average episode length: 153.9
average episode reward: -0.369
termination reasons: ['success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2040
average episode length: 204.0
average episode reward: -0.304
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2541
average episode length: 254.1
average episode reward: -0.029
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3042
average episode length: 304.2
average episode reward: 0.038
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3543
average episode length: 354.3
average episode reward: -0.081
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4044
average episode length: 404.4
average episode reward: -0.139
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4545
average episode length: 454.5
average episode reward: -0.076
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4545, 56]), actions: torch.Size([4545, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3937', '7.6935', '6.4905', '7.8437', '6.8338']
Training reward models...
Reward model losses: ['0.0143', '0.1942', '0.0094', '0.8975', '0.2230']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0301', '0.9956', '1.0768', '1.0337', '1.0650']
Training action VAE models...
Action VAE losses: ['1.4078', '1.4873', '1.5220', '1.2872', '1.3286']
CM score components:
transition disagreement: 0.4159
reward disagreement: 0.3338
state disagreement: 0.4676
action disagreement: 0.5452
total CM score: 1.7625
2025-07-22 17:05:48,012 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 17:05:49,037 774412 INFO Episode 1: reward=-2.834, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 17:05:50,050 774412 INFO Episode 2: reward=-2.834, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 17:05:51,069 774412 INFO Episode 3: reward=-2.834, length=501, success=False
2025-07-22 17:05:58,155 774412 INFO Results: avg_reward=-2.834, success_rate=0.000, avg_length=501.0
2025-07-22 17:05:58,157 774412 INFO
Testing intervention 2/2: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -0.825
total data points collected: 501
average episode length: 50.1
average episode reward: -0.082
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: -2.457
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.328
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -3.446
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.673
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -1.051
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -1.544
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2537
average episode length: 253.7
average episode reward: -1.619
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 3038
average episode length: 303.8
average episode reward: -1.803
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 3086
average episode length: 308.6
average episode reward: -1.806
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 2/10
total data points collected: 3587
average episode length: 358.7
average episode reward: -2.013
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 3628
average episode length: 362.8
average episode reward: -2.150
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length', 'success']
success rate: 3/10
tensor shapes - states: torch.Size([3628, 56]), actions: torch.Size([3628, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8044', '6.9445', '6.7114', '7.8325', '6.7150']
Training reward models...
Reward model losses: ['0.1150', '0.0439', '0.0759', '0.2621', '0.0850']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7902', '1.8060', '1.7349', '1.7100', '1.6727']
Training action VAE models...
Action VAE losses: ['1.5912', '1.3396', '1.3672', '1.3799', '1.3579']
CM score components:
transition disagreement: 0.4341
reward disagreement: 0.0989
state disagreement: 0.5505
action disagreement: 0.5411
total CM score: 1.6247
2025-07-22 17:06:06,525 774412 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 17:06:07,503 774412 INFO Episode 1: reward=-1.283, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 17:06:08,525 774412 INFO Episode 2: reward=-4.434, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 17:06:08,624 774412 INFO Episode 3: reward=-1.394, length=25, success=True
2025-07-22 17:06:15,223 774412 INFO Results: avg_reward=-1.094, success_rate=0.100, avg_length=453.4
2025-07-22 17:06:15,225 774412 INFO Best intervention for stage 6: angle (Unified score: 0.5000)
2025-07-22 17:06:15,225 774412 INFO === stage 6/7: training on angle intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_6_angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
Reset #2: angle intervention applied (success: True)
Reset #3: angle intervention applied (success: True)
-------------------------------------
| custom/              |            |
|    intervention_type | angle      |
|    stage             | 6          |
| rollout/             |            |
|    ep_len_mean       | 399        |
|    ep_rew_mean       | -0.6910759 |
| time/                |            |
|    fps               | 443        |
|    iterations        | 1          |
|    time_elapsed      | 9          |
|    total_timesteps   | 5316608    |
-------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | -0.9370638  |
| time/                   |             |
|    fps                  | 267         |
|    iterations           | 2           |
|    time_elapsed         | 30          |
|    total_timesteps      | 5320704     |
| train/                  |             |
|    approx_kl            | 0.067680374 |
|    clip_fraction        | 0.576       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.8       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.769      |
|    n_updates            | 2145        |
|    policy_gradient_loss | -0.0979     |
|    std                  | 9.4         |
|    value_loss           | 0.0283      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | -0.9577721  |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 3           |
|    time_elapsed         | 51          |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.067521475 |
|    clip_fraction        | 0.578       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33         |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.772      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.103      |
|    std                  | 9.65        |
|    value_loss           | 0.0348      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 394        |
|    ep_rew_mean          | -1.0052783 |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 4          |
|    time_elapsed         | 72         |
|    total_timesteps      | 5328896    |
| train/                  |            |
|    approx_kl            | 0.07672306 |
|    clip_fraction        | 0.597      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.2      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.773     |
|    n_updates            | 2175       |
|    policy_gradient_loss | -0.105     |
|    std                  | 9.92       |
|    value_loss           | 0.0264     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 402        |
|    ep_rew_mean          | -1.0395643 |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 5          |
|    time_elapsed         | 93         |
|    total_timesteps      | 5332992    |
| train/                  |            |
|    approx_kl            | 0.07659538 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.782     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.106     |
|    std                  | 10         |
|    value_loss           | 0.0281     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | -1.2197313 |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 6          |
|    time_elapsed         | 115        |
|    total_timesteps      | 5337088    |
| train/                  |            |
|    approx_kl            | 0.06395786 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.788     |
|    n_updates            | 2205       |
|    policy_gradient_loss | -0.106     |
|    std                  | 10.2       |
|    value_loss           | 0.042      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | -1.2721163 |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 7          |
|    time_elapsed         | 136        |
|    total_timesteps      | 5341184    |
| train/                  |            |
|    approx_kl            | 0.0650155  |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.7      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.788     |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.11      |
|    std                  | 10.4       |
|    value_loss           | 0.0272     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 411        |
|    ep_rew_mean          | -1.4340935 |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 8          |
|    time_elapsed         | 157        |
|    total_timesteps      | 5345280    |
| train/                  |            |
|    approx_kl            | 0.06882358 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.8      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.791     |
|    n_updates            | 2235       |
|    policy_gradient_loss | -0.101     |
|    std                  | 10.5       |
|    value_loss           | 0.0214     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | -1.676785   |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 9           |
|    time_elapsed         | 178         |
|    total_timesteps      | 5349376     |
| train/                  |             |
|    approx_kl            | 0.077473804 |
|    clip_fraction        | 0.589       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34         |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.794      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.104      |
|    std                  | 10.8        |
|    value_loss           | 0.0234      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 411        |
|    ep_rew_mean          | -1.6520447 |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 10         |
|    time_elapsed         | 200        |
|    total_timesteps      | 5353472    |
| train/                  |            |
|    approx_kl            | 0.07589555 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.794     |
|    n_updates            | 2265       |
|    policy_gradient_loss | -0.101     |
|    std                  | 11.1       |
|    value_loss           | 0.0277     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 412        |
|    ep_rew_mean          | -1.6619481 |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 11         |
|    time_elapsed         | 221        |
|    total_timesteps      | 5357568    |
| train/                  |            |
|    approx_kl            | 0.07227192 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.4      |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.791     |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0946    |
|    std                  | 11.4       |
|    value_loss           | 0.0355     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 424        |
|    ep_rew_mean          | -1.8039212 |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 12         |
|    time_elapsed         | 243        |
|    total_timesteps      | 5361664    |
| train/                  |            |
|    approx_kl            | 0.07936223 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.805     |
|    n_updates            | 2295       |
|    policy_gradient_loss | -0.105     |
|    std                  | 11.6       |
|    value_loss           | 0.0385     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 420         |
|    ep_rew_mean          | -1.8417592  |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 13          |
|    time_elapsed         | 264         |
|    total_timesteps      | 5365760     |
| train/                  |             |
|    approx_kl            | 0.072134376 |
|    clip_fraction        | 0.573       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.81       |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.105      |
|    std                  | 11.9        |
|    value_loss           | 0.0253      |
-----------------------------------------
2025-07-22 17:10:51,937 774412 INFO model saved to cm_sequencing_logs/model_stage_6_angle
2025-07-22 17:10:51,939 774412 INFO
Completed stage 6. Intervention 'angle' removed from list.
2025-07-22 17:10:51,939 774412 INFO Remaining interventions: 1
2025-07-22 17:10:51,940 774412 INFO CURRICULUM STAGE 7/7
2025-07-22 17:10:51,940 774412 INFO Remaining interventions: ['visual']
2025-07-22 17:10:51,940 774412 INFO
Testing intervention 1/1: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: -4.703
total data points collected: 501
average episode length: 50.1
average episode reward: -0.470
termination reasons: ['max_length']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: -2.909
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.761
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.936
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.955
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -1.057
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2031
average episode length: 203.1
average episode reward: -1.193
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 2532
average episode length: 253.2
average episode reward: -1.018
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 3033
average episode length: 303.3
average episode reward: -1.752
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3534
average episode length: 353.4
average episode reward: -2.026
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3603
average episode length: 360.3
average episode reward: -2.071
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 4104
average episode length: 410.4
average episode reward: -2.125
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4104, 56]), actions: torch.Size([4104, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.4358', '6.8668', '7.2797', '7.7661', '8.1389']
Training reward models...
Reward model losses: ['0.0511', '0.0156', '0.1482', '0.0237', '0.1515']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4978', '1.3560', '1.2374', '1.3189', '1.3164']
Training action VAE models...
Action VAE losses: ['1.4413', '1.4556', '1.4686', '1.4472', '1.4182']
CM score components:
transition disagreement: 0.4557
reward disagreement: 0.0786
state disagreement: 0.5147
action disagreement: 0.5608
total CM score: 1.6098
2025-07-22 17:11:01,123 774412 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 17:11:02,084 774412 INFO Episode 1: reward=0.056, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 17:11:03,030 774412 INFO Episode 2: reward=0.056, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 17:11:03,975 774412 INFO Episode 3: reward=0.056, length=501, success=False
2025-07-22 17:11:10,595 774412 INFO Results: avg_reward=0.056, success_rate=0.000, avg_length=501.0
2025-07-22 17:11:10,597 774412 INFO Best intervention for stage 7: visual (Unified score: 0.0000)
2025-07-22 17:11:10,597 774412 INFO === stage 7/7: training on visual intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_7_visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
-------------------------------------
| custom/              |            |
|    intervention_type | visual     |
|    stage             | 7          |
| rollout/             |            |
|    ep_len_mean       | 416        |
|    ep_rew_mean       | -1.8812615 |
| time/                |            |
|    fps               | 442        |
|    iterations        | 1          |
|    time_elapsed      | 9          |
|    total_timesteps   | 5369856    |
-------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 425        |
|    ep_rew_mean          | -1.8124825 |
| time/                   |            |
|    fps                  | 266        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 5373952    |
| train/                  |            |
|    approx_kl            | 0.08266119 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.822     |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.106     |
|    std                  | 12.4       |
|    value_loss           | 0.0369     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | visual      |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 420         |
|    ep_rew_mean          | -1.7111146  |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 3           |
|    time_elapsed         | 52          |
|    total_timesteps      | 5378048     |
| train/                  |             |
|    approx_kl            | 0.070042595 |
|    clip_fraction        | 0.583       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.831      |
|    n_updates            | 2355        |
|    policy_gradient_loss | -0.109      |
|    std                  | 12.6        |
|    value_loss           | 0.0264      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | -1.7386633 |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 4          |
|    time_elapsed         | 73         |
|    total_timesteps      | 5382144    |
| train/                  |            |
|    approx_kl            | 0.07649417 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.836     |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.102     |
|    std                  | 12.9       |
|    value_loss           | 0.0234     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 415        |
|    ep_rew_mean          | -1.6959454 |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 5          |
|    time_elapsed         | 95         |
|    total_timesteps      | 5386240    |
| train/                  |            |
|    approx_kl            | 0.08025548 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.833     |
|    n_updates            | 2385       |
|    policy_gradient_loss | -0.107     |
|    std                  | 13.2       |
|    value_loss           | 0.0266     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 411        |
|    ep_rew_mean          | -1.6452943 |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 6          |
|    time_elapsed         | 116        |
|    total_timesteps      | 5390336    |
| train/                  |            |
|    approx_kl            | 0.07388218 |
|    clip_fraction        | 0.604      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36        |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.85      |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.113     |
|    std                  | 13.5       |
|    value_loss           | 0.0235     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 421        |
|    ep_rew_mean          | -1.6067313 |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 7          |
|    time_elapsed         | 138        |
|    total_timesteps      | 5394432    |
| train/                  |            |
|    approx_kl            | 0.085095   |
|    clip_fraction        | 0.601      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.2      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.839     |
|    n_updates            | 2415       |
|    policy_gradient_loss | -0.108     |
|    std                  | 13.9       |
|    value_loss           | 0.0287     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | -1.5640732 |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 8          |
|    time_elapsed         | 159        |
|    total_timesteps      | 5398528    |
| train/                  |            |
|    approx_kl            | 0.06433956 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.4      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.853     |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.109     |
|    std                  | 14.1       |
|    value_loss           | 0.0253     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 423        |
|    ep_rew_mean          | -1.4449342 |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 9          |
|    time_elapsed         | 181        |
|    total_timesteps      | 5402624    |
| train/                  |            |
|    approx_kl            | 0.06391514 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.841     |
|    n_updates            | 2445       |
|    policy_gradient_loss | -0.106     |
|    std                  | 14.3       |
|    value_loss           | 0.0292     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | visual      |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 423         |
|    ep_rew_mean          | -1.2888864  |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 10          |
|    time_elapsed         | 202         |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.070557795 |
|    clip_fraction        | 0.596       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.6       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.846      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.112      |
|    std                  | 14.5        |
|    value_loss           | 0.0287      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | -1.2204266 |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 11         |
|    time_elapsed         | 223        |
|    total_timesteps      | 5410816    |
| train/                  |            |
|    approx_kl            | 0.06126409 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.851     |
|    n_updates            | 2475       |
|    policy_gradient_loss | -0.105     |
|    std                  | 14.7       |
|    value_loss           | 0.0242     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | visual      |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 441         |
|    ep_rew_mean          | -1.1846833  |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 12          |
|    time_elapsed         | 245         |
|    total_timesteps      | 5414912     |
| train/                  |             |
|    approx_kl            | 0.072866395 |
|    clip_fraction        | 0.601       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.9       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.864      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.11       |
|    std                  | 15.1        |
|    value_loss           | 0.0249      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 421        |
|    ep_rew_mean          | -1.0698715 |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 13         |
|    time_elapsed         | 266        |
|    total_timesteps      | 5419008    |
| train/                  |            |
|    approx_kl            | 0.07233085 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.852     |
|    n_updates            | 2505       |
|    policy_gradient_loss | -0.102     |
|    std                  | 15.2       |
|    value_loss           | 0.0192     |
----------------------------------------
2025-07-22 17:15:49,630 774412 INFO model saved to cm_sequencing_logs/model_stage_7_visual
2025-07-22 17:15:49,632 774412 INFO
Completed stage 7. Intervention 'visual' removed from list.
2025-07-22 17:15:49,632 774412 INFO Remaining interventions: 0
Aggregated SB3 progress saved to cm_sequencing_logs/all_progress.csv
2025-07-22 17:15:57,063 774412 INFO SB3 training curves saved to cm_sequencing_logs/plots/training_curves_sb3.png
2025-07-22 17:15:57,063 774412 INFO ===final evaluation===
2025-07-22 17:16:12,105 774412 INFO episode 16: reward=-0.021, length=501, success=False
2025-07-22 17:16:13,045 774412 INFO episode 17: reward=-0.021, length=501, success=False
2025-07-22 17:16:13,984 774412 INFO episode 18: reward=-0.021, length=501, success=False
2025-07-22 17:16:14,931 774412 INFO episode 19: reward=-0.021, length=501, success=False
2025-07-22 17:16:15,875 774412 INFO episode 20: reward=-0.021, length=501, success=False
2025-07-22 17:16:15,876 774412 INFO Final performance
2025-07-22 17:16:15,876 774412 INFO average reward: -0.021 +/- 0.000
2025-07-22 17:16:15,876 774412 INFO success rate: 0.000
2025-07-22 17:16:15,876 774412 INFO average episode length: 501.0
2025-07-22 17:16:15,877 774412 INFO curriculum sequencing completed
2025-07-22 17:16:15,877 774412 INFO sequence order:
2025-07-22 17:16:15,877 774412 INFO 1. position (test_reward: -0.018)
2025-07-22 17:16:15,877 774412 INFO 2. goal (test_reward: 3.192)
2025-07-22 17:16:15,877 774412 INFO 3. random (test_reward: 1.656)
2025-07-22 17:16:15,877 774412 INFO 4. friction (test_reward: -1.280)
2025-07-22 17:16:15,877 774412 INFO 5. mass (test_reward: -0.537)
2025-07-22 17:16:15,877 774412 INFO 6. angle (test_reward: -1.094)
2025-07-22 17:16:15,877 774412 INFO 7. visual (test_reward: 0.056)
2025-07-22 17:16:15,878 774412 INFO
performance comparison
2025-07-22 17:16:15,878 774412 INFO initial: 3.394 reward, 1.000 success
2025-07-22 17:16:15,878 774412 INFO final: -0.021 reward, 0.000 success
2025-07-22 17:16:15,878 774412 INFO improvement: -3.414 reward
2025-07-22 17:16:15,891 774412 INFO Final model saved to cm_sequencing_logs/final_model_after_sequencing.zip
Traceback (most recent call last):
  File "baselines.py", line 1399, in <module>
    plots_dir = os.path.join(log_dir, "plots")
  File "baselines.py", line 1303, in main
    'sequence': completed_sequence,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/__init__.py", line 179, in dump
    for chunk in iterable:
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 325, in _iterencode_list
    yield from chunks
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 438, in _iterencode
    o = _default(o)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type type is not JSON serializable
