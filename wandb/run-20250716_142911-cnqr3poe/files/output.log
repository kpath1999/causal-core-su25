
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 10
   student training steps: 10000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:228: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4623', '1.2807', '1.2986', '1.2467', '1.3687']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.4949
action disagreement: 0.5153
total CM score: 1.7433
goal is complete. CM score: 1.7433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6173', '1.5881', '1.5435', '1.7845', '1.4572']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.5085
action disagreement: 0.5139
total CM score: 1.5571
mass is complete. CM score: 1.5571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3495', '1.5457', '1.8414', '1.6155', '1.6662']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.4994
action disagreement: 0.5360
total CM score: 1.5964
friction is complete. CM score: 1.5964
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.6746', '1.3558', '1.4321', '1.5538']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.4757
action disagreement: 0.5367
total CM score: 1.4193
visual is complete. CM score: 1.4193
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7699', '1.7515', '1.6568', '1.7130', '1.9990']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5431
action disagreement: 0.5396
total CM score: 1.6187
pose is complete. CM score: 1.6187
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3205', '1.5355', '1.5504', '1.3586', '1.4045']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.5242
action disagreement: 0.5467
total CM score: 1.8817
random is complete. CM score: 1.8817
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 460       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.030029923 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.529      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0914     |
|    std                  | 2.66        |
|    value_loss           | 0.0522      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 3           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.034628637 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.539      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.088      |
|    std                  | 2.67        |
|    value_loss           | 0.0245      |
-----------------------------------------
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_0.zip

evaluating student performance (5 episodes)...
episode 1: length=30, reward=-0.071, success=True
episode 2: length=30, reward=-0.071, success=True
episode 3: length=30, reward=-0.071, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.071
average episode length: 30.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.667
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.987
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.255
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.654
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6616', '8.3757', '7.3218', '7.7552', '8.1955']
Training reward models...
Reward model losses: ['0.0039', '0.2415', '0.0301', '1.4375', '1.7175']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2921', '1.3409', '1.1455', '1.1171', '1.1787']
Training action VAE models...
Action VAE losses: ['0.8644', '0.8392', '0.9063', '0.9003', '0.8912']
CM score components:
transition disagreement: 0.4428
reward disagreement: 0.4744
state disagreement: 0.4850
action disagreement: 0.5218
total CM score: 1.9240
goal is complete. CM score: 1.9240
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.419
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 5.381
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.717
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.910
termination reasons: ['max_length', 'success', 'success', 'max_length', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6777', '7.3139', '7.8172', '7.5691', '6.7929']
Training reward models...
Reward model losses: ['0.5977', '0.4633', '0.0107', '0.1204', '0.2557']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5592', '1.3000', '1.5235', '1.5057', '1.8064']
Training action VAE models...
Action VAE losses: ['0.8598', '0.8510', '0.8082', '0.8861', '0.8890']
CM score components:
transition disagreement: 0.4293
reward disagreement: 0.3214
state disagreement: 0.5049
action disagreement: 0.5000
total CM score: 1.7556
mass is complete. CM score: 1.7556
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.754
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.535
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.490
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.085
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3770', '7.3520', '7.2140', '6.8148', '8.0313']
Training reward models...
Reward model losses: ['0.7387', '0.0319', '0.0344', '0.0375', '0.2270']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8222', '1.6877', '1.4305', '1.8662', '1.5514']
Training action VAE models...
Action VAE losses: ['0.9324', '0.9244', '0.9332', '0.9486', '0.9968']
CM score components:
transition disagreement: 0.4115
reward disagreement: 0.2794
state disagreement: 0.5201
action disagreement: 0.5249
total CM score: 1.7360
friction is complete. CM score: 1.7360
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.482
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.482
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.482
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.482
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6613', '7.5968', '7.6609', '7.6448', '7.2517']
Training reward models...
Reward model losses: ['0.5076', '0.5199', '0.0033', '0.0122', '0.1794']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4844', '1.7714', '1.6763', '1.6355', '1.4203']
Training action VAE models...
Action VAE losses: ['0.8892', '0.9243', '1.0302', '0.9597', '0.9364']
CM score components:
transition disagreement: 0.4080
reward disagreement: 0.1841
state disagreement: 0.5319
action disagreement: 0.5574
total CM score: 1.6814
visual is complete. CM score: 1.6814
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.801
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.316
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.943
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.191
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9448', '7.0877', '8.2069', '8.0133', '6.6458']
Training reward models...
Reward model losses: ['0.3822', '0.0100', '0.1310', '0.0140', '0.0188']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1908', '2.1702', '1.9373', '1.7970', '1.9431']
Training action VAE models...
Action VAE losses: ['1.3757', '1.3086', '1.2725', '1.4055', '1.3669']
CM score components:
transition disagreement: 0.4310
reward disagreement: 0.0856
state disagreement: 0.5827
action disagreement: 0.5143
total CM score: 1.6136
pose is complete. CM score: 1.6136
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.760
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.612
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: 0.540
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.087
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6740', '7.5919', '6.8594', '7.9568', '7.8938']
Training reward models...
Reward model losses: ['0.9668', '0.4467', '0.0963', '0.9782', '0.0237']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3650', '1.3890', '1.5778', '1.3587', '1.4321']
Training action VAE models...
Action VAE losses: ['1.4838', '1.5010', '1.4033', '1.3524', '1.4127']
CM score components:
transition disagreement: 0.4184
reward disagreement: 0.5055
state disagreement: 0.5086
action disagreement: 0.5534
total CM score: 1.9858
random is complete. CM score: 1.9858
INFO:root:Meta-Episode 1/10: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
Traceback (most recent call last):
  File "meta_teacher_student.py", line 811, in <module>
    main()
  File "meta_teacher_student.py", line 784, in main
    'teacher_q_values': teacher.q_net(torch.tensor(S_prime_teacher.to(device)).detach().cpu().numpy().tolist())
AttributeError: 'numpy.ndarray' object has no attribute 'to'
