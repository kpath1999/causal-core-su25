2025-07-22 18:31:36,777 905523 INFO [PRETRAINED] Using pretrained model path: ppo_pushing_sb3/final_model.zip
2025-07-22 18:31:36,779 905523 INFO Starting with 7 interventions
2025-07-22 18:31:36,780 905523 INFO ===final evaluation===
2025-07-22 18:31:47,344 905523 INFO Final performance
2025-07-22 18:31:47,344 905523 INFO average reward: 3.394 +/- 0.000
2025-07-22 18:31:47,345 905523 INFO success rate: 1.000
2025-07-22 18:31:47,345 905523 INFO average episode length: 501.0
2025-07-22 18:31:47,345 905523 INFO initial performance: {'avg_reward': 3.393721938342554, 'reward_std': 4.440892098500626e-16, 'avg_length': 501.0, 'success_rate': 1.0, 'total_episodes': 10}
2025-07-22 18:31:47,345 905523 INFO CURRICULUM STAGE 1/7
2025-07-22 18:31:47,345 905523 INFO Remaining interventions: ['goal', 'mass', 'friction', 'visual', 'position', 'angle', 'random']
2025-07-22 18:31:47,345 905523 INFO
Testing intervention 1/7: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.176
total data points collected: 501
average episode length: 50.1
average episode reward: -0.018
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.340
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.216
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.679
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.149
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.008
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.141
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.376
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.420
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.463
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.595
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.421
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
baselines.py:200: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3214', '7.0977', '6.8085', '7.1494', '7.6413']
Training reward models...
Reward model losses: ['0.2433', '0.2068', '0.2358', '0.0093', '0.0194']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2406', '1.2135', '1.2528', '1.3185', '1.3497']
Training action VAE models...
Action VAE losses: ['1.5555', '1.4313', '1.4925', '1.3706', '1.3680']
CM score components:
transition disagreement: 0.4095
reward disagreement: 0.0940
state disagreement: 0.4722
action disagreement: 0.5613
total CM score: 1.5369
2025-07-22 18:31:58,159 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 18:31:58,271 905523 INFO Episode 1: reward=1.270, length=20, success=True
Reset #2: goal intervention applied (success: True)
2025-07-22 18:31:59,293 905523 INFO Episode 2: reward=0.955, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 18:32:00,353 905523 INFO Episode 3: reward=2.391, length=501, success=False
2025-07-22 18:32:07,474 905523 INFO Results: avg_reward=0.533, success_rate=0.100, avg_length=452.9
2025-07-22 18:32:07,477 905523 INFO
Testing intervention 2/7: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 33 steps, reward: -0.196
total data points collected: 33
average episode length: 3.3
average episode reward: -0.020
termination reasons: ['success']
success rate: 1/10
Reset #2: mass intervention applied (success: True)
episode 2: 33 steps, reward: -0.226
total data points collected: 66
average episode length: 6.6
average episode reward: -0.042
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.175
total data points collected: 567
average episode length: 56.7
average episode reward: 0.275
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 606
average episode length: 60.6
average episode reward: 0.171
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 869
average episode length: 86.9
average episode reward: 0.407
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 1370
average episode length: 137.0
average episode reward: 0.560
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 1871
average episode length: 187.1
average episode reward: 0.995
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 2372
average episode length: 237.2
average episode reward: 1.482
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 2410
average episode length: 241.0
average episode reward: 1.536
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 5/10
total data points collected: 2533
average episode length: 253.3
average episode reward: 1.517
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 6/10
tensor shapes - states: torch.Size([2533, 56]), actions: torch.Size([2533, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0618', '7.7143', '7.1078', '6.8695', '8.2497']
Training reward models...
Reward model losses: ['0.0085', '0.3513', '0.0077', '0.3001', '0.5900']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8094', '1.8174', '1.8916', '1.9281', '2.4819']
Training action VAE models...
Action VAE losses: ['1.4794', '1.4375', '1.3607', '1.4333', '1.3518']
CM score components:
transition disagreement: 0.4201
reward disagreement: 0.2101
state disagreement: 0.6020
action disagreement: 0.5512
total CM score: 1.7833
2025-07-22 18:32:13,702 905523 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 18:32:13,833 905523 INFO Episode 1: reward=-0.245, length=33, success=True
Reset #2: mass intervention applied (success: True)
2025-07-22 18:32:13,946 905523 INFO Episode 2: reward=-0.088, length=31, success=True
Reset #3: mass intervention applied (success: True)
2025-07-22 18:32:14,053 905523 INFO Episode 3: reward=-0.618, length=29, success=True
2025-07-22 18:32:14,824 905523 INFO Results: avg_reward=-0.426, success_rate=1.000, avg_length=31.1
2025-07-22 18:32:14,826 905523 INFO
Testing intervention 3/7: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 40 steps, reward: 0.452
total data points collected: 40
average episode length: 4.0
average episode reward: 0.045
termination reasons: ['success']
success rate: 1/10
Reset #2: friction intervention applied (success: True)
episode 2: 40 steps, reward: -0.118
total data points collected: 80
average episode length: 8.0
average episode reward: 0.033
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.317
total data points collected: 581
average episode length: 58.1
average episode reward: 0.365
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 613
average episode length: 61.3
average episode reward: 0.354
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 877
average episode length: 87.7
average episode reward: 0.609
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 1378
average episode length: 137.8
average episode reward: 1.002
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 1879
average episode length: 187.9
average episode reward: 1.386
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 1961
average episode length: 196.1
average episode reward: 1.438
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success']
success rate: 5/10
total data points collected: 1996
average episode length: 199.6
average episode reward: 1.429
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success', 'success']
success rate: 6/10
total data points collected: 2497
average episode length: 249.7
average episode reward: 1.617
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 6/10
tensor shapes - states: torch.Size([2497, 56]), actions: torch.Size([2497, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9008', '7.2530', '6.8730', '7.7103', '7.0574']
Training reward models...
Reward model losses: ['0.0653', '1.1019', '0.5024', '0.0242', '0.2265']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7264', '1.5889', '1.5984', '1.8298', '1.7181']
Training action VAE models...
Action VAE losses: ['1.2996', '1.3674', '1.2929', '1.3067', '1.4268']
CM score components:
transition disagreement: 0.4215
reward disagreement: 0.4378
state disagreement: 0.5306
action disagreement: 0.5179
total CM score: 1.9078
2025-07-22 18:32:20,968 905523 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 18:32:21,083 905523 INFO Episode 1: reward=-0.775, length=27, success=True
Reset #2: friction intervention applied (success: True)
2025-07-22 18:32:21,188 905523 INFO Episode 2: reward=-0.774, length=27, success=True
Reset #3: friction intervention applied (success: True)
2025-07-22 18:32:21,291 905523 INFO Episode 3: reward=-0.774, length=27, success=True
2025-07-22 18:32:22,021 905523 INFO Results: avg_reward=-0.820, success_rate=1.000, avg_length=27.4
2025-07-22 18:32:22,023 905523 INFO
Testing intervention 4/7: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.799
total data points collected: 501
average episode length: 50.1
average episode reward: 0.380
termination reasons: ['max_length']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.047
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.685
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: visual intervention applied (success: True)
episode 3: 401 steps, reward: 4.829
total data points collected: 1403
average episode length: 140.3
average episode reward: 1.167
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1904
average episode length: 190.4
average episode reward: 1.487
termination reasons: ['max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 1.396
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 2/10
total data points collected: 1978
average episode length: 197.8
average episode reward: 1.337
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 2010
average episode length: 201.0
average episode reward: 1.159
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 2511
average episode length: 251.1
average episode reward: 1.386
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 4/10
total data points collected: 3012
average episode length: 301.2
average episode reward: 1.803
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length']
success rate: 4/10
total data points collected: 3513
average episode length: 351.3
average episode reward: 2.134
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 4/10
tensor shapes - states: torch.Size([3513, 56]), actions: torch.Size([3513, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1150', '7.7619', '7.0914', '7.1919', '7.4913']
Training reward models...
Reward model losses: ['0.0551', '0.2885', '0.1062', '0.0198', '0.0581']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6096', '1.7050', '1.6790', '1.7611', '1.6296']
Training action VAE models...
Action VAE losses: ['1.4084', '1.3043', '1.3701', '1.4613', '1.5215']
CM score components:
transition disagreement: 0.4395
reward disagreement: 0.1506
state disagreement: 0.5061
action disagreement: 0.5374
total CM score: 1.6336
2025-07-22 18:32:29,998 905523 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 18:32:30,118 905523 INFO Episode 1: reward=-0.773, length=27, success=True
Reset #2: visual intervention applied (success: True)
2025-07-22 18:32:30,224 905523 INFO Episode 2: reward=-0.773, length=27, success=True
Reset #3: visual intervention applied (success: True)
2025-07-22 18:32:30,328 905523 INFO Episode 3: reward=-0.773, length=27, success=True
2025-07-22 18:32:31,059 905523 INFO Results: avg_reward=-0.773, success_rate=1.000, avg_length=27.0
2025-07-22 18:32:31,061 905523 INFO
Testing intervention 5/7: position (CM score)
IntervenedCausalWorld created with position intervention
evaluating CM score for position intervention...
Reset #1: position intervention applied (success: True)
episode 1: 35 steps, reward: -1.696
total data points collected: 35
average episode length: 3.5
average episode reward: -0.170
termination reasons: ['success']
success rate: 1/10
Reset #2: position intervention applied (success: True)
episode 2: 501 steps, reward: -3.449
total data points collected: 536
average episode length: 53.6
average episode reward: -0.515
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: position intervention applied (success: True)
episode 3: 501 steps, reward: 2.055
total data points collected: 1037
average episode length: 103.7
average episode reward: -0.309
termination reasons: ['success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 1538
average episode length: 153.8
average episode reward: -0.190
termination reasons: ['success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2039
average episode length: 203.9
average episode reward: -0.029
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2540
average episode length: 254.0
average episode reward: 0.394
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3041
average episode length: 304.1
average episode reward: 0.628
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3542
average episode length: 354.2
average episode reward: 0.782
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 0.803
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 4078
average episode length: 407.8
average episode reward: 1.155
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4078, 56]), actions: torch.Size([4078, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7852', '7.7244', '7.6858', '6.6319', '7.3930']
Training reward models...
Reward model losses: ['3.0782', '1.1042', '0.1575', '0.0203', '0.0340']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6499', '1.7088', '1.5572', '1.6576', '1.4569']
Training action VAE models...
Action VAE losses: ['1.3798', '1.5109', '1.4191', '1.4644', '1.3449']
CM score components:
transition disagreement: 0.4450
reward disagreement: 0.7511
state disagreement: 0.5280
action disagreement: 0.5551
total CM score: 2.2792
2025-07-22 18:32:40,135 905523 INFO testing intervention: position
IntervenedCausalWorld created with position intervention
Reset #1: position intervention applied (success: True)
2025-07-22 18:32:40,256 905523 INFO Episode 1: reward=-2.303, length=30, success=True
Reset #2: position intervention applied (success: True)
2025-07-22 18:32:41,313 905523 INFO Episode 2: reward=3.930, length=501, success=False
Reset #3: position intervention applied (success: True)
2025-07-22 18:32:42,369 905523 INFO Episode 3: reward=4.586, length=501, success=False
2025-07-22 18:32:48,400 905523 INFO Results: avg_reward=1.162, success_rate=0.200, avg_length=406.6
2025-07-22 18:32:48,402 905523 INFO
Testing intervention 6/7: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 338 steps, reward: 2.353
total data points collected: 338
average episode length: 33.8
average episode reward: 0.235
termination reasons: ['success']
success rate: 1/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: 0.355
total data points collected: 839
average episode length: 83.9
average episode reward: 0.271
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 33 steps, reward: -1.702
total data points collected: 872
average episode length: 87.2
average episode reward: 0.101
termination reasons: ['success', 'max_length', 'success']
success rate: 2/10
total data points collected: 1201
average episode length: 120.1
average episode reward: 0.305
termination reasons: ['success', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 1702
average episode length: 170.2
average episode reward: 0.311
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length']
success rate: 3/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 0.525
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 4/10
total data points collected: 1975
average episode length: 197.5
average episode reward: 0.541
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success']
success rate: 5/10
total data points collected: 2476
average episode length: 247.6
average episode reward: 0.835
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length']
success rate: 5/10
total data points collected: 2516
average episode length: 251.6
average episode reward: 0.395
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 6/10
total data points collected: 3017
average episode length: 301.7
average episode reward: 0.351
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length']
success rate: 6/10
tensor shapes - states: torch.Size([3017, 56]), actions: torch.Size([3017, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4578', '7.3447', '8.3567', '7.3963', '7.3306']
Training reward models...
Reward model losses: ['0.1261', '0.0766', '0.0544', '0.3345', '0.1984']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4926', '1.4158', '1.3596', '1.4830', '1.5776']
Training action VAE models...
Action VAE losses: ['1.4831', '1.4249', '1.4275', '1.4818', '1.4880']
CM score components:
transition disagreement: 0.3766
reward disagreement: 0.1347
state disagreement: 0.5085
action disagreement: 0.5841
total CM score: 1.6039
2025-07-22 18:32:55,431 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 18:32:56,464 905523 INFO Episode 1: reward=2.751, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 18:32:57,512 905523 INFO Episode 2: reward=2.993, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 18:32:58,529 905523 INFO Episode 3: reward=2.576, length=501, success=False
2025-07-22 18:33:05,639 905523 INFO Results: avg_reward=0.975, success_rate=0.000, avg_length=501.0
2025-07-22 18:33:05,641 905523 INFO
Testing intervention 7/7: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 225 steps, reward: 4.185
total data points collected: 225
average episode length: 22.5
average episode reward: 0.419
termination reasons: ['success']
success rate: 1/10
Reset #2: random intervention applied (success: True)
episode 2: 46 steps, reward: 1.844
total data points collected: 271
average episode length: 27.1
average episode reward: 0.603
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.957
total data points collected: 772
average episode length: 77.2
average episode reward: 0.507
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 1273
average episode length: 127.3
average episode reward: 0.240
termination reasons: ['success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 1774
average episode length: 177.4
average episode reward: 0.578
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2275
average episode length: 227.5
average episode reward: 0.730
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2776
average episode length: 277.6
average episode reward: 0.988
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3038
average episode length: 303.8
average episode reward: 1.275
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3539
average episode length: 353.9
average episode reward: 1.057
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 1.048
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3577, 56]), actions: torch.Size([3577, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1869', '7.1633', '7.7889', '7.8317', '8.8476']
Training reward models...
Reward model losses: ['0.2678', '0.0461', '0.3041', '0.0354', '0.0399']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2985', '1.3248', '1.3389', '1.2574', '1.2420']
Training action VAE models...
Action VAE losses: ['1.4852', '1.5103', '1.3810', '1.4729', '1.4495']
CM score components:
transition disagreement: 0.4526
reward disagreement: 0.1635
state disagreement: 0.4906
action disagreement: 0.5830
total CM score: 1.6898
2025-07-22 18:33:13,724 905523 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 18:33:14,765 905523 INFO Episode 1: reward=6.065, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 18:33:15,751 905523 INFO Episode 2: reward=-0.789, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 18:33:16,735 905523 INFO Episode 3: reward=2.712, length=501, success=False
2025-07-22 18:33:22,930 905523 INFO Results: avg_reward=1.483, success_rate=0.100, avg_length=455.2
2025-07-22 18:33:22,932 905523 INFO [Stage 1] goal         | reward=0.53 (norm 0.587) | CM=1.537 (norm 0.000) | α=0.50 → unified=0.294
2025-07-22 18:33:22,932 905523 INFO [Stage 1] mass         | reward=-0.43 (norm 0.171) | CM=1.783 (norm 0.332) | α=0.50 → unified=0.251
2025-07-22 18:33:22,933 905523 INFO [Stage 1] friction     | reward=-0.82 (norm 0.000) | CM=1.908 (norm 0.500) | α=0.50 → unified=0.250
2025-07-22 18:33:22,933 905523 INFO [Stage 1] visual       | reward=-0.77 (norm 0.020) | CM=1.634 (norm 0.130) | α=0.50 → unified=0.075
2025-07-22 18:33:22,933 905523 INFO [Stage 1] position     | reward=1.16 (norm 0.861) | CM=2.279 (norm 1.000) | α=0.50 → unified=0.930
2025-07-22 18:33:22,933 905523 INFO [Stage 1] angle        | reward=0.98 (norm 0.780) | CM=1.604 (norm 0.090) | α=0.50 → unified=0.435
2025-07-22 18:33:22,933 905523 INFO [Stage 1] random       | reward=1.48 (norm 1.000) | CM=1.690 (norm 0.206) | α=0.50 → unified=0.603
2025-07-22 18:33:22,933 905523 INFO Best intervention for stage 1: position (Unified score: 0.9303)
2025-07-22 18:33:22,933 905523 INFO === stage 1/7: training on position intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_1_position
IntervenedCausalWorld created with position intervention
Reset #1: position intervention applied (success: True)
Reset #2: position intervention applied (success: True)
Reset #3: position intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | position  |
|    stage             | 1         |
| rollout/             |           |
|    ep_len_mean       | 268       |
|    ep_rew_mean       | 2.0396163 |
| time/                |           |
|    fps               | 453       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5050368   |
------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.9497699   |
| time/                   |             |
|    fps                  | 277         |
|    iterations           | 2           |
|    time_elapsed         | 29          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.044647623 |
|    clip_fraction        | 0.493       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.108      |
|    std                  | 2.67        |
|    value_loss           | 0.0252      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 1.6769154   |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 3           |
|    time_elapsed         | 49          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.037631687 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.534      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0902     |
|    std                  | 2.7         |
|    value_loss           | 0.0208      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 1.5872107   |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 4           |
|    time_elapsed         | 70          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.065796316 |
|    clip_fraction        | 0.565       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.105      |
|    std                  | 2.71        |
|    value_loss           | 0.0299      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 339         |
|    ep_rew_mean          | 1.3940446   |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 5           |
|    time_elapsed         | 90          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.043427818 |
|    clip_fraction        | 0.434       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.547      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0917     |
|    std                  | 2.73        |
|    value_loss           | 0.0314      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 1.2106693   |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 6           |
|    time_elapsed         | 110         |
|    total_timesteps      | 5070848     |
| train/                  |             |
|    approx_kl            | 0.046077747 |
|    clip_fraction        | 0.47        |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.541      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 2.75        |
|    value_loss           | 0.0462      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 1.0472485   |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 7           |
|    time_elapsed         | 131         |
|    total_timesteps      | 5074944     |
| train/                  |             |
|    approx_kl            | 0.052667886 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.546      |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.102      |
|    std                  | 2.78        |
|    value_loss           | 0.0315      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 0.85194606  |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 8           |
|    time_elapsed         | 151         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.061902195 |
|    clip_fraction        | 0.549       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.56       |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.106      |
|    std                  | 2.81        |
|    value_loss           | 0.0199      |
-----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | position  |
|    stage                | 1         |
| rollout/                |           |
|    ep_len_mean          | 412       |
|    ep_rew_mean          | 0.7065226 |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 9         |
|    time_elapsed         | 172       |
|    total_timesteps      | 5083136   |
| train/                  |           |
|    approx_kl            | 0.3833355 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.558    |
|    n_updates            | 1275      |
|    policy_gradient_loss | -0.11     |
|    std                  | 2.83      |
|    value_loss           | 0.0188    |
---------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 0.3684297  |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 10         |
|    time_elapsed         | 192        |
|    total_timesteps      | 5087232    |
| train/                  |            |
|    approx_kl            | 0.04994037 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00025    |
|    loss                 | -0.565     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.11      |
|    std                  | 2.85       |
|    value_loss           | 0.0199     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 448        |
|    ep_rew_mean          | 0.24491644 |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 11         |
|    time_elapsed         | 212        |
|    total_timesteps      | 5091328    |
| train/                  |            |
|    approx_kl            | 0.06606398 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.557     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.104     |
|    std                  | 2.9        |
|    value_loss           | 0.028      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 0.30077672 |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 12         |
|    time_elapsed         | 233        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.06117303 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.578     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.115     |
|    std                  | 2.94       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 436         |
|    ep_rew_mean          | 0.3670481   |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 13          |
|    time_elapsed         | 253         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.078939214 |
|    clip_fraction        | 0.624       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.583      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.115      |
|    std                  | 2.99        |
|    value_loss           | 0.0146      |
-----------------------------------------
2025-07-22 18:37:48,062 905523 INFO model saved to cm_sequencing_logs/model_stage_1_position
2025-07-22 18:37:48,064 905523 INFO
Completed stage 1. Intervention 'position' removed from list.
2025-07-22 18:37:48,064 905523 INFO Remaining interventions: 6
2025-07-22 18:37:48,064 905523 INFO CURRICULUM STAGE 2/7
2025-07-22 18:37:48,065 905523 INFO Remaining interventions: ['goal', 'mass', 'friction', 'visual', 'angle', 'random']
2025-07-22 18:37:48,065 905523 INFO
Testing intervention 1/6: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 0.463
total data points collected: 501
average episode length: 50.1
average episode reward: 0.046
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 295 steps, reward: 2.686
total data points collected: 796
average episode length: 79.6
average episode reward: 0.315
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -1.758
total data points collected: 1297
average episode length: 129.7
average episode reward: 0.139
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1798
average episode length: 179.8
average episode reward: 0.152
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2299
average episode length: 229.9
average episode reward: 0.354
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2800
average episode length: 280.0
average episode reward: 0.337
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3301
average episode length: 330.1
average episode reward: 0.306
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3802
average episode length: 380.2
average episode reward: 0.438
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4303
average episode length: 430.3
average episode reward: 0.637
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4804
average episode length: 480.4
average episode reward: 0.248
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4804, 56]), actions: torch.Size([4804, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4697', '7.2781', '7.6518', '8.5692', '7.9831']
Training reward models...
Reward model losses: ['0.0168', '0.1472', '1.7272', '0.2074', '0.1726']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3364', '1.5354', '1.3675', '1.2900', '1.4034']
Training action VAE models...
Action VAE losses: ['1.3965', '1.4227', '1.3653', '1.3740', '1.3704']
CM score components:
transition disagreement: 0.4291
reward disagreement: 0.4628
state disagreement: 0.5345
action disagreement: 0.5425
total CM score: 1.9687
2025-07-22 18:37:58,448 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 18:37:59,408 905523 INFO Episode 1: reward=0.240, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 18:38:00,402 905523 INFO Episode 2: reward=-0.499, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 18:38:01,341 905523 INFO Episode 3: reward=1.800, length=501, success=False
2025-07-22 18:38:07,503 905523 INFO Results: avg_reward=0.920, success_rate=0.100, avg_length=453.0
2025-07-22 18:38:07,505 905523 INFO
Testing intervention 2/6: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.560
total data points collected: 501
average episode length: 50.1
average episode reward: 0.356
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 3.396
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.696
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.286
total data points collected: 1503
average episode length: 150.3
average episode reward: 1.124
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 1632
average episode length: 163.2
average episode reward: 1.243
termination reasons: ['max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1672
average episode length: 167.2
average episode reward: 1.273
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 2173
average episode length: 217.3
average episode reward: 1.398
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 2674
average episode length: 267.4
average episode reward: 1.412
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3175
average episode length: 317.5
average episode reward: 1.727
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3204
average episode length: 320.4
average episode reward: 1.798
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3242
average episode length: 324.2
average episode reward: 1.894
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3242, 56]), actions: torch.Size([3242, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2166', '7.6009', '7.0086', '7.6727', '6.4510']
Training reward models...
Reward model losses: ['0.0171', '0.0340', '0.1850', '0.0238', '0.2576']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.9954', '1.8801', '2.0157', '1.9890', '1.9011']
Training action VAE models...
Action VAE losses: ['1.4605', '1.4743', '1.3381', '1.2991', '1.5320']
CM score components:
transition disagreement: 0.4383
reward disagreement: 0.1509
state disagreement: 0.5499
action disagreement: 0.5441
total CM score: 1.6832
2025-07-22 18:38:14,948 905523 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 18:38:15,918 905523 INFO Episode 1: reward=4.137, length=501, success=False
Reset #2: mass intervention applied (success: True)
2025-07-22 18:38:16,867 905523 INFO Episode 2: reward=4.556, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 18:38:16,978 905523 INFO Episode 3: reward=0.982, length=32, success=True
2025-07-22 18:38:23,064 905523 INFO Results: avg_reward=3.573, success_rate=0.200, avg_length=407.2
2025-07-22 18:38:23,065 905523 INFO
Testing intervention 3/6: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: -0.782
total data points collected: 501
average episode length: 50.1
average episode reward: -0.078
termination reasons: ['max_length']
success rate: 0/10
Reset #2: friction intervention applied (success: True)
episode 2: 264 steps, reward: 1.648
total data points collected: 765
average episode length: 76.5
average episode reward: 0.087
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 1.547
total data points collected: 1266
average episode length: 126.6
average episode reward: 0.241
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1767
average episode length: 176.7
average episode reward: 0.212
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2268
average episode length: 226.8
average episode reward: 0.051
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2769
average episode length: 276.9
average episode reward: -0.224
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2808
average episode length: 280.8
average episode reward: -0.281
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 2844
average episode length: 284.4
average episode reward: -0.179
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 2872
average episode length: 287.2
average episode reward: -0.166
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 3373
average episode length: 337.3
average episode reward: 0.124
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 4/10
tensor shapes - states: torch.Size([3373, 56]), actions: torch.Size([3373, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9254', '7.5901', '7.2962', '6.8289', '7.5785']
Training reward models...
Reward model losses: ['0.0088', '0.0502', '0.2647', '0.2440', '0.2046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6881', '1.5163', '1.6501', '1.9112', '1.5810']
Training action VAE models...
Action VAE losses: ['1.5875', '1.4398', '1.4791', '1.4553', '1.4522']
CM score components:
transition disagreement: 0.4272
reward disagreement: 0.1618
state disagreement: 0.5529
action disagreement: 0.5811
total CM score: 1.7229
2025-07-22 18:38:30,709 905523 INFO testing intervention: friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
2025-07-22 18:38:31,654 905523 INFO Episode 1: reward=4.698, length=501, success=False
Reset #2: friction intervention applied (success: True)
2025-07-22 18:38:32,599 905523 INFO Episode 2: reward=4.579, length=501, success=False
Reset #3: friction intervention applied (success: True)
2025-07-22 18:38:33,546 905523 INFO Episode 3: reward=4.582, length=501, success=False
2025-07-22 18:38:40,178 905523 INFO Results: avg_reward=4.476, success_rate=0.000, avg_length=501.0
2025-07-22 18:38:40,179 905523 INFO
Testing intervention 4/6: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 55 steps, reward: 1.031
total data points collected: 55
average episode length: 5.5
average episode reward: 0.103
termination reasons: ['success']
success rate: 1/10
Reset #2: visual intervention applied (success: True)
episode 2: 31 steps, reward: 0.100
total data points collected: 86
average episode length: 8.6
average episode reward: 0.113
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: -1.161
total data points collected: 587
average episode length: 58.7
average episode reward: -0.003
termination reasons: ['success', 'success', 'max_length']
success rate: 2/10
total data points collected: 621
average episode length: 62.1
average episode reward: 0.027
termination reasons: ['success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 676
average episode length: 67.6
average episode reward: 0.123
termination reasons: ['success', 'success', 'max_length', 'success', 'success']
success rate: 4/10
total data points collected: 712
average episode length: 71.2
average episode reward: 0.223
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success']
success rate: 5/10
total data points collected: 1213
average episode length: 121.3
average episode reward: 0.760
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length']
success rate: 5/10
total data points collected: 1243
average episode length: 124.3
average episode reward: 0.854
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success']
success rate: 6/10
total data points collected: 1277
average episode length: 127.7
average episode reward: 0.749
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success', 'success']
success rate: 7/10
total data points collected: 1317
average episode length: 131.7
average episode reward: 0.662
termination reasons: ['success', 'success', 'max_length', 'success', 'success', 'success', 'max_length', 'success', 'success', 'success']
success rate: 8/10
tensor shapes - states: torch.Size([1317, 56]), actions: torch.Size([1317, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4718', '7.3469', '7.5401', '7.3774', '7.3294']
Training reward models...
Reward model losses: ['0.2178', '0.0375', '0.3594', '0.0220', '0.0177']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2681', '1.3573', '1.3982', '1.4295', '1.3115']
Training action VAE models...
Action VAE losses: ['1.2959', '1.3916', '1.3979', '1.4953', '1.4806']
CM score components:
transition disagreement: 0.4281
reward disagreement: 0.1849
state disagreement: 0.5227
action disagreement: 0.5245
total CM score: 1.6602
2025-07-22 18:38:44,108 905523 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 18:38:45,065 905523 INFO Episode 1: reward=4.688, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 18:38:46,008 905523 INFO Episode 2: reward=4.688, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 18:38:46,948 905523 INFO Episode 3: reward=4.688, length=501, success=False
2025-07-22 18:38:53,529 905523 INFO Results: avg_reward=4.688, success_rate=0.000, avg_length=501.0
2025-07-22 18:38:53,531 905523 INFO
Testing intervention 5/6: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: 2.023
total data points collected: 501
average episode length: 50.1
average episode reward: 0.202
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 33 steps, reward: -2.310
total data points collected: 534
average episode length: 53.4
average episode reward: -0.029
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -1.060
total data points collected: 1035
average episode length: 103.5
average episode reward: -0.135
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1536
average episode length: 153.6
average episode reward: -0.409
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2037
average episode length: 203.7
average episode reward: -0.541
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2538
average episode length: 253.8
average episode reward: -0.275
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3039
average episode length: 303.9
average episode reward: -0.700
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3540
average episode length: 354.0
average episode reward: -0.767
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4041
average episode length: 404.1
average episode reward: -1.085
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 4542
average episode length: 454.2
average episode reward: -1.394
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
tensor shapes - states: torch.Size([4542, 56]), actions: torch.Size([4542, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7739', '7.6519', '8.3817', '7.8143', '7.4538']
Training reward models...
Reward model losses: ['0.0479', '0.0413', '0.3454', '0.0342', '0.1802']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0288', '1.1060', '1.1069', '1.1002', '1.0163']
Training action VAE models...
Action VAE losses: ['1.4539', '1.5299', '1.4185', '1.4049', '1.5202']
CM score components:
transition disagreement: 0.4231
reward disagreement: 0.1640
state disagreement: 0.4724
action disagreement: 0.5676
total CM score: 1.6271
2025-07-22 18:39:03,273 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 18:39:04,231 905523 INFO Episode 1: reward=-0.135, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 18:39:04,336 905523 INFO Episode 2: reward=-0.134, length=28, success=True
Reset #3: angle intervention applied (success: True)
2025-07-22 18:39:05,301 905523 INFO Episode 3: reward=4.081, length=501, success=False
2025-07-22 18:39:11,975 905523 INFO Results: avg_reward=0.748, success_rate=0.100, avg_length=453.7
2025-07-22 18:39:11,977 905523 INFO
Testing intervention 6/6: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.807
total data points collected: 501
average episode length: 50.1
average episode reward: -0.081
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: 3.933
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.313
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.839
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.129
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.259
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.351
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.324
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.371
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.441
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.550
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.576
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6651', '7.1368', '6.6226', '7.3791', '7.0876']
Training reward models...
Reward model losses: ['0.2963', '0.0148', '0.5422', '0.0197', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5245', '1.3919', '1.5092', '1.5745', '1.4365']
Training action VAE models...
Action VAE losses: ['1.4914', '1.4027', '1.4766', '1.3909', '1.3687']
CM score components:
transition disagreement: 0.4195
reward disagreement: 0.2221
state disagreement: 0.5067
action disagreement: 0.5399
total CM score: 1.6882
2025-07-22 18:39:22,729 905523 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 18:39:23,787 905523 INFO Episode 1: reward=-1.290, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 18:39:24,767 905523 INFO Episode 2: reward=-0.827, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 18:39:25,939 905523 INFO Episode 3: reward=0.819, length=501, success=False
2025-07-22 18:39:31,978 905523 INFO Results: avg_reward=1.852, success_rate=0.100, avg_length=454.0
2025-07-22 18:39:31,979 905523 INFO [Stage 2] goal         | reward=0.92 (norm 0.044) | CM=1.969 (norm 1.000) | α=0.50 → unified=0.522
2025-07-22 18:39:31,979 905523 INFO [Stage 2] mass         | reward=3.57 (norm 0.717) | CM=1.683 (norm 0.164) | α=0.50 → unified=0.441
2025-07-22 18:39:31,980 905523 INFO [Stage 2] friction     | reward=4.48 (norm 0.946) | CM=1.723 (norm 0.281) | α=0.50 → unified=0.613
2025-07-22 18:39:31,980 905523 INFO [Stage 2] visual       | reward=4.69 (norm 1.000) | CM=1.660 (norm 0.097) | α=0.50 → unified=0.548
2025-07-22 18:39:31,980 905523 INFO [Stage 2] angle        | reward=0.75 (norm 0.000) | CM=1.627 (norm 0.000) | α=0.50 → unified=0.000
2025-07-22 18:39:31,980 905523 INFO [Stage 2] random       | reward=1.85 (norm 0.280) | CM=1.688 (norm 0.179) | α=0.50 → unified=0.230
2025-07-22 18:39:31,980 905523 INFO Best intervention for stage 2: friction (Unified score: 0.6133)
2025-07-22 18:39:31,980 905523 INFO === stage 2/7: training on friction intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_2_friction
IntervenedCausalWorld created with friction intervention
Reset #1: friction intervention applied (success: True)
Reset #2: friction intervention applied (success: True)
Reset #3: friction intervention applied (success: True)
-------------------------------------
| custom/              |            |
|    intervention_type | friction   |
|    stage             | 2          |
| rollout/             |            |
|    ep_len_mean       | 419        |
|    ep_rew_mean       | 0.41970855 |
| time/                |            |
|    fps               | 446        |
|    iterations        | 1          |
|    time_elapsed      | 9          |
|    total_timesteps   | 5103616    |
-------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 409        |
|    ep_rew_mean          | 0.4944391  |
| time/                   |            |
|    fps                  | 274        |
|    iterations           | 2          |
|    time_elapsed         | 29         |
|    total_timesteps      | 5107712    |
| train/                  |            |
|    approx_kl            | 0.06271002 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.573     |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.104     |
|    std                  | 3.09       |
|    value_loss           | 0.021      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 0.72028863 |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 3          |
|    time_elapsed         | 50         |
|    total_timesteps      | 5111808    |
| train/                  |            |
|    approx_kl            | 0.07099304 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.575     |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.104     |
|    std                  | 3.13       |
|    value_loss           | 0.0243     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 382        |
|    ep_rew_mean          | 0.7963303  |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 4          |
|    time_elapsed         | 71         |
|    total_timesteps      | 5115904    |
| train/                  |            |
|    approx_kl            | 0.06209653 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.586     |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.112     |
|    std                  | 3.2        |
|    value_loss           | 0.0166     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 0.8426345   |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 5           |
|    time_elapsed         | 92          |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.082455516 |
|    clip_fraction        | 0.603       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.3       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.572      |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.106      |
|    std                  | 3.25        |
|    value_loss           | 0.0436      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 359        |
|    ep_rew_mean          | 1.1536866  |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 6          |
|    time_elapsed         | 113        |
|    total_timesteps      | 5124096    |
| train/                  |            |
|    approx_kl            | 0.07887247 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.574     |
|    n_updates            | 1425       |
|    policy_gradient_loss | -0.0982    |
|    std                  | 3.31       |
|    value_loss           | 0.0236     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 331        |
|    ep_rew_mean          | 1.2492553  |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 7          |
|    time_elapsed         | 133        |
|    total_timesteps      | 5128192    |
| train/                  |            |
|    approx_kl            | 0.06841344 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.584     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.102     |
|    std                  | 3.39       |
|    value_loss           | 0.0226     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 305        |
|    ep_rew_mean          | 1.3008661  |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 8          |
|    time_elapsed         | 154        |
|    total_timesteps      | 5132288    |
| train/                  |            |
|    approx_kl            | 0.07266644 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.9      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.6       |
|    n_updates            | 1455       |
|    policy_gradient_loss | -0.108     |
|    std                  | 3.45       |
|    value_loss           | 0.0199     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | friction    |
|    stage                | 2           |
| rollout/                |             |
|    ep_len_mean          | 305         |
|    ep_rew_mean          | 1.277597    |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 9           |
|    time_elapsed         | 175         |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.063208655 |
|    clip_fraction        | 0.593       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.594      |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.107      |
|    std                  | 3.52        |
|    value_loss           | 0.0312      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 284        |
|    ep_rew_mean          | 1.1286781  |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 10         |
|    time_elapsed         | 196        |
|    total_timesteps      | 5140480    |
| train/                  |            |
|    approx_kl            | 0.08430341 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.1      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.591     |
|    n_updates            | 1485       |
|    policy_gradient_loss | -0.105     |
|    std                  | 3.54       |
|    value_loss           | 0.0379     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 303        |
|    ep_rew_mean          | 1.2187141  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 11         |
|    time_elapsed         | 216        |
|    total_timesteps      | 5144576    |
| train/                  |            |
|    approx_kl            | 0.10438423 |
|    clip_fraction        | 0.614      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.598     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.111     |
|    std                  | 3.6        |
|    value_loss           | 0.0282     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | friction   |
|    stage                | 2          |
| rollout/                |            |
|    ep_len_mean          | 317        |
|    ep_rew_mean          | 1.1059538  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 12         |
|    time_elapsed         | 237        |
|    total_timesteps      | 5148672    |
| train/                  |            |
|    approx_kl            | 0.10877081 |
|    clip_fraction        | 0.636      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.4      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.613     |
|    n_updates            | 1515       |
|    policy_gradient_loss | -0.115     |
|    std                  | 3.67       |
|    value_loss           | 0.0239     |
----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | friction  |
|    stage                | 2         |
| rollout/                |           |
|    ep_len_mean          | 315       |
|    ep_rew_mean          | 1.2215338 |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 13        |
|    time_elapsed         | 258       |
|    total_timesteps      | 5152768   |
| train/                  |           |
|    approx_kl            | 0.0923505 |
|    clip_fraction        | 0.605     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.6     |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.598    |
|    n_updates            | 1530      |
|    policy_gradient_loss | -0.103    |
|    std                  | 3.73      |
|    value_loss           | 0.0235    |
---------------------------------------
2025-07-22 18:44:01,668 905523 INFO model saved to cm_sequencing_logs/model_stage_2_friction
2025-07-22 18:44:01,669 905523 INFO
Completed stage 2. Intervention 'friction' removed from list.
2025-07-22 18:44:01,669 905523 INFO Remaining interventions: 5
2025-07-22 18:44:01,670 905523 INFO CURRICULUM STAGE 3/7
2025-07-22 18:44:01,670 905523 INFO Remaining interventions: ['goal', 'mass', 'visual', 'angle', 'random']
2025-07-22 18:44:01,670 905523 INFO
Testing intervention 1/5: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 0.012
total data points collected: 501
average episode length: 50.1
average episode reward: 0.001
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.050
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.106
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 0.454
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.152
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.235
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.171
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -0.115
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.170
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.278
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.482
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4538
average episode length: 453.8
average episode reward: 0.785
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
tensor shapes - states: torch.Size([4538, 56]), actions: torch.Size([4538, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8125', '7.5167', '6.9757', '7.2967', '7.1559']
Training reward models...
Reward model losses: ['0.2208', '0.0538', '0.0166', '0.0339', '0.0392']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0842', '1.1466', '1.1008', '1.1484', '1.0977']
Training action VAE models...
Action VAE losses: ['1.5616', '1.4143', '1.3844', '1.4300', '1.3683']
CM score components:
transition disagreement: 0.3932
reward disagreement: 0.1400
state disagreement: 0.4871
action disagreement: 0.5467
total CM score: 1.5669
2025-07-22 18:44:11,374 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 18:44:12,301 905523 INFO Episode 1: reward=2.392, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 18:44:13,213 905523 INFO Episode 2: reward=-0.215, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 18:44:14,134 905523 INFO Episode 3: reward=0.663, length=501, success=False
2025-07-22 18:44:19,718 905523 INFO Results: avg_reward=1.402, success_rate=0.100, avg_length=454.2
2025-07-22 18:44:19,720 905523 INFO
Testing intervention 2/5: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 1.122
total data points collected: 501
average episode length: 50.1
average episode reward: 0.112
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 39 steps, reward: 0.762
total data points collected: 540
average episode length: 54.0
average episode reward: 0.188
termination reasons: ['max_length', 'success']
success rate: 1/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: -0.549
total data points collected: 1041
average episode length: 104.1
average episode reward: 0.134
termination reasons: ['max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 1542
average episode length: 154.2
average episode reward: 0.254
termination reasons: ['max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 1582
average episode length: 158.2
average episode reward: 0.307
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 1618
average episode length: 161.8
average episode reward: 0.359
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/10
total data points collected: 1717
average episode length: 171.7
average episode reward: 0.494
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 1866
average episode length: 186.6
average episode reward: 0.641
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success', 'success', 'success', 'success']
success rate: 5/10
total data points collected: 2367
average episode length: 236.7
average episode reward: 0.438
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success', 'success', 'success', 'success', 'max_length']
success rate: 5/10
total data points collected: 2868
average episode length: 286.8
average episode reward: 0.760
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'success', 'success', 'success', 'success', 'max_length', 'max_length']
success rate: 5/10
tensor shapes - states: torch.Size([2868, 56]), actions: torch.Size([2868, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6457', '7.8785', '6.6686', '7.1118', '7.2830']
Training reward models...
Reward model losses: ['0.0082', '0.1115', '0.5950', '0.0133', '0.0336']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8260', '1.6793', '1.6062', '1.5682', '1.7265']
Training action VAE models...
Action VAE losses: ['1.3437', '1.5885', '1.5182', '1.3347', '1.4967']
CM score components:
transition disagreement: 0.3900
reward disagreement: 0.2575
state disagreement: 0.5467
action disagreement: 0.5638
total CM score: 1.7581
2025-07-22 18:44:26,431 905523 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 18:44:27,364 905523 INFO Episode 1: reward=3.650, length=501, success=False
Reset #2: mass intervention applied (success: True)
2025-07-22 18:44:28,283 905523 INFO Episode 2: reward=3.781, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 18:44:29,208 905523 INFO Episode 3: reward=1.235, length=501, success=False
2025-07-22 18:44:35,656 905523 INFO Results: avg_reward=3.482, success_rate=0.000, avg_length=501.0
2025-07-22 18:44:35,658 905523 INFO
Testing intervention 3/5: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 395 steps, reward: 3.273
total data points collected: 395
average episode length: 39.5
average episode reward: 0.327
termination reasons: ['success']
success rate: 1/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 2.149
total data points collected: 896
average episode length: 89.6
average episode reward: 0.542
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 1.451
total data points collected: 1397
average episode length: 139.7
average episode reward: 0.687
termination reasons: ['success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 1429
average episode length: 142.9
average episode reward: 0.791
termination reasons: ['success', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 1930
average episode length: 193.0
average episode reward: 1.135
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 2431
average episode length: 243.1
average episode reward: 1.722
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2932
average episode length: 293.2
average episode reward: 1.467
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3433
average episode length: 343.3
average episode reward: 1.143
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 3646
average episode length: 364.6
average episode reward: 1.410
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 3688
average episode length: 368.8
average episode reward: 1.476
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3688, 56]), actions: torch.Size([3688, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1180', '7.6154', '7.9625', '6.8498', '7.7056']
Training reward models...
Reward model losses: ['0.0455', '0.0120', '0.8679', '0.7897', '0.3355']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6308', '1.6430', '1.8145', '1.5591', '1.4801']
Training action VAE models...
Action VAE losses: ['1.3978', '1.3280', '1.5700', '1.6962', '1.5055']
CM score components:
transition disagreement: 0.4498
reward disagreement: 0.4255
state disagreement: 0.5099
action disagreement: 0.5904
total CM score: 1.9756
2025-07-22 18:44:43,817 905523 INFO testing intervention: visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
2025-07-22 18:44:44,735 905523 INFO Episode 1: reward=5.350, length=501, success=False
Reset #2: visual intervention applied (success: True)
2025-07-22 18:44:45,652 905523 INFO Episode 2: reward=5.350, length=501, success=False
Reset #3: visual intervention applied (success: True)
2025-07-22 18:44:46,552 905523 INFO Episode 3: reward=5.350, length=501, success=False
2025-07-22 18:44:52,869 905523 INFO Results: avg_reward=5.350, success_rate=0.000, avg_length=501.0
2025-07-22 18:44:52,871 905523 INFO
Testing intervention 4/5: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -2.379
total data points collected: 501
average episode length: 50.1
average episode reward: -0.238
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: -2.497
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.488
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -4.068
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.894
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 1566
average episode length: 156.6
average episode reward: -0.897
termination reasons: ['max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 2067
average episode length: 206.7
average episode reward: -0.614
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 2568
average episode length: 256.8
average episode reward: -0.470
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2599
average episode length: 259.9
average episode reward: -0.367
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 3100
average episode length: 310.0
average episode reward: -0.241
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 3313
average episode length: 331.3
average episode reward: -0.115
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 3470
average episode length: 347.0
average episode reward: -0.213
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'success', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3470, 56]), actions: torch.Size([3470, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3879', '7.2249', '7.6906', '7.2288', '6.5917']
Training reward models...
Reward model losses: ['0.0892', '0.0952', '0.0894', '0.0541', '0.0500']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5730', '1.8934', '1.7810', '1.5855', '1.8035']
Training action VAE models...
Action VAE losses: ['1.3437', '1.3950', '1.5557', '1.4486', '1.4898']
CM score components:
transition disagreement: 0.4101
reward disagreement: 0.1116
state disagreement: 0.5370
action disagreement: 0.5663
total CM score: 1.6251
2025-07-22 18:45:00,683 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 18:45:01,600 905523 INFO Episode 1: reward=1.298, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 18:45:02,506 905523 INFO Episode 2: reward=2.855, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 18:45:03,486 905523 INFO Episode 3: reward=-0.585, length=501, success=False
2025-07-22 18:45:09,491 905523 INFO Results: avg_reward=0.530, success_rate=0.100, avg_length=454.8
2025-07-22 18:45:09,493 905523 INFO
Testing intervention 5/5: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.519
total data points collected: 501
average episode length: 50.1
average episode reward: 0.152
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: 3.787
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.531
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: 4.550
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.986
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.989
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 1.180
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 1.436
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 1.539
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 2.052
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 2.523
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4540
average episode length: 454.0
average episode reward: 2.521
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
tensor shapes - states: torch.Size([4540, 56]), actions: torch.Size([4540, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3883', '8.0379', '7.4889', '7.3693', '6.4938']
Training reward models...
Reward model losses: ['0.0392', '0.7516', '0.0372', '0.4897', '0.0797']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2588', '1.3389', '1.2773', '1.2611', '1.2838']
Training action VAE models...
Action VAE losses: ['1.4687', '1.3950', '1.4764', '1.3817', '1.4644']
CM score components:
transition disagreement: 0.4633
reward disagreement: 0.2579
state disagreement: 0.4978
action disagreement: 0.5855
total CM score: 1.8045
2025-07-22 18:45:19,226 905523 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 18:45:20,143 905523 INFO Episode 1: reward=2.121, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 18:45:21,239 905523 INFO Episode 2: reward=-0.690, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 18:45:22,178 905523 INFO Episode 3: reward=4.637, length=501, success=False
2025-07-22 18:45:29,246 905523 INFO Results: avg_reward=1.249, success_rate=0.000, avg_length=501.0
2025-07-22 18:45:29,248 905523 INFO [Stage 3] goal         | reward=1.40 (norm 0.181) | CM=1.567 (norm 0.000) | α=0.50 → unified=0.090
2025-07-22 18:45:29,248 905523 INFO [Stage 3] mass         | reward=3.48 (norm 0.612) | CM=1.758 (norm 0.468) | α=0.50 → unified=0.540
2025-07-22 18:45:29,248 905523 INFO [Stage 3] visual       | reward=5.35 (norm 1.000) | CM=1.976 (norm 1.000) | α=0.50 → unified=1.000
2025-07-22 18:45:29,249 905523 INFO [Stage 3] angle        | reward=0.53 (norm 0.000) | CM=1.625 (norm 0.142) | α=0.50 → unified=0.071
2025-07-22 18:45:29,249 905523 INFO [Stage 3] random       | reward=1.25 (norm 0.149) | CM=1.804 (norm 0.581) | α=0.50 → unified=0.365
2025-07-22 18:45:29,249 905523 INFO Best intervention for stage 3: visual (Unified score: 1.0000)
2025-07-22 18:45:29,249 905523 INFO === stage 3/7: training on visual intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_3_visual
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | visual    |
|    stage             | 3         |
| rollout/             |           |
|    ep_len_mean       | 317       |
|    ep_rew_mean       | 1.2284613 |
| time/                |           |
|    fps               | 453       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5156864   |
------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | visual      |
|    stage                | 3           |
| rollout/                |             |
|    ep_len_mean          | 324         |
|    ep_rew_mean          | 1.2485107   |
| time/                   |             |
|    fps                  | 273         |
|    iterations           | 2           |
|    time_elapsed         | 29          |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.089726135 |
|    clip_fraction        | 0.626       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.594      |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0997     |
|    std                  | 3.9         |
|    value_loss           | 0.0214      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 333        |
|    ep_rew_mean          | 1.2035601  |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 3          |
|    time_elapsed         | 51         |
|    total_timesteps      | 5165056    |
| train/                  |            |
|    approx_kl            | 0.07176524 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.1      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.62      |
|    n_updates            | 1575       |
|    policy_gradient_loss | -0.11      |
|    std                  | 3.98       |
|    value_loss           | 0.0153     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 354        |
|    ep_rew_mean          | 1.2151458  |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 4          |
|    time_elapsed         | 71         |
|    total_timesteps      | 5169152    |
| train/                  |            |
|    approx_kl            | 0.17760262 |
|    clip_fraction        | 0.615      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.608     |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.105     |
|    std                  | 4.04       |
|    value_loss           | 0.0392     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 356        |
|    ep_rew_mean          | 1.2163223  |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 5          |
|    time_elapsed         | 92         |
|    total_timesteps      | 5173248    |
| train/                  |            |
|    approx_kl            | 0.07685268 |
|    clip_fraction        | 0.627      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.4      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.625     |
|    n_updates            | 1605       |
|    policy_gradient_loss | -0.113     |
|    std                  | 4.09       |
|    value_loss           | 0.03       |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 346        |
|    ep_rew_mean          | 1.2734202  |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 6          |
|    time_elapsed         | 113        |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.08803242 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.5      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.638     |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.117     |
|    std                  | 4.16       |
|    value_loss           | 0.0226     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 340        |
|    ep_rew_mean          | 1.4516159  |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 7          |
|    time_elapsed         | 134        |
|    total_timesteps      | 5181440    |
| train/                  |            |
|    approx_kl            | 0.08548541 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.7      |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.612     |
|    n_updates            | 1635       |
|    policy_gradient_loss | -0.101     |
|    std                  | 4.27       |
|    value_loss           | 0.0395     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 344        |
|    ep_rew_mean          | 1.3157682  |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 8          |
|    time_elapsed         | 155        |
|    total_timesteps      | 5185536    |
| train/                  |            |
|    approx_kl            | 0.06082327 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.9      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.636     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.113     |
|    std                  | 4.33       |
|    value_loss           | 0.0225     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 352        |
|    ep_rew_mean          | 1.1383493  |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 9          |
|    time_elapsed         | 175        |
|    total_timesteps      | 5189632    |
| train/                  |            |
|    approx_kl            | 0.26960126 |
|    clip_fraction        | 0.612      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26        |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.634     |
|    n_updates            | 1665       |
|    policy_gradient_loss | -0.105     |
|    std                  | 4.39       |
|    value_loss           | 0.0339     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 349        |
|    ep_rew_mean          | 1.0910565  |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 10         |
|    time_elapsed         | 196        |
|    total_timesteps      | 5193728    |
| train/                  |            |
|    approx_kl            | 0.52536964 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.2      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.635     |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.107     |
|    std                  | 4.48       |
|    value_loss           | 0.0387     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 359        |
|    ep_rew_mean          | 1.1523403  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 11         |
|    time_elapsed         | 216        |
|    total_timesteps      | 5197824    |
| train/                  |            |
|    approx_kl            | 0.07263616 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.4      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.636     |
|    n_updates            | 1695       |
|    policy_gradient_loss | -0.101     |
|    std                  | 4.57       |
|    value_loss           | 0.032      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 374        |
|    ep_rew_mean          | 1.0091168  |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 12         |
|    time_elapsed         | 237        |
|    total_timesteps      | 5201920    |
| train/                  |            |
|    approx_kl            | 0.07646435 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.5      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.643     |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.102     |
|    std                  | 4.66       |
|    value_loss           | 0.0297     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | visual     |
|    stage                | 3          |
| rollout/                |            |
|    ep_len_mean          | 382        |
|    ep_rew_mean          | 1.2081622  |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 13         |
|    time_elapsed         | 258        |
|    total_timesteps      | 5206016    |
| train/                  |            |
|    approx_kl            | 0.09546302 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.7      |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.656     |
|    n_updates            | 1725       |
|    policy_gradient_loss | -0.114     |
|    std                  | 4.76       |
|    value_loss           | 0.0239     |
----------------------------------------
2025-07-22 18:49:59,357 905523 INFO model saved to cm_sequencing_logs/model_stage_3_visual
2025-07-22 18:49:59,359 905523 INFO
Completed stage 3. Intervention 'visual' removed from list.
2025-07-22 18:49:59,359 905523 INFO Remaining interventions: 4
2025-07-22 18:49:59,359 905523 INFO CURRICULUM STAGE 4/7
2025-07-22 18:49:59,359 905523 INFO Remaining interventions: ['goal', 'mass', 'angle', 'random']
2025-07-22 18:49:59,360 905523 INFO
Testing intervention 1/4: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 4.334
total data points collected: 501
average episode length: 50.1
average episode reward: 0.433
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.088
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.442
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 0.241
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.466
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.704
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2039
average episode length: 203.9
average episode reward: 0.959
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 2540
average episode length: 254.0
average episode reward: 1.208
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 2570
average episode length: 257.0
average episode reward: 1.426
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 2/10
total data points collected: 3071
average episode length: 307.1
average episode reward: 1.449
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 3105
average episode length: 310.5
average episode reward: 1.554
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 3606
average episode length: 360.6
average episode reward: 1.594
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length', 'success', 'max_length']
success rate: 3/10
tensor shapes - states: torch.Size([3606, 56]), actions: torch.Size([3606, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0642', '7.7221', '7.5103', '7.5767', '7.7748']
Training reward models...
Reward model losses: ['0.0750', '0.0533', '0.0844', '0.0311', '0.0739']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4898', '1.3431', '1.4102', '1.3141', '1.5220']
Training action VAE models...
Action VAE losses: ['1.4872', '1.3545', '1.3700', '1.3833', '1.4576']
CM score components:
transition disagreement: 0.4120
reward disagreement: 0.1074
state disagreement: 0.5377
action disagreement: 0.5490
total CM score: 1.6062
2025-07-22 18:50:07,437 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 18:50:08,393 905523 INFO Episode 1: reward=0.399, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 18:50:09,362 905523 INFO Episode 2: reward=-0.591, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 18:50:10,329 905523 INFO Episode 3: reward=1.016, length=501, success=False
2025-07-22 18:50:16,056 905523 INFO Results: avg_reward=1.216, success_rate=0.100, avg_length=454.1
2025-07-22 18:50:16,057 905523 INFO
Testing intervention 2/4: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 0.238
total data points collected: 501
average episode length: 50.1
average episode reward: 0.024
termination reasons: ['max_length']
success rate: 0/10
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: -0.034
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.020
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: mass intervention applied (success: True)
episode 3: 36 steps, reward: 0.669
total data points collected: 1038
average episode length: 103.8
average episode reward: 0.087
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1539
average episode length: 153.9
average episode reward: 0.587
termination reasons: ['max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 2040
average episode length: 204.0
average episode reward: 1.017
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2541
average episode length: 254.1
average episode reward: 1.178
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2697
average episode length: 269.7
average episode reward: 1.387
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 3198
average episode length: 319.8
average episode reward: 1.603
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 3239
average episode length: 323.9
average episode reward: 1.656
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 3740
average episode length: 374.0
average episode reward: 1.598
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'success', 'max_length', 'success', 'max_length']
success rate: 3/10
tensor shapes - states: torch.Size([3740, 56]), actions: torch.Size([3740, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9736', '7.2138', '7.0801', '7.0361', '8.3254']
Training reward models...
Reward model losses: ['1.6082', '0.1474', '0.0460', '0.1190', '0.0078']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8084', '2.1131', '1.7688', '1.8427', '1.7775']
Training action VAE models...
Action VAE losses: ['1.3995', '1.6031', '1.6407', '1.3263', '1.3735']
CM score components:
transition disagreement: 0.4288
reward disagreement: 0.4300
state disagreement: 0.5762
action disagreement: 0.5943
total CM score: 2.0294
2025-07-22 18:50:24,385 905523 INFO testing intervention: mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
2025-07-22 18:50:24,538 905523 INFO Episode 1: reward=1.368, length=52, success=True
Reset #2: mass intervention applied (success: True)
2025-07-22 18:50:25,484 905523 INFO Episode 2: reward=3.203, length=501, success=False
Reset #3: mass intervention applied (success: True)
2025-07-22 18:50:26,481 905523 INFO Episode 3: reward=2.319, length=501, success=False
2025-07-22 18:50:33,338 905523 INFO Results: avg_reward=2.279, success_rate=0.100, avg_length=456.1
2025-07-22 18:50:33,340 905523 INFO
Testing intervention 3/4: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -1.586
total data points collected: 501
average episode length: 50.1
average episode reward: -0.159
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: -1.767
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.335
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -2.306
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.566
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.380
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2047
average episode length: 204.7
average episode reward: -0.557
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 2085
average episode length: 208.5
average episode reward: -0.664
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 2586
average episode length: 258.6
average episode reward: -1.162
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 2628
average episode length: 262.8
average episode reward: -1.024
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'success']
success rate: 3/10
total data points collected: 3129
average episode length: 312.9
average episode reward: -1.035
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 3208
average episode length: 320.8
average episode reward: -1.045
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'success', 'max_length', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3208, 56]), actions: torch.Size([3208, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8961', '7.9315', '7.2457', '7.8059', '7.2142']
Training reward models...
Reward model losses: ['0.2182', '0.0439', '0.1311', '0.3216', '0.0419']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3353', '1.4337', '1.4612', '1.5925', '1.6091']
Training action VAE models...
Action VAE losses: ['1.3295', '1.3727', '1.4515', '1.4562', '1.4845']
CM score components:
transition disagreement: 0.4189
reward disagreement: 0.1133
state disagreement: 0.5063
action disagreement: 0.5501
total CM score: 1.5887
2025-07-22 18:50:40,913 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 18:50:41,886 905523 INFO Episode 1: reward=3.081, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 18:50:42,835 905523 INFO Episode 2: reward=-2.419, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 18:50:43,768 905523 INFO Episode 3: reward=3.793, length=501, success=False
2025-07-22 18:50:50,370 905523 INFO Results: avg_reward=1.703, success_rate=0.000, avg_length=501.0
2025-07-22 18:50:50,371 905523 INFO
Testing intervention 4/4: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 4.147
total data points collected: 501
average episode length: 50.1
average episode reward: 0.415
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: 5.860
total data points collected: 1002
average episode length: 100.2
average episode reward: 1.001
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: 0.661
total data points collected: 1503
average episode length: 150.3
average episode reward: 1.067
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: 0.973
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.881
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 1.044
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 1.388
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 1.291
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 1.587
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 1.477
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8687', '6.7595', '7.4670', '6.9197', '7.3112']
Training reward models...
Reward model losses: ['0.1930', '0.1442', '0.1563', '0.0274', '0.0220']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4729', '1.5016', '1.5123', '1.4458', '1.4562']
Training action VAE models...
Action VAE losses: ['1.3648', '1.3941', '1.3111', '1.3087', '1.3518']
CM score components:
transition disagreement: 0.3986
reward disagreement: 0.1065
state disagreement: 0.5083
action disagreement: 0.5147
total CM score: 1.5281
2025-07-22 18:51:00,968 905523 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 18:51:01,912 905523 INFO Episode 1: reward=5.296, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 18:51:02,835 905523 INFO Episode 2: reward=1.575, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 18:51:03,780 905523 INFO Episode 3: reward=5.652, length=501, success=False
2025-07-22 18:51:09,570 905523 INFO Results: avg_reward=2.484, success_rate=0.100, avg_length=454.1
2025-07-22 18:51:09,571 905523 INFO [Stage 4] goal         | reward=1.22 (norm 0.000) | CM=1.606 (norm 0.156) | α=0.50 → unified=0.078
2025-07-22 18:51:09,572 905523 INFO [Stage 4] mass         | reward=2.28 (norm 0.838) | CM=2.029 (norm 1.000) | α=0.50 → unified=0.919
2025-07-22 18:51:09,572 905523 INFO [Stage 4] angle        | reward=1.70 (norm 0.384) | CM=1.589 (norm 0.121) | α=0.50 → unified=0.253
2025-07-22 18:51:09,572 905523 INFO [Stage 4] random       | reward=2.48 (norm 1.000) | CM=1.528 (norm 0.000) | α=0.50 → unified=0.500
2025-07-22 18:51:09,572 905523 INFO Best intervention for stage 4: mass (Unified score: 0.9191)
2025-07-22 18:51:09,572 905523 INFO === stage 4/7: training on mass intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_4_mass
IntervenedCausalWorld created with mass intervention
Reset #1: mass intervention applied (success: True)
Reset #2: mass intervention applied (success: True)
Reset #3: mass intervention applied (success: True)
-----------------------------------
| custom/              |          |
|    intervention_type | mass     |
|    stage             | 4        |
| rollout/             |          |
|    ep_len_mean       | 383      |
|    ep_rew_mean       | 1.237988 |
| time/                |          |
|    fps               | 453      |
|    iterations        | 1        |
|    time_elapsed      | 9        |
|    total_timesteps   | 5210112  |
-----------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 391        |
|    ep_rew_mean          | 1.329913   |
| time/                   |            |
|    fps                  | 274        |
|    iterations           | 2          |
|    time_elapsed         | 29         |
|    total_timesteps      | 5214208    |
| train/                  |            |
|    approx_kl            | 0.08514014 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.1      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.655     |
|    n_updates            | 1755       |
|    policy_gradient_loss | -0.107     |
|    std                  | 4.97       |
|    value_loss           | 0.0216     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 1.4182779  |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 3          |
|    time_elapsed         | 50         |
|    total_timesteps      | 5218304    |
| train/                  |            |
|    approx_kl            | 0.08004178 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.3      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.00025    |
|    loss                 | -0.654     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0979    |
|    std                  | 5.08       |
|    value_loss           | 0.031      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 403         |
|    ep_rew_mean          | 1.4539832   |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 4           |
|    time_elapsed         | 71          |
|    total_timesteps      | 5222400     |
| train/                  |             |
|    approx_kl            | 0.059231143 |
|    clip_fraction        | 0.58        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.5       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.661      |
|    n_updates            | 1785        |
|    policy_gradient_loss | -0.0976     |
|    std                  | 5.18        |
|    value_loss           | 0.0208      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 403        |
|    ep_rew_mean          | 1.5279586  |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 5          |
|    time_elapsed         | 92         |
|    total_timesteps      | 5226496    |
| train/                  |            |
|    approx_kl            | 0.06567257 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.7      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.653     |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0934    |
|    std                  | 5.3        |
|    value_loss           | 0.0265     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 389        |
|    ep_rew_mean          | 1.6650788  |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 6          |
|    time_elapsed         | 113        |
|    total_timesteps      | 5230592    |
| train/                  |            |
|    approx_kl            | 0.06861426 |
|    clip_fraction        | 0.627      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.9      |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.666     |
|    n_updates            | 1815       |
|    policy_gradient_loss | -0.103     |
|    std                  | 5.42       |
|    value_loss           | 0.0304     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 1.774618    |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 7           |
|    time_elapsed         | 134         |
|    total_timesteps      | 5234688     |
| train/                  |             |
|    approx_kl            | 0.056111537 |
|    clip_fraction        | 0.526       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.654      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0898     |
|    std                  | 5.5         |
|    value_loss           | 0.0271      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 404        |
|    ep_rew_mean          | 1.7888201  |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 8          |
|    time_elapsed         | 155        |
|    total_timesteps      | 5238784    |
| train/                  |            |
|    approx_kl            | 0.05867955 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.673     |
|    n_updates            | 1845       |
|    policy_gradient_loss | -0.0954    |
|    std                  | 5.61       |
|    value_loss           | 0.0205     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 396        |
|    ep_rew_mean          | 1.9569858  |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 9          |
|    time_elapsed         | 176        |
|    total_timesteps      | 5242880    |
| train/                  |            |
|    approx_kl            | 0.08038656 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.4      |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.67      |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.094     |
|    std                  | 5.75       |
|    value_loss           | 0.0316     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | 1.8746544   |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 10          |
|    time_elapsed         | 197         |
|    total_timesteps      | 5246976     |
| train/                  |             |
|    approx_kl            | 0.060776666 |
|    clip_fraction        | 0.581       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.662      |
|    n_updates            | 1875        |
|    policy_gradient_loss | -0.0963     |
|    std                  | 5.88        |
|    value_loss           | 0.0412      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | mass       |
|    stage                | 4          |
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 1.7686872  |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 11         |
|    time_elapsed         | 218        |
|    total_timesteps      | 5251072    |
| train/                  |            |
|    approx_kl            | 0.07632448 |
|    clip_fraction        | 0.65       |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.9      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.693     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.108     |
|    std                  | 6.08       |
|    value_loss           | 0.026      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 386         |
|    ep_rew_mean          | 1.8010178   |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 12          |
|    time_elapsed         | 239         |
|    total_timesteps      | 5255168     |
| train/                  |             |
|    approx_kl            | 0.057130627 |
|    clip_fraction        | 0.598       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.692      |
|    n_updates            | 1905        |
|    policy_gradient_loss | -0.109      |
|    std                  | 6.16        |
|    value_loss           | 0.0277      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | mass        |
|    stage                | 4           |
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 1.5698735   |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 13          |
|    time_elapsed         | 260         |
|    total_timesteps      | 5259264     |
| train/                  |             |
|    approx_kl            | 0.072767675 |
|    clip_fraction        | 0.594       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.682      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0972     |
|    std                  | 6.33        |
|    value_loss           | 0.038       |
-----------------------------------------
2025-07-22 18:55:42,266 905523 INFO model saved to cm_sequencing_logs/model_stage_4_mass
2025-07-22 18:55:42,267 905523 INFO
Completed stage 4. Intervention 'mass' removed from list.
2025-07-22 18:55:42,268 905523 INFO Remaining interventions: 3
2025-07-22 18:55:42,268 905523 INFO CURRICULUM STAGE 5/7
2025-07-22 18:55:42,268 905523 INFO Remaining interventions: ['goal', 'angle', 'random']
2025-07-22 18:55:42,268 905523 INFO
Testing intervention 1/3: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 1.216
total data points collected: 501
average episode length: 50.1
average episode reward: 0.122
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.865
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.208
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -3.375
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.129
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.007
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.005
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -0.096
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.153
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.507
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.610
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.913
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1372', '7.9167', '7.9484', '8.0145', '7.5553']
Training reward models...
Reward model losses: ['0.0188', '0.0758', '0.5821', '0.2580', '0.0154']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2859', '1.3475', '1.4004', '1.4304', '1.3905']
Training action VAE models...
Action VAE losses: ['1.3883', '1.3584', '1.5449', '1.3845', '1.3784']
CM score components:
transition disagreement: 0.4385
reward disagreement: 0.1754
state disagreement: 0.5264
action disagreement: 0.5278
total CM score: 1.6681
2025-07-22 18:55:53,086 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 18:55:54,033 905523 INFO Episode 1: reward=-0.308, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 18:55:55,038 905523 INFO Episode 2: reward=-5.494, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 18:55:55,963 905523 INFO Episode 3: reward=1.385, length=501, success=False
2025-07-22 18:56:01,998 905523 INFO Results: avg_reward=0.756, success_rate=0.100, avg_length=453.5
2025-07-22 18:56:02,000 905523 INFO
Testing intervention 2/3: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 166 steps, reward: 0.163
total data points collected: 166
average episode length: 16.6
average episode reward: 0.016
termination reasons: ['success']
success rate: 1/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: 2.979
total data points collected: 667
average episode length: 66.7
average episode reward: 0.314
termination reasons: ['success', 'max_length']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 351 steps, reward: 2.366
total data points collected: 1018
average episode length: 101.8
average episode reward: 0.551
termination reasons: ['success', 'max_length', 'success']
success rate: 2/10
total data points collected: 1519
average episode length: 151.9
average episode reward: 0.881
termination reasons: ['success', 'max_length', 'success', 'max_length']
success rate: 2/10
total data points collected: 2020
average episode length: 202.0
average episode reward: 0.597
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 2056
average episode length: 205.6
average episode reward: 0.710
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length', 'success']
success rate: 3/10
total data points collected: 2557
average episode length: 255.7
average episode reward: 1.001
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 3/10
total data points collected: 3058
average episode length: 305.8
average episode reward: 1.241
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 3559
average episode length: 355.9
average episode reward: 1.408
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 3/10
total data points collected: 4060
average episode length: 406.0
average episode reward: 1.258
termination reasons: ['success', 'max_length', 'success', 'max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 3/10
tensor shapes - states: torch.Size([4060, 56]), actions: torch.Size([4060, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7516', '7.3358', '7.3761', '7.4638', '7.4157']
Training reward models...
Reward model losses: ['0.0337', '1.2043', '0.0209', '0.0142', '0.0456']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7704', '1.5772', '1.5544', '1.8485', '1.6020']
Training action VAE models...
Action VAE losses: ['1.4350', '1.6057', '1.4119', '1.3553', '1.3429']
CM score components:
transition disagreement: 0.4075
reward disagreement: 0.3393
state disagreement: 0.5368
action disagreement: 0.5639
total CM score: 1.8476
2025-07-22 18:56:11,027 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 18:56:11,977 905523 INFO Episode 1: reward=-0.203, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 18:56:12,998 905523 INFO Episode 2: reward=1.619, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 18:56:14,016 905523 INFO Episode 3: reward=-0.147, length=501, success=False
2025-07-22 18:56:20,945 905523 INFO Results: avg_reward=0.064, success_rate=0.000, avg_length=501.0
2025-07-22 18:56:20,947 905523 INFO
Testing intervention 3/3: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 1.234
total data points collected: 501
average episode length: 50.1
average episode reward: 0.123
termination reasons: ['max_length']
success rate: 0/10
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.675
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.044
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -3.394
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.383
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.083
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -0.317
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -0.414
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: -0.401
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: -0.774
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: -1.301
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: -1.358
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6296', '7.5389', '7.1169', '7.4408', '6.8998']
Training reward models...
Reward model losses: ['1.4392', '0.3655', '0.0589', '2.0408', '0.2456']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4464', '1.4785', '1.6759', '1.5032', '1.5529']
Training action VAE models...
Action VAE losses: ['1.5016', '1.3852', '1.3953', '1.5542', '1.4487']
CM score components:
transition disagreement: 0.3922
reward disagreement: 0.6849
state disagreement: 0.5112
action disagreement: 0.5708
total CM score: 2.1590
2025-07-22 18:56:31,748 905523 INFO testing intervention: random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
2025-07-22 18:56:32,790 905523 INFO Episode 1: reward=-4.686, length=501, success=False
Reset #2: random intervention applied (success: True)
2025-07-22 18:56:33,790 905523 INFO Episode 2: reward=-1.953, length=501, success=False
Reset #3: random intervention applied (success: True)
2025-07-22 18:56:34,777 905523 INFO Episode 3: reward=2.217, length=501, success=False
2025-07-22 18:56:41,583 905523 INFO Results: avg_reward=0.894, success_rate=0.000, avg_length=501.0
2025-07-22 18:56:41,585 905523 INFO [Stage 5] goal         | reward=0.76 (norm 0.834) | CM=1.668 (norm 0.000) | α=0.50 → unified=0.417
2025-07-22 18:56:41,585 905523 INFO [Stage 5] angle        | reward=0.06 (norm 0.000) | CM=1.848 (norm 0.366) | α=0.50 → unified=0.183
2025-07-22 18:56:41,585 905523 INFO [Stage 5] random       | reward=0.89 (norm 1.000) | CM=2.159 (norm 1.000) | α=0.50 → unified=1.000
2025-07-22 18:56:41,585 905523 INFO Best intervention for stage 5: random (Unified score: 1.0000)
2025-07-22 18:56:41,585 905523 INFO === stage 5/7: training on random intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_5_random
IntervenedCausalWorld created with random intervention
Reset #1: random intervention applied (success: True)
Reset #2: random intervention applied (success: True)
Reset #3: random intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | random    |
|    stage             | 5         |
| rollout/             |           |
|    ep_len_mean       | 406       |
|    ep_rew_mean       | 1.4927704 |
| time/                |           |
|    fps               | 446       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5263360   |
------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 1.2493491  |
| time/                   |            |
|    fps                  | 270        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 5267456    |
| train/                  |            |
|    approx_kl            | 0.09190939 |
|    clip_fraction        | 0.67       |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.6      |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.713     |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.113     |
|    std                  | 6.59       |
|    value_loss           | 0.0299     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 420        |
|    ep_rew_mean          | 1.2327045  |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 3          |
|    time_elapsed         | 51         |
|    total_timesteps      | 5271552    |
| train/                  |            |
|    approx_kl            | 0.08249073 |
|    clip_fraction        | 0.644      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.8      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.716     |
|    n_updates            | 1965       |
|    policy_gradient_loss | -0.112     |
|    std                  | 6.74       |
|    value_loss           | 0.0488     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 1.1511533  |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 4          |
|    time_elapsed         | 72         |
|    total_timesteps      | 5275648    |
| train/                  |            |
|    approx_kl            | 0.07546612 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.678     |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.0817    |
|    std                  | 6.94       |
|    value_loss           | 0.0767     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 432        |
|    ep_rew_mean          | 1.0886228  |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 5          |
|    time_elapsed         | 93         |
|    total_timesteps      | 5279744    |
| train/                  |            |
|    approx_kl            | 0.05846726 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.3      |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.702     |
|    n_updates            | 1995       |
|    policy_gradient_loss | -0.101     |
|    std                  | 7.1        |
|    value_loss           | 0.0474     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 434        |
|    ep_rew_mean          | 0.76961035 |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 6          |
|    time_elapsed         | 115        |
|    total_timesteps      | 5283840    |
| train/                  |            |
|    approx_kl            | 0.22048093 |
|    clip_fraction        | 0.699      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.5      |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.731     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.116     |
|    std                  | 7.3        |
|    value_loss           | 0.0522     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 444        |
|    ep_rew_mean          | 0.5011631  |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 7          |
|    time_elapsed         | 136        |
|    total_timesteps      | 5287936    |
| train/                  |            |
|    approx_kl            | 0.07130166 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.7      |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.691     |
|    n_updates            | 2025       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 7.42       |
|    value_loss           | 0.0651     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 451        |
|    ep_rew_mean          | 0.5038849  |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 8          |
|    time_elapsed         | 157        |
|    total_timesteps      | 5292032    |
| train/                  |            |
|    approx_kl            | 0.07044526 |
|    clip_fraction        | 0.615      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.8      |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.73      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.103     |
|    std                  | 7.55       |
|    value_loss           | 0.0382     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 457        |
|    ep_rew_mean          | 0.45586085 |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 9          |
|    time_elapsed         | 177        |
|    total_timesteps      | 5296128    |
| train/                  |            |
|    approx_kl            | 0.07857266 |
|    clip_fraction        | 0.626      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31        |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.731     |
|    n_updates            | 2055       |
|    policy_gradient_loss | -0.106     |
|    std                  | 7.72       |
|    value_loss           | 0.0251     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 0.47018388 |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 10         |
|    time_elapsed         | 199        |
|    total_timesteps      | 5300224    |
| train/                  |            |
|    approx_kl            | 0.08627847 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.743     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.115     |
|    std                  | 7.86       |
|    value_loss           | 0.0359     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 474        |
|    ep_rew_mean          | 0.4110722  |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 11         |
|    time_elapsed         | 220        |
|    total_timesteps      | 5304320    |
| train/                  |            |
|    approx_kl            | 0.06764163 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.4      |
|    explained_variance   | 0.503      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.701     |
|    n_updates            | 2085       |
|    policy_gradient_loss | -0.0927    |
|    std                  | 8.06       |
|    value_loss           | 0.117      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 0.5088022  |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 12         |
|    time_elapsed         | 241        |
|    total_timesteps      | 5308416    |
| train/                  |            |
|    approx_kl            | 0.07149217 |
|    clip_fraction        | 0.636      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.7      |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.752     |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.108     |
|    std                  | 8.35       |
|    value_loss           | 0.032      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | random     |
|    stage                | 5          |
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | 0.5322491  |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 13         |
|    time_elapsed         | 262        |
|    total_timesteps      | 5312512    |
| train/                  |            |
|    approx_kl            | 0.07452878 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.9      |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.727     |
|    n_updates            | 2115       |
|    policy_gradient_loss | -0.0887    |
|    std                  | 8.55       |
|    value_loss           | 0.0673     |
----------------------------------------
2025-07-22 19:01:15,728 905523 INFO model saved to cm_sequencing_logs/model_stage_5_random
2025-07-22 19:01:15,730 905523 INFO
Completed stage 5. Intervention 'random' removed from list.
2025-07-22 19:01:15,730 905523 INFO Remaining interventions: 2
2025-07-22 19:01:15,730 905523 INFO CURRICULUM STAGE 6/7
2025-07-22 19:01:15,731 905523 INFO Remaining interventions: ['goal', 'angle']
2025-07-22 19:01:15,731 905523 INFO
Testing intervention 1/2: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -1.591
total data points collected: 501
average episode length: 50.1
average episode reward: -0.159
termination reasons: ['max_length']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: -2.403
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.399
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 30 steps, reward: 3.658
total data points collected: 1032
average episode length: 103.2
average episode reward: -0.034
termination reasons: ['max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 1533
average episode length: 153.3
average episode reward: -0.015
termination reasons: ['max_length', 'max_length', 'success', 'max_length']
success rate: 1/10
total data points collected: 2034
average episode length: 203.4
average episode reward: 0.350
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 2535
average episode length: 253.5
average episode reward: 0.405
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3036
average episode length: 303.6
average episode reward: 0.332
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3537
average episode length: 353.7
average episode reward: 0.601
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/10
total data points collected: 3562
average episode length: 356.2
average episode reward: 0.699
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/10
total data points collected: 4063
average episode length: 406.3
average episode reward: 0.693
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4063, 56]), actions: torch.Size([4063, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3954', '7.7074', '6.4887', '7.8487', '6.8460']
Training reward models...
Reward model losses: ['0.0207', '0.2038', '0.0155', '0.9056', '0.2340']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2908', '1.4183', '1.2964', '1.3043', '1.3424']
Training action VAE models...
Action VAE losses: ['1.4065', '1.4846', '1.5195', '1.2877', '1.3290']
CM score components:
transition disagreement: 0.4175
reward disagreement: 0.3364
state disagreement: 0.4839
action disagreement: 0.5468
total CM score: 1.7846
2025-07-22 19:01:24,672 905523 INFO testing intervention: goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
2025-07-22 19:01:25,614 905523 INFO Episode 1: reward=1.779, length=501, success=False
Reset #2: goal intervention applied (success: True)
2025-07-22 19:01:26,542 905523 INFO Episode 2: reward=0.213, length=501, success=False
Reset #3: goal intervention applied (success: True)
2025-07-22 19:01:27,526 905523 INFO Episode 3: reward=-0.340, length=501, success=False
2025-07-22 19:01:34,163 905523 INFO Results: avg_reward=1.340, success_rate=0.000, avg_length=501.0
2025-07-22 19:01:34,165 905523 INFO
Testing intervention 2/2: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: 0.908
total data points collected: 501
average episode length: 50.1
average episode reward: 0.091
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: -2.527
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.162
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -3.451
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.507
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.895
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -0.791
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: -0.887
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: -0.824
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: -1.218
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: -1.126
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: -1.230
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8172', '6.9548', '6.7157', '7.8291', '6.6885']
Training reward models...
Reward model losses: ['0.1169', '0.0396', '0.0769', '0.2598', '0.0732']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1752', '1.1783', '1.2293', '1.1732', '1.2299']
Training action VAE models...
Action VAE losses: ['1.5867', '1.3406', '1.3662', '1.3786', '1.3539']
CM score components:
transition disagreement: 0.4333
reward disagreement: 0.1007
state disagreement: 0.4766
action disagreement: 0.5381
total CM score: 1.5487
2025-07-22 19:01:44,771 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 19:01:44,999 905523 INFO Episode 1: reward=1.192, length=92, success=True
Reset #2: angle intervention applied (success: True)
2025-07-22 19:01:46,000 905523 INFO Episode 2: reward=-3.563, length=501, success=False
Reset #3: angle intervention applied (success: True)
2025-07-22 19:01:46,124 905523 INFO Episode 3: reward=-1.156, length=39, success=True
2025-07-22 19:01:50,302 905523 INFO Results: avg_reward=-1.148, success_rate=0.500, avg_length=273.5
2025-07-22 19:01:50,304 905523 INFO [Stage 6] goal         | reward=1.34 (norm 1.000) | CM=1.785 (norm 1.000) | α=0.50 → unified=1.000
2025-07-22 19:01:50,304 905523 INFO [Stage 6] angle        | reward=-1.15 (norm 0.000) | CM=1.549 (norm 0.000) | α=0.50 → unified=0.000
2025-07-22 19:01:50,305 905523 INFO Best intervention for stage 6: goal (Unified score: 1.0000)
2025-07-22 19:01:50,305 905523 INFO === stage 6/7: training on goal intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_6_goal
IntervenedCausalWorld created with goal intervention
Reset #1: goal intervention applied (success: True)
Reset #2: goal intervention applied (success: True)
Reset #3: goal intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | goal      |
|    stage             | 6         |
| rollout/             |           |
|    ep_len_mean       | 472       |
|    ep_rew_mean       | 0.6351431 |
| time/                |           |
|    fps               | 454       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5316608   |
------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 463        |
|    ep_rew_mean          | 0.52616143 |
| time/                   |            |
|    fps                  | 270        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 5320704    |
| train/                  |            |
|    approx_kl            | 0.06808059 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.3      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.767     |
|    n_updates            | 2145       |
|    policy_gradient_loss | -0.113     |
|    std                  | 8.93       |
|    value_loss           | 0.0351     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 0.4871732  |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 3          |
|    time_elapsed         | 51         |
|    total_timesteps      | 5324800    |
| train/                  |            |
|    approx_kl            | 0.06425578 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.5      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.768     |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0997    |
|    std                  | 9.16       |
|    value_loss           | 0.025      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 0.65511835 |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 4          |
|    time_elapsed         | 72         |
|    total_timesteps      | 5328896    |
| train/                  |            |
|    approx_kl            | 0.07632306 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.8      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.764     |
|    n_updates            | 2175       |
|    policy_gradient_loss | -0.1       |
|    std                  | 9.41       |
|    value_loss           | 0.0253     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 472        |
|    ep_rew_mean          | 0.66328263 |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 5          |
|    time_elapsed         | 93         |
|    total_timesteps      | 5332992    |
| train/                  |            |
|    approx_kl            | 0.07317001 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.9      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.768     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.101     |
|    std                  | 9.6        |
|    value_loss           | 0.0225     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 465         |
|    ep_rew_mean          | 0.67699975  |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 6           |
|    time_elapsed         | 114         |
|    total_timesteps      | 5337088     |
| train/                  |             |
|    approx_kl            | 0.072675616 |
|    clip_fraction        | 0.58        |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.2       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.753      |
|    n_updates            | 2205        |
|    policy_gradient_loss | -0.0828     |
|    std                  | 9.85        |
|    value_loss           | 0.0395      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 457        |
|    ep_rew_mean          | 0.6785585  |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 7          |
|    time_elapsed         | 136        |
|    total_timesteps      | 5341184    |
| train/                  |            |
|    approx_kl            | 0.07577133 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.4      |
|    explained_variance   | 0.377      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.765     |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0921    |
|    std                  | 10.1       |
|    value_loss           | 0.0335     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 457        |
|    ep_rew_mean          | 0.5553193  |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 8          |
|    time_elapsed         | 157        |
|    total_timesteps      | 5345280    |
| train/                  |            |
|    approx_kl            | 0.07120765 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.5      |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.802     |
|    n_updates            | 2235       |
|    policy_gradient_loss | -0.113     |
|    std                  | 10.2       |
|    value_loss           | 0.0249     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 458        |
|    ep_rew_mean          | 0.48368198 |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 9          |
|    time_elapsed         | 178        |
|    total_timesteps      | 5349376    |
| train/                  |            |
|    approx_kl            | 0.06073003 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.75      |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.082     |
|    std                  | 10.3       |
|    value_loss           | 0.0272     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 454        |
|    ep_rew_mean          | 0.5681073  |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 10         |
|    time_elapsed         | 200        |
|    total_timesteps      | 5353472    |
| train/                  |            |
|    approx_kl            | 0.07033597 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.9      |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.773     |
|    n_updates            | 2265       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 10.8       |
|    value_loss           | 0.0299     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 449        |
|    ep_rew_mean          | 0.4917203  |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 11         |
|    time_elapsed         | 221        |
|    total_timesteps      | 5357568    |
| train/                  |            |
|    approx_kl            | 0.07161199 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.2      |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.796     |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0975    |
|    std                  | 11.1       |
|    value_loss           | 0.0206     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | goal        |
|    stage                | 6           |
| rollout/                |             |
|    ep_len_mean          | 454         |
|    ep_rew_mean          | 0.3853346   |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 12          |
|    time_elapsed         | 1650        |
|    total_timesteps      | 5361664     |
| train/                  |             |
|    approx_kl            | 0.086892515 |
|    clip_fraction        | 0.598       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.5       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.799      |
|    n_updates            | 2295        |
|    policy_gradient_loss | -0.102      |
|    std                  | 11.5        |
|    value_loss           | 0.0298      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | goal       |
|    stage                | 6          |
| rollout/                |            |
|    ep_len_mean          | 458        |
|    ep_rew_mean          | 0.34964737 |
| time/                   |            |
|    fps                  | 31         |
|    iterations           | 13         |
|    time_elapsed         | 1672       |
|    total_timesteps      | 5365760    |
| train/                  |            |
|    approx_kl            | 0.05850994 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.799     |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0973    |
|    std                  | 11.7       |
|    value_loss           | 0.0489     |
----------------------------------------
2025-07-22 19:29:54,674 905523 INFO model saved to cm_sequencing_logs/model_stage_6_goal
2025-07-22 19:29:54,676 905523 INFO
Completed stage 6. Intervention 'goal' removed from list.
2025-07-22 19:29:54,676 905523 INFO Remaining interventions: 1
2025-07-22 19:29:54,676 905523 INFO CURRICULUM STAGE 7/7
2025-07-22 19:29:54,677 905523 INFO Remaining interventions: ['angle']
2025-07-22 19:29:54,677 905523 INFO
Testing intervention 1/1: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 501 steps, reward: -5.347
total data points collected: 501
average episode length: 50.1
average episode reward: -0.535
termination reasons: ['max_length']
success rate: 0/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: 0.891
total data points collected: 1002
average episode length: 100.2
average episode reward: -0.446
termination reasons: ['max_length', 'max_length']
success rate: 0/10
Reset #3: angle intervention applied (success: True)
episode 3: 501 steps, reward: -3.059
total data points collected: 1503
average episode length: 150.3
average episode reward: -0.751
termination reasons: ['max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.948
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: -1.047
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/10
total data points collected: 2698
average episode length: 269.8
average episode reward: -0.866
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/10
total data points collected: 2732
average episode length: 273.2
average episode reward: -0.714
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/10
total data points collected: 3233
average episode length: 323.3
average episode reward: -0.485
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length']
success rate: 2/10
total data points collected: 3734
average episode length: 373.4
average episode reward: -0.549
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/10
total data points collected: 4235
average episode length: 423.5
average episode reward: -0.646
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length', 'success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/10
tensor shapes - states: torch.Size([4235, 56]), actions: torch.Size([4235, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.4430', '6.8732', '7.2670', '7.7513', '8.1213']
Training reward models...
Reward model losses: ['0.0617', '0.0273', '0.1570', '0.0342', '0.1544']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4044', '1.3101', '1.1516', '1.2688', '1.2345']
Training action VAE models...
Action VAE losses: ['1.4413', '1.4570', '1.4709', '1.4503', '1.4223']
CM score components:
transition disagreement: 0.4549
reward disagreement: 0.0770
state disagreement: 0.5033
action disagreement: 0.5616
total CM score: 1.5968
2025-07-22 19:30:04,039 905523 INFO testing intervention: angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
2025-07-22 19:30:05,164 905523 INFO Episode 1: reward=-3.348, length=501, success=False
Reset #2: angle intervention applied (success: True)
2025-07-22 19:30:05,329 905523 INFO Episode 2: reward=0.843, length=61, success=True
Reset #3: angle intervention applied (success: True)
2025-07-22 19:30:06,343 905523 INFO Episode 3: reward=-0.841, length=501, success=False
2025-07-22 19:30:13,091 905523 INFO Results: avg_reward=-0.599, success_rate=0.100, avg_length=457.0
2025-07-22 19:30:13,093 905523 INFO [Stage 7] angle        | reward=-0.60 (norm 0.000) | CM=1.597 (norm 0.000) | α=0.50 → unified=0.000
2025-07-22 19:30:13,093 905523 INFO Best intervention for stage 7: angle (Unified score: 0.0000)
2025-07-22 19:30:13,093 905523 INFO === stage 7/7: training on angle intervention ===
Logging to cm_sequencing_logs/sb3_csv_logs_7_angle
IntervenedCausalWorld created with angle intervention
Reset #1: angle intervention applied (success: True)
Reset #2: angle intervention applied (success: True)
Reset #3: angle intervention applied (success: True)
-------------------------------------
| custom/              |            |
|    intervention_type | angle      |
|    stage             | 7          |
| rollout/             |            |
|    ep_len_mean       | 450        |
|    ep_rew_mean       | 0.37849158 |
| time/                |            |
|    fps               | 452        |
|    iterations        | 1          |
|    time_elapsed      | 9          |
|    total_timesteps   | 5369856    |
-------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 438         |
|    ep_rew_mean          | 0.24810418  |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 2           |
|    time_elapsed         | 30          |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.060520004 |
|    clip_fraction        | 0.539       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.784      |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0865     |
|    std                  | 12.1        |
|    value_loss           | 0.0319      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 429         |
|    ep_rew_mean          | 0.114717215 |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 3           |
|    time_elapsed         | 51          |
|    total_timesteps      | 5378048     |
| train/                  |             |
|    approx_kl            | 0.07005217  |
|    clip_fraction        | 0.584       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.2       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.806      |
|    n_updates            | 2355        |
|    policy_gradient_loss | -0.0999     |
|    std                  | 12.5        |
|    value_loss           | 0.0517      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 430        |
|    ep_rew_mean          | -0.0132798 |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 4          |
|    time_elapsed         | 73         |
|    total_timesteps      | 5382144    |
| train/                  |            |
|    approx_kl            | 0.07279195 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.5      |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.824     |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.107     |
|    std                  | 12.8       |
|    value_loss           | 0.059      |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 429         |
|    ep_rew_mean          | -0.26233345 |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 5           |
|    time_elapsed         | 94          |
|    total_timesteps      | 5386240     |
| train/                  |             |
|    approx_kl            | 0.07794024  |
|    clip_fraction        | 0.622       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.6       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.83       |
|    n_updates            | 2385        |
|    policy_gradient_loss | -0.113      |
|    std                  | 13          |
|    value_loss           | 0.0426      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | -0.46980163 |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 6           |
|    time_elapsed         | 116         |
|    total_timesteps      | 5390336     |
| train/                  |             |
|    approx_kl            | 0.06906374  |
|    clip_fraction        | 0.584       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.827      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.105      |
|    std                  | 13.3        |
|    value_loss           | 0.0366      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | -0.81014943 |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 7           |
|    time_elapsed         | 137         |
|    total_timesteps      | 5394432     |
| train/                  |             |
|    approx_kl            | 0.06262973  |
|    clip_fraction        | 0.584       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36         |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.844      |
|    n_updates            | 2415        |
|    policy_gradient_loss | -0.107      |
|    std                  | 13.6        |
|    value_loss           | 0.0447      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 371        |
|    ep_rew_mean          | -0.8185583 |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 8          |
|    time_elapsed         | 159        |
|    total_timesteps      | 5398528    |
| train/                  |            |
|    approx_kl            | 0.06368968 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.672      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.84      |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.108     |
|    std                  | 13.7       |
|    value_loss           | 0.0336     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 375         |
|    ep_rew_mean          | -0.8131119  |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 9           |
|    time_elapsed         | 180         |
|    total_timesteps      | 5402624     |
| train/                  |             |
|    approx_kl            | 0.071114875 |
|    clip_fraction        | 0.61        |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.843      |
|    n_updates            | 2445        |
|    policy_gradient_loss | -0.11       |
|    std                  | 14          |
|    value_loss           | 0.0367      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 372        |
|    ep_rew_mean          | -1.0010794 |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 10         |
|    time_elapsed         | 201        |
|    total_timesteps      | 5406720    |
| train/                  |            |
|    approx_kl            | 0.06735776 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.5      |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.848     |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.11      |
|    std                  | 14.3       |
|    value_loss           | 0.0283     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | angle       |
|    stage                | 7           |
| rollout/                |             |
|    ep_len_mean          | 362         |
|    ep_rew_mean          | -1.0663695  |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 11          |
|    time_elapsed         | 223         |
|    total_timesteps      | 5410816     |
| train/                  |             |
|    approx_kl            | 0.062061135 |
|    clip_fraction        | 0.562       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.6       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.851      |
|    n_updates            | 2475        |
|    policy_gradient_loss | -0.107      |
|    std                  | 14.5        |
|    value_loss           | 0.0295      |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 366        |
|    ep_rew_mean          | -1.0260533 |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 12         |
|    time_elapsed         | 245        |
|    total_timesteps      | 5414912    |
| train/                  |            |
|    approx_kl            | 0.0700666  |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.856     |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.108     |
|    std                  | 14.8       |
|    value_loss           | 0.0302     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | angle      |
|    stage                | 7          |
| rollout/                |            |
|    ep_len_mean          | 367        |
|    ep_rew_mean          | -1.101225  |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 13         |
|    time_elapsed         | 266        |
|    total_timesteps      | 5419008    |
| train/                  |            |
|    approx_kl            | 0.07505879 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.861     |
|    n_updates            | 2505       |
|    policy_gradient_loss | -0.11      |
|    std                  | 15.2       |
|    value_loss           | 0.0237     |
----------------------------------------
2025-07-22 19:34:52,118 905523 INFO model saved to cm_sequencing_logs/model_stage_7_angle
2025-07-22 19:34:52,120 905523 INFO
Completed stage 7. Intervention 'angle' removed from list.
2025-07-22 19:34:52,120 905523 INFO Remaining interventions: 0
Aggregated SB3 progress saved to cm_sequencing_logs/all_progress.csv
2025-07-22 19:34:59,517 905523 INFO SB3 training curves saved to cm_sequencing_logs/plots/training_curves_sb3.png
2025-07-22 19:34:59,518 905523 INFO ===final evaluation===
2025-07-22 19:35:15,138 905523 INFO episode 16: reward=-1.851, length=501, success=True
2025-07-22 19:35:16,111 905523 INFO episode 17: reward=-1.851, length=501, success=True
2025-07-22 19:35:17,089 905523 INFO episode 18: reward=-1.851, length=501, success=True
2025-07-22 19:35:18,064 905523 INFO episode 19: reward=-1.851, length=501, success=True
2025-07-22 19:35:19,039 905523 INFO episode 20: reward=-1.851, length=501, success=True
2025-07-22 19:35:19,040 905523 INFO Final performance
2025-07-22 19:35:19,040 905523 INFO average reward: -1.851 +/- 0.000
2025-07-22 19:35:19,041 905523 INFO success rate: 1.000
2025-07-22 19:35:19,041 905523 INFO average episode length: 501.0
2025-07-22 19:35:19,041 905523 INFO curriculum sequencing completed
2025-07-22 19:35:19,041 905523 INFO sequence order:
2025-07-22 19:35:19,041 905523 INFO 1. position (test_reward: 1.162)
2025-07-22 19:35:19,041 905523 INFO 2. friction (test_reward: 4.476)
2025-07-22 19:35:19,041 905523 INFO 3. visual (test_reward: 5.350)
2025-07-22 19:35:19,041 905523 INFO 4. mass (test_reward: 2.279)
2025-07-22 19:35:19,041 905523 INFO 5. random (test_reward: 0.894)
2025-07-22 19:35:19,042 905523 INFO 6. goal (test_reward: 1.340)
2025-07-22 19:35:19,042 905523 INFO 7. angle (test_reward: -0.599)
2025-07-22 19:35:19,042 905523 INFO
performance comparison
2025-07-22 19:35:19,042 905523 INFO initial: 3.394 reward, 1.000 success
2025-07-22 19:35:19,042 905523 INFO final: -1.851 reward, 1.000 success
2025-07-22 19:35:19,042 905523 INFO improvement: -5.245 reward
2025-07-22 19:35:19,055 905523 INFO Final model saved to cm_sequencing_logs/final_model_after_sequencing.zip
2025-07-22 19:35:19,056 905523 INFO results saved to cm_sequencing_logs/sequencing_results.json
