
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 30
   student training steps: 50000
==================================================
2025-07-17 00:11:17,226 3817156 INFO Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
performance summary:
success rate: 1.000 (5/5)
average reward: -0.771
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 45.214
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 16.304
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 14.194
total data points collected: 40025
average episode length: 8005.0
average episode reward: 18.592
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
meta_teacher_student.py:251: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4530', '7.8095', '7.3438', '7.0046', '7.5950']
Training reward models...
Reward model losses: ['0.3488', '0.1874', '0.0210', '0.9681', '0.0037']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9971', '1.0240', '1.0652', '1.0113', '0.9587']
Training action VAE models...
Action VAE losses: ['1.0915', '1.1447', '1.0703', '1.0606', '0.9321']
CM score components:
transition disagreement: 0.3868
reward disagreement: 0.3493
state disagreement: 0.4907
action disagreement: 0.5201
total CM score: 1.7469
goal is complete. CM score: 1.7469
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 29 steps, reward: -0.761
Reset #2: mass intervention applied (success: True)
episode 2: 34 steps, reward: -0.422
Reset #3: mass intervention applied (success: True)
episode 3: 30 steps, reward: -0.534
total data points collected: 151
average episode length: 30.2
average episode reward: -0.741
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([151, 56]), actions: torch.Size([151, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3180', '6.9642', '7.7626', '7.5738', '7.8932']
Training reward models...
Reward model losses: ['0.1668', '0.0219', '0.0115', '0.0837', '0.0423']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8087', '1.7376', '1.7994', '1.9091', '1.7669']
Training action VAE models...
Action VAE losses: ['1.3928', '1.3632', '1.4283', '1.4441', '1.3337']
CM score components:
transition disagreement: 0.4351
reward disagreement: 0.1166
state disagreement: 0.5346
action disagreement: 0.5420
total CM score: 1.6283
mass is complete. CM score: 1.6283
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 27 steps, reward: -0.764
Reset #2: friction intervention applied (success: True)
episode 2: 27 steps, reward: -0.771
Reset #3: friction intervention applied (success: True)
episode 3: 27 steps, reward: -0.771
total data points collected: 136
average episode length: 27.2
average episode reward: -0.790
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([136, 56]), actions: torch.Size([136, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8421', '7.4845', '7.6449', '7.9799', '6.7912']
Training reward models...
Reward model losses: ['0.0217', '0.1581', '0.5899', '0.0356', '0.0690']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3773', '1.5370', '1.7074', '1.5879', '1.5768']
Training action VAE models...
Action VAE losses: ['1.4240', '1.3458', '1.5365', '1.3256', '1.4250']
CM score components:
transition disagreement: 0.3655
reward disagreement: 0.2209
state disagreement: 0.5122
action disagreement: 0.5682
total CM score: 1.6668
friction is complete. CM score: 1.6668
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 27 steps, reward: -0.771
Reset #2: visual intervention applied (success: True)
episode 2: 27 steps, reward: -0.771
Reset #3: visual intervention applied (success: True)
episode 3: 27 steps, reward: -0.771
total data points collected: 135
average episode length: 27.0
average episode reward: -0.771
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([135, 56]), actions: torch.Size([135, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7998', '7.7304', '7.8474', '7.5528', '7.4615']
Training reward models...
Reward model losses: ['0.0465', '0.0097', '0.0692', '0.0378', '0.0049']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6547', '1.8368', '1.5178', '1.5826', '1.7961']
Training action VAE models...
Action VAE losses: ['1.4371', '1.4053', '1.4527', '1.4541', '1.4050']
CM score components:
transition disagreement: 0.3661
reward disagreement: 0.0685
state disagreement: 0.4895
action disagreement: 0.5668
total CM score: 1.4909
visual is complete. CM score: 1.4909
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 104.537
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 18.875
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 12.484
total data points collected: 40030
average episode length: 8006.0
average episode reward: 30.308
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40030, 56]), actions: torch.Size([40030, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3780', '7.3473', '7.6298', '7.7960', '7.4765']
Training reward models...
Reward model losses: ['0.0141', '0.0530', '0.0155', '0.0131', '0.0166']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9747', '0.9201', '0.9646', '0.9782', '1.0084']
Training action VAE models...
Action VAE losses: ['1.2062', '1.0083', '1.0961', '1.1192', '1.3217']
CM score components:
transition disagreement: 0.4235
reward disagreement: 0.1052
state disagreement: 0.4664
action disagreement: 0.5390
total CM score: 1.5341
pose is complete. CM score: 1.5341
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 18.717
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.825
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.332
total data points collected: 50005
average episode length: 10001.0
average episode reward: 28.136
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3097', '6.8237', '7.3788', '7.5246', '7.5979']
Training reward models...
Reward model losses: ['0.0046', '0.1462', '0.0785', '0.0061', '1.3262']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1516', '1.3164', '1.2524', '1.1430', '1.3041']
Training action VAE models...
Action VAE losses: ['1.3559', '1.3617', '1.3519', '1.3987', '1.4783']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.3924
state disagreement: 0.5085
action disagreement: 0.5451
total CM score: 1.8716
random is complete. CM score: 1.8716
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 254       |
|    ep_rew_mean     | 2.2132688 |
| time/              |           |
|    fps             | 463       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 254         |
|    ep_rew_mean          | 2.2132688   |
| time/                   |             |
|    fps                  | 289         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.035862505 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.519      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0915     |
|    std                  | 2.65        |
|    value_loss           | 0.0527      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 254         |
|    ep_rew_mean          | 2.2132688   |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 3           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.029173575 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.517      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.071      |
|    std                  | 2.66        |
|    value_loss           | 0.0384      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 352         |
|    ep_rew_mean          | 2.285199    |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 4           |
|    time_elapsed         | 67          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.022583915 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.479      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0459     |
|    std                  | 2.69        |
|    value_loss           | 0.0241      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 352         |
|    ep_rew_mean          | 2.285199    |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 5           |
|    time_elapsed         | 86          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.030001668 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.538      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0901     |
|    std                  | 2.72        |
|    value_loss           | 0.024       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 449         |
|    ep_rew_mean          | 2.9759078   |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 6           |
|    time_elapsed         | 105         |
|    total_timesteps      | 5070848     |
| train/                  |             |
|    approx_kl            | 0.034329638 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.555      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0969     |
|    std                  | 2.74        |
|    value_loss           | 0.0147      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 449        |
|    ep_rew_mean          | 2.9759078  |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 7          |
|    time_elapsed         | 124        |
|    total_timesteps      | 5074944    |
| train/                  |            |
|    approx_kl            | 0.04215638 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.537     |
|    n_updates            | 1245       |
|    policy_gradient_loss | -0.0842    |
|    std                  | 2.76       |
|    value_loss           | 0.0223     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 547         |
|    ep_rew_mean          | 2.9819613   |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 8           |
|    time_elapsed         | 143         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.046823226 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.571      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.108      |
|    std                  | 2.78        |
|    value_loss           | 0.00819     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 558         |
|    ep_rew_mean          | 3.044444    |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 9           |
|    time_elapsed         | 162         |
|    total_timesteps      | 5083136     |
| train/                  |             |
|    approx_kl            | 0.052595314 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.559      |
|    n_updates            | 1275        |
|    policy_gradient_loss | -0.105      |
|    std                  | 2.82        |
|    value_loss           | 0.00389     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 558        |
|    ep_rew_mean          | 3.044444   |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 10         |
|    time_elapsed         | 181        |
|    total_timesteps      | 5087232    |
| train/                  |            |
|    approx_kl            | 0.04730013 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.574     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.84       |
|    value_loss           | 0.0121     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 654        |
|    ep_rew_mean          | 3.4917626  |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 11         |
|    time_elapsed         | 200        |
|    total_timesteps      | 5091328    |
| train/                  |            |
|    approx_kl            | 0.03797015 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.565     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 2.88       |
|    value_loss           | 0.00487    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 673        |
|    ep_rew_mean          | 3.5982306  |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 12         |
|    time_elapsed         | 220        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.04562588 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.542     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0757    |
|    std                  | 2.94       |
|    value_loss           | 0.0103     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 673         |
|    ep_rew_mean          | 3.5982306   |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 13          |
|    time_elapsed         | 239         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.054817595 |
|    clip_fraction        | 0.479       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.541      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.0884     |
|    std                  | 2.97        |
|    value_loss           | 0.0161      |
-----------------------------------------
saved student model to meta_teacher_student_logs/temp_student_model_episode_0.zip

evaluating student performance (5 episodes)...
performance summary:
success rate: 0.000 (0/5)
average reward: -5.788
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 32.797
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -4.442
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 59.364
total data points collected: 50005
average episode length: 10001.0
average episode reward: 16.415
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6941', '8.2222', '7.3892', '7.7608', '8.1667']
Training reward models...
Reward model losses: ['0.0062', '0.1796', '0.0252', '1.2898', '2.0538']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3687', '1.3061', '1.0904', '1.3522', '1.2533']
Training action VAE models...
Action VAE losses: ['1.3162', '1.2916', '1.2430', '1.3873', '1.3377']
CM score components:
transition disagreement: 0.4416
reward disagreement: 0.5064
state disagreement: 0.5218
action disagreement: 0.5381
total CM score: 2.0079
goal is complete. CM score: 2.0079
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.881
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.183
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -1.569
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.689
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7529', '7.2748', '7.9162', '7.6451', '6.8259']
Training reward models...
Reward model losses: ['0.5648', '0.3951', '0.0179', '0.1775', '0.1967']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3786', '1.1544', '1.0985', '1.1547', '1.2717']
Training action VAE models...
Action VAE losses: ['1.2619', '1.3100', '1.2488', '1.3636', '1.3184']
CM score components:
transition disagreement: 0.4419
reward disagreement: 0.2921
state disagreement: 0.4818
action disagreement: 0.5290
total CM score: 1.7448
mass is complete. CM score: 1.7448
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 98.382
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.436
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 88.286
total data points collected: 40143
average episode length: 8028.6
average episode reward: 55.244
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40143, 56]), actions: torch.Size([40143, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4768', '7.3328', '7.2265', '6.8358', '8.0935']
Training reward models...
Reward model losses: ['0.6858', '0.0291', '0.0542', '0.0884', '0.2257']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2015', '1.2825', '1.1388', '1.1745', '1.1527']
Training action VAE models...
Action VAE losses: ['1.3756', '1.4566', '1.3440', '1.4531', '1.4621']
CM score components:
transition disagreement: 0.4142
reward disagreement: 0.2548
state disagreement: 0.4971
action disagreement: 0.5383
total CM score: 1.7044
friction is complete. CM score: 1.7044
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -5.788
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -5.788
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -5.788
total data points collected: 50005
average episode length: 10001.0
average episode reward: -5.788
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8179', '7.4912', '7.7537', '7.5726', '7.3305']
Training reward models...
Reward model losses: ['0.3307', '0.7182', '0.0059', '0.0020', '0.2118']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7225', '0.8995', '0.7539', '0.7329', '0.7103']
Training action VAE models...
Action VAE losses: ['1.0113', '1.0080', '1.2113', '0.9570', '0.9114']
CM score components:
transition disagreement: 0.4114
reward disagreement: 0.2204
state disagreement: 0.4376
action disagreement: 0.5723
total CM score: 1.6417
visual is complete. CM score: 1.6417
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: -3.642
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 21.361
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -3.529
total data points collected: 40486
average episode length: 8097.2
average episode reward: 6.340
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40486, 56]), actions: torch.Size([40486, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9687', '7.2350', '8.2653', '8.1007', '6.7037']
Training reward models...
Reward model losses: ['0.4562', '0.0125', '0.1832', '0.0159', '0.0295']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3499', '1.4039', '1.1343', '1.1977', '1.2635']
Training action VAE models...
Action VAE losses: ['1.3649', '1.4465', '1.3129', '1.4893', '1.4513']
CM score components:
transition disagreement: 0.4401
reward disagreement: 0.1452
state disagreement: 0.5178
action disagreement: 0.5146
total CM score: 1.6176
pose is complete. CM score: 1.6176
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.928
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.950
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -2.430
total data points collected: 50005
average episode length: 10001.0
average episode reward: 10.634
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7797', '7.6487', '6.9178', '7.8685', '7.8869']
Training reward models...
Reward model losses: ['0.9332', '0.3959', '0.1069', '0.9247', '0.0060']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1874', '1.3184', '1.2892', '1.1878', '1.2260']
Training action VAE models...
Action VAE losses: ['1.4712', '1.5055', '1.4898', '1.3125', '1.3265']
CM score components:
transition disagreement: 0.4157
reward disagreement: 0.4790
state disagreement: 0.4875
action disagreement: 0.5584
total CM score: 1.9406
random is complete. CM score: 1.9406
2025-07-17 00:31:02,977 3817156 INFO Meta-Episode 1/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
2025-07-17 00:31:02,977 3817156 INFO Meta-Episode 1: Stagnation check - steps_since_change=0, patience_remaining=50000
loading student model from meta_teacher_student_logs/temp_student_model_episode_0.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 32.797
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -4.442
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 59.364
total data points collected: 50005
average episode length: 10001.0
average episode reward: 16.415
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3750', '7.2335', '6.8366', '7.0218', '7.6629']
Training reward models...
Reward model losses: ['0.1818', '0.2181', '0.2570', '0.0040', '0.0176']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1362', '1.3063', '1.2660', '1.2336', '1.1902']
Training action VAE models...
Action VAE losses: ['1.4703', '1.3383', '1.3608', '1.2465', '1.2210']
CM score components:
transition disagreement: 0.4165
reward disagreement: 0.0743
state disagreement: 0.4629
action disagreement: 0.5592
total CM score: 1.5129
goal is complete. CM score: 1.5129
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.881
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.183
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -1.569
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.689
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0873', '7.7257', '6.9919', '6.8720', '8.3271']
Training reward models...
Reward model losses: ['0.0080', '0.1393', '0.0117', '0.5823', '0.4622']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3068', '1.1171', '1.1611', '1.2143', '1.3609']
Training action VAE models...
Action VAE losses: ['1.3162', '1.3377', '1.2512', '1.3308', '1.2513']
CM score components:
transition disagreement: 0.4192
reward disagreement: 0.1788
state disagreement: 0.5182
action disagreement: 0.5462
total CM score: 1.6624
mass is complete. CM score: 1.6624
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 98.382
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.436
Reset #3: friction intervention applied (success: True)
