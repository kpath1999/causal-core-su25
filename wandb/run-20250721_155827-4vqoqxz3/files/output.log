2025-07-21 15:58:30,191 3247582 INFO [PRETRAINED] Using pretrained model path: ppo_pushing_sb3/final_model.zip
2025-07-21 15:58:30,194 3247582 INFO Starting with 7 interventions
2025-07-21 15:58:30,194 3247582 INFO ===final evaluation===
2025-07-21 15:58:40,852 3247582 INFO Final performance
2025-07-21 15:58:40,853 3247582 INFO average reward: 3.394 +/- 0.000
2025-07-21 15:58:40,853 3247582 INFO success rate: 1.000
2025-07-21 15:58:40,853 3247582 INFO average episode length: 501.0
2025-07-21 15:58:40,853 3247582 INFO initial performance: {'avg_reward': 3.393721938342554, 'reward_std': 4.440892098500626e-16, 'avg_length': 501.0, 'success_rate': 1.0, 'total_episodes': 10}
2025-07-21 15:58:40,853 3247582 INFO CURRICULUM STAGE 1/7
2025-07-21 15:58:40,853 3247582 INFO Remaining interventions: ['goal', 'mass', 'friction', 'visual', 'position', 'angle', 'random']
2025-07-21 15:58:40,854 3247582 INFO
Testing intervention 1/7: goal (CM score)
IntervenedCausalWorld created with goal intervention
evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: -0.176
total data points collected: 501
average episode length: 50.1
average episode reward: -0.018
termination reasons: ['other']
success rate: 0/10
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.340
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.216
termination reasons: ['other', 'other']
success rate: 0/10
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: -0.679
total data points collected: 1503
average episode length: 150.3
average episode reward: 0.149
termination reasons: ['other', 'other', 'other']
success rate: 0/10
total data points collected: 2004
average episode length: 200.4
average episode reward: -0.008
termination reasons: ['other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 2505
average episode length: 250.5
average episode reward: 0.141
termination reasons: ['other', 'other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 3006
average episode length: 300.6
average episode reward: 0.376
termination reasons: ['other', 'other', 'other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 3507
average episode length: 350.7
average episode reward: 0.420
termination reasons: ['other', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 4008
average episode length: 400.8
average episode reward: 0.463
termination reasons: ['other', 'other', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 4509
average episode length: 450.9
average episode reward: 0.595
termination reasons: ['other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 0/10
total data points collected: 5010
average episode length: 501.0
average episode reward: 0.421
termination reasons: ['other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 0/10
baselines.py:200: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([5010, 56]), actions: torch.Size([5010, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3214', '7.0977', '6.8085', '7.1494', '7.6413']
Training reward models...
Reward model losses: ['0.2433', '0.2068', '0.2358', '0.0093', '0.0194']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2406', '1.2135', '1.2528', '1.3185', '1.3497']
Training action VAE models...
Action VAE losses: ['1.5555', '1.4313', '1.4925', '1.3706', '1.3680']
CM score components:
transition disagreement: 0.4095
reward disagreement: 0.0940
state disagreement: 0.4722
action disagreement: 0.5613
total CM score: 1.5369
2025-07-21 15:58:51,728 3247582 INFO
Testing intervention 2/7: mass (CM score)
IntervenedCausalWorld created with mass intervention
evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 33 steps, reward: -0.196
total data points collected: 33
average episode length: 3.3
average episode reward: -0.020
termination reasons: ['success']
success rate: 1/10
Reset #2: mass intervention applied (success: True)
episode 2: 33 steps, reward: -0.226
total data points collected: 66
average episode length: 6.6
average episode reward: -0.042
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.175
total data points collected: 567
average episode length: 56.7
average episode reward: 0.275
termination reasons: ['success', 'success', 'other']
success rate: 2/10
total data points collected: 606
average episode length: 60.6
average episode reward: 0.171
termination reasons: ['success', 'success', 'other', 'success']
success rate: 3/10
total data points collected: 869
average episode length: 86.9
average episode reward: 0.407
termination reasons: ['success', 'success', 'other', 'success', 'success']
success rate: 4/10
total data points collected: 1370
average episode length: 137.0
average episode reward: 0.560
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other']
success rate: 4/10
total data points collected: 1871
average episode length: 187.1
average episode reward: 0.995
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other']
success rate: 4/10
total data points collected: 2372
average episode length: 237.2
average episode reward: 1.482
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'other']
success rate: 4/10
total data points collected: 2410
average episode length: 241.0
average episode reward: 1.536
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'other', 'success']
success rate: 5/10
total data points collected: 2533
average episode length: 253.3
average episode reward: 1.517
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'other', 'success', 'success']
success rate: 6/10
tensor shapes - states: torch.Size([2533, 56]), actions: torch.Size([2533, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0618', '7.7143', '7.1078', '6.8695', '8.2497']
Training reward models...
Reward model losses: ['0.0085', '0.3513', '0.0077', '0.3001', '0.5900']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8094', '1.8174', '1.8916', '1.9281', '2.4819']
Training action VAE models...
Action VAE losses: ['1.4794', '1.4375', '1.3607', '1.4333', '1.3518']
CM score components:
transition disagreement: 0.4201
reward disagreement: 0.2101
state disagreement: 0.6020
action disagreement: 0.5512
total CM score: 1.7833
2025-07-21 15:58:58,025 3247582 INFO
Testing intervention 3/7: friction (CM score)
IntervenedCausalWorld created with friction intervention
evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 40 steps, reward: 0.452
total data points collected: 40
average episode length: 4.0
average episode reward: 0.045
termination reasons: ['success']
success rate: 1/10
Reset #2: friction intervention applied (success: True)
episode 2: 40 steps, reward: -0.118
total data points collected: 80
average episode length: 8.0
average episode reward: 0.033
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.317
total data points collected: 581
average episode length: 58.1
average episode reward: 0.365
termination reasons: ['success', 'success', 'other']
success rate: 2/10
total data points collected: 613
average episode length: 61.3
average episode reward: 0.354
termination reasons: ['success', 'success', 'other', 'success']
success rate: 3/10
total data points collected: 877
average episode length: 87.7
average episode reward: 0.609
termination reasons: ['success', 'success', 'other', 'success', 'success']
success rate: 4/10
total data points collected: 1378
average episode length: 137.8
average episode reward: 1.002
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other']
success rate: 4/10
total data points collected: 1879
average episode length: 187.9
average episode reward: 1.386
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other']
success rate: 4/10
total data points collected: 1961
average episode length: 196.1
average episode reward: 1.438
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'success']
success rate: 5/10
total data points collected: 1996
average episode length: 199.6
average episode reward: 1.429
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'success', 'success']
success rate: 6/10
total data points collected: 2497
average episode length: 249.7
average episode reward: 1.617
termination reasons: ['success', 'success', 'other', 'success', 'success', 'other', 'other', 'success', 'success', 'other']
success rate: 6/10
tensor shapes - states: torch.Size([2497, 56]), actions: torch.Size([2497, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9008', '7.2530', '6.8730', '7.7103', '7.0574']
Training reward models...
Reward model losses: ['0.0653', '1.1019', '0.5024', '0.0242', '0.2265']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7264', '1.5889', '1.5984', '1.8298', '1.7181']
Training action VAE models...
Action VAE losses: ['1.2996', '1.3674', '1.2929', '1.3067', '1.4268']
CM score components:
transition disagreement: 0.4215
reward disagreement: 0.4378
state disagreement: 0.5306
action disagreement: 0.5179
total CM score: 1.9078
2025-07-21 15:59:04,262 3247582 INFO
Testing intervention 4/7: visual (CM score)
IntervenedCausalWorld created with visual intervention
evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.799
total data points collected: 501
average episode length: 50.1
average episode reward: 0.380
termination reasons: ['other']
success rate: 0/10
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.047
total data points collected: 1002
average episode length: 100.2
average episode reward: 0.685
termination reasons: ['other', 'other']
success rate: 0/10
Reset #3: visual intervention applied (success: True)
episode 3: 401 steps, reward: 4.829
total data points collected: 1403
average episode length: 140.3
average episode reward: 1.167
termination reasons: ['other', 'other', 'success']
success rate: 1/10
total data points collected: 1904
average episode length: 190.4
average episode reward: 1.487
termination reasons: ['other', 'other', 'success', 'other']
success rate: 1/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 1.396
termination reasons: ['other', 'other', 'success', 'other', 'success']
success rate: 2/10
total data points collected: 1978
average episode length: 197.8
average episode reward: 1.337
termination reasons: ['other', 'other', 'success', 'other', 'success', 'success']
success rate: 3/10
total data points collected: 2010
average episode length: 201.0
average episode reward: 1.159
termination reasons: ['other', 'other', 'success', 'other', 'success', 'success', 'success']
success rate: 4/10
total data points collected: 2511
average episode length: 251.1
average episode reward: 1.386
termination reasons: ['other', 'other', 'success', 'other', 'success', 'success', 'success', 'other']
success rate: 4/10
total data points collected: 3012
average episode length: 301.2
average episode reward: 1.803
termination reasons: ['other', 'other', 'success', 'other', 'success', 'success', 'success', 'other', 'other']
success rate: 4/10
total data points collected: 3513
average episode length: 351.3
average episode reward: 2.134
termination reasons: ['other', 'other', 'success', 'other', 'success', 'success', 'success', 'other', 'other', 'other']
success rate: 4/10
tensor shapes - states: torch.Size([3513, 56]), actions: torch.Size([3513, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1150', '7.7619', '7.0914', '7.1919', '7.4913']
Training reward models...
Reward model losses: ['0.0551', '0.2885', '0.1062', '0.0198', '0.0581']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6096', '1.7050', '1.6790', '1.7611', '1.6296']
Training action VAE models...
Action VAE losses: ['1.4084', '1.3043', '1.3701', '1.4613', '1.5215']
CM score components:
transition disagreement: 0.4395
reward disagreement: 0.1506
state disagreement: 0.5061
action disagreement: 0.5374
total CM score: 1.6336
2025-07-21 15:59:12,304 3247582 INFO
Testing intervention 5/7: position (CM score)
IntervenedCausalWorld created with position intervention
evaluating CM score for position intervention...
Reset #1: position intervention applied (success: True)
episode 1: 35 steps, reward: -1.696
total data points collected: 35
average episode length: 3.5
average episode reward: -0.170
termination reasons: ['success']
success rate: 1/10
Reset #2: position intervention applied (success: True)
episode 2: 501 steps, reward: -3.449
total data points collected: 536
average episode length: 53.6
average episode reward: -0.515
termination reasons: ['success', 'other']
success rate: 1/10
Reset #3: position intervention applied (success: True)
episode 3: 501 steps, reward: 2.055
total data points collected: 1037
average episode length: 103.7
average episode reward: -0.309
termination reasons: ['success', 'other', 'other']
success rate: 1/10
total data points collected: 1538
average episode length: 153.8
average episode reward: -0.190
termination reasons: ['success', 'other', 'other', 'other']
success rate: 1/10
total data points collected: 2039
average episode length: 203.9
average episode reward: -0.029
termination reasons: ['success', 'other', 'other', 'other', 'other']
success rate: 1/10
total data points collected: 2540
average episode length: 254.0
average episode reward: 0.394
termination reasons: ['success', 'other', 'other', 'other', 'other', 'other']
success rate: 1/10
total data points collected: 3041
average episode length: 304.1
average episode reward: 0.628
termination reasons: ['success', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 1/10
total data points collected: 3542
average episode length: 354.2
average episode reward: 0.782
termination reasons: ['success', 'other', 'other', 'other', 'other', 'other', 'other', 'other']
success rate: 1/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 0.803
termination reasons: ['success', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'success']
success rate: 2/10
total data points collected: 4078
average episode length: 407.8
average episode reward: 1.155
termination reasons: ['success', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'success', 'other']
success rate: 2/10
tensor shapes - states: torch.Size([4078, 56]), actions: torch.Size([4078, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7852', '7.7244', '7.6858', '6.6319', '7.3930']
Training reward models...
Reward model losses: ['3.0782', '1.1042', '0.1575', '0.0203', '0.0340']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6499', '1.7088', '1.5572', '1.6576', '1.4569']
Training action VAE models...
Action VAE losses: ['1.3798', '1.5109', '1.4191', '1.4644', '1.3449']
CM score components:
transition disagreement: 0.4450
reward disagreement: 0.7511
state disagreement: 0.5280
action disagreement: 0.5551
total CM score: 2.2792
2025-07-21 15:59:21,476 3247582 INFO
Testing intervention 6/7: angle (CM score)
IntervenedCausalWorld created with angle intervention
evaluating CM score for angle intervention...
Reset #1: angle intervention applied (success: True)
episode 1: 338 steps, reward: 2.353
total data points collected: 338
average episode length: 33.8
average episode reward: 0.235
termination reasons: ['success']
success rate: 1/10
Reset #2: angle intervention applied (success: True)
episode 2: 501 steps, reward: 0.355
total data points collected: 839
average episode length: 83.9
average episode reward: 0.271
termination reasons: ['success', 'other']
success rate: 1/10
Reset #3: angle intervention applied (success: True)
episode 3: 33 steps, reward: -1.702
total data points collected: 872
average episode length: 87.2
average episode reward: 0.101
termination reasons: ['success', 'other', 'success']
success rate: 2/10
total data points collected: 1201
average episode length: 120.1
average episode reward: 0.305
termination reasons: ['success', 'other', 'success', 'success']
success rate: 3/10
total data points collected: 1702
average episode length: 170.2
average episode reward: 0.311
termination reasons: ['success', 'other', 'success', 'success', 'other']
success rate: 3/10
total data points collected: 1942
average episode length: 194.2
average episode reward: 0.525
termination reasons: ['success', 'other', 'success', 'success', 'other', 'success']
success rate: 4/10
total data points collected: 1975
average episode length: 197.5
average episode reward: 0.541
termination reasons: ['success', 'other', 'success', 'success', 'other', 'success', 'success']
success rate: 5/10
total data points collected: 2476
average episode length: 247.6
average episode reward: 0.835
termination reasons: ['success', 'other', 'success', 'success', 'other', 'success', 'success', 'other']
success rate: 5/10
total data points collected: 2516
average episode length: 251.6
average episode reward: 0.395
termination reasons: ['success', 'other', 'success', 'success', 'other', 'success', 'success', 'other', 'success']
success rate: 6/10
total data points collected: 3017
average episode length: 301.7
average episode reward: 0.351
termination reasons: ['success', 'other', 'success', 'success', 'other', 'success', 'success', 'other', 'success', 'other']
success rate: 6/10
tensor shapes - states: torch.Size([3017, 56]), actions: torch.Size([3017, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4578', '7.3447', '8.3567', '7.3963', '7.3306']
Training reward models...
Reward model losses: ['0.1261', '0.0766', '0.0544', '0.3345', '0.1984']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4926', '1.4158', '1.3596', '1.4830', '1.5776']
Training action VAE models...
Action VAE losses: ['1.4831', '1.4249', '1.4275', '1.4818', '1.4880']
CM score components:
transition disagreement: 0.3766
reward disagreement: 0.1347
state disagreement: 0.5085
action disagreement: 0.5841
total CM score: 1.6039
2025-07-21 15:59:28,561 3247582 INFO
Testing intervention 7/7: random (CM score)
IntervenedCausalWorld created with random intervention
evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 225 steps, reward: 4.185
total data points collected: 225
average episode length: 22.5
average episode reward: 0.419
termination reasons: ['success']
success rate: 1/10
Reset #2: random intervention applied (success: True)
episode 2: 46 steps, reward: 1.844
total data points collected: 271
average episode length: 27.1
average episode reward: 0.603
termination reasons: ['success', 'success']
success rate: 2/10
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.957
total data points collected: 772
average episode length: 77.2
average episode reward: 0.507
termination reasons: ['success', 'success', 'other']
success rate: 2/10
total data points collected: 1273
average episode length: 127.3
average episode reward: 0.240
termination reasons: ['success', 'success', 'other', 'other']
success rate: 2/10
total data points collected: 1774
average episode length: 177.4
average episode reward: 0.578
termination reasons: ['success', 'success', 'other', 'other', 'other']
success rate: 2/10
total data points collected: 2275
average episode length: 227.5
average episode reward: 0.730
termination reasons: ['success', 'success', 'other', 'other', 'other', 'other']
success rate: 2/10
total data points collected: 2776
average episode length: 277.6
average episode reward: 0.988
termination reasons: ['success', 'success', 'other', 'other', 'other', 'other', 'other']
success rate: 2/10
total data points collected: 3038
average episode length: 303.8
average episode reward: 1.275
termination reasons: ['success', 'success', 'other', 'other', 'other', 'other', 'other', 'success']
success rate: 3/10
total data points collected: 3539
average episode length: 353.9
average episode reward: 1.057
termination reasons: ['success', 'success', 'other', 'other', 'other', 'other', 'other', 'success', 'other']
success rate: 3/10
total data points collected: 3577
average episode length: 357.7
average episode reward: 1.048
termination reasons: ['success', 'success', 'other', 'other', 'other', 'other', 'other', 'success', 'other', 'success']
success rate: 4/10
tensor shapes - states: torch.Size([3577, 56]), actions: torch.Size([3577, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1869', '7.1633', '7.7889', '7.8317', '8.8476']
Training reward models...
Reward model losses: ['0.2678', '0.0461', '0.3041', '0.0354', '0.0399']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2985', '1.3248', '1.3389', '1.2574', '1.2420']
Training action VAE models...
Action VAE losses: ['1.4852', '1.5103', '1.3810', '1.4729', '1.4495']
CM score components:
transition disagreement: 0.4526
reward disagreement: 0.1635
state disagreement: 0.4906
action disagreement: 0.5830
total CM score: 1.6898
2025-07-21 15:59:36,675 3247582 INFO Best intervention for stage 1: position (CM score: 2.2792)
2025-07-21 15:59:36,675 3247582 INFO === stage 1/7: training on position intervention ===
Logging to highest_reward_sequencing_logs/sb3_csv_logs_1_position
IntervenedCausalWorld created with position intervention
Reset #1: position intervention applied (success: True)
Reset #2: position intervention applied (success: True)
Reset #3: position intervention applied (success: True)
------------------------------------
| custom/              |           |
|    intervention_type | position  |
|    stage             | 1         |
| rollout/             |           |
|    ep_len_mean       | 268       |
|    ep_rew_mean       | 2.0396163 |
| time/                |           |
|    fps               | 449       |
|    iterations        | 1         |
|    time_elapsed      | 9         |
|    total_timesteps   | 5050368   |
------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.9497699   |
| time/                   |             |
|    fps                  | 276         |
|    iterations           | 2           |
|    time_elapsed         | 29          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.044647623 |
|    clip_fraction        | 0.493       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.108      |
|    std                  | 2.67        |
|    value_loss           | 0.0252      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 1.6769154   |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 3           |
|    time_elapsed         | 50          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.037631687 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.534      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.0902     |
|    std                  | 2.7         |
|    value_loss           | 0.0208      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 1.5872107   |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 4           |
|    time_elapsed         | 71          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.065796316 |
|    clip_fraction        | 0.565       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.553      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.105      |
|    std                  | 2.71        |
|    value_loss           | 0.0299      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 339         |
|    ep_rew_mean          | 1.3940446   |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 5           |
|    time_elapsed         | 91          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.043427818 |
|    clip_fraction        | 0.434       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.547      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0917     |
|    std                  | 2.73        |
|    value_loss           | 0.0314      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 1.2106693   |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 6           |
|    time_elapsed         | 112         |
|    total_timesteps      | 5070848     |
| train/                  |             |
|    approx_kl            | 0.046077747 |
|    clip_fraction        | 0.47        |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.541      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 2.75        |
|    value_loss           | 0.0462      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 1.0472485   |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 7           |
|    time_elapsed         | 132         |
|    total_timesteps      | 5074944     |
| train/                  |             |
|    approx_kl            | 0.052667886 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.546      |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.102      |
|    std                  | 2.78        |
|    value_loss           | 0.0315      |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 0.85194606  |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 8           |
|    time_elapsed         | 153         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.061902195 |
|    clip_fraction        | 0.549       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.56       |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.106      |
|    std                  | 2.81        |
|    value_loss           | 0.0199      |
-----------------------------------------
---------------------------------------
| custom/                 |           |
|    intervention_type    | position  |
|    stage                | 1         |
| rollout/                |           |
|    ep_len_mean          | 412       |
|    ep_rew_mean          | 0.7065226 |
| time/                   |           |
|    fps                  | 211       |
|    iterations           | 9         |
|    time_elapsed         | 174       |
|    total_timesteps      | 5083136   |
| train/                  |           |
|    approx_kl            | 0.3833355 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.2       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.558    |
|    n_updates            | 1275      |
|    policy_gradient_loss | -0.11     |
|    std                  | 2.83      |
|    value_loss           | 0.0188    |
---------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | 0.3684297  |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 10         |
|    time_elapsed         | 195        |
|    total_timesteps      | 5087232    |
| train/                  |            |
|    approx_kl            | 0.04994037 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00025    |
|    loss                 | -0.565     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.11      |
|    std                  | 2.85       |
|    value_loss           | 0.0199     |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 448        |
|    ep_rew_mean          | 0.24491644 |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 11         |
|    time_elapsed         | 215        |
|    total_timesteps      | 5091328    |
| train/                  |            |
|    approx_kl            | 0.06606398 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.557     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.104     |
|    std                  | 2.9        |
|    value_loss           | 0.028      |
----------------------------------------
----------------------------------------
| custom/                 |            |
|    intervention_type    | position   |
|    stage                | 1          |
| rollout/                |            |
|    ep_len_mean          | 442        |
|    ep_rew_mean          | 0.30077672 |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 12         |
|    time_elapsed         | 236        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.06117303 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.578     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.115     |
|    std                  | 2.94       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    intervention_type    | position    |
|    stage                | 1           |
| rollout/                |             |
|    ep_len_mean          | 436         |
|    ep_rew_mean          | 0.3670481   |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 13          |
|    time_elapsed         | 257         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.078939214 |
|    clip_fraction        | 0.624       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.583      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.115      |
|    std                  | 2.99        |
|    value_loss           | 0.0146      |
-----------------------------------------
2025-07-21 16:04:05,496 3247582 INFO model saved to highest_reward_sequencing_logs/model_stage_1_position
2025-07-21 16:04:05,498 3247582 INFO
Completed stage 1. Intervention 'position' removed from list.
2025-07-21 16:04:05,498 3247582 INFO Remaining interventions: 6
Traceback (most recent call last):
  File "baselines.py", line 1356, in <module>
    main()
  File "baselines.py", line 1214, in main
    f'stage_{stage}_test_reward': test_metrics['avg_reward'] if test_metrics else None,
KeyError: 'avg_reward'
