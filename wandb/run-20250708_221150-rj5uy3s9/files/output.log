[AutoCaLC-CPDRL] Evaluating intervention: goal
autocalc_cpdrl.py:126: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
  Reward: -0.2112, CM: 2.4740
[AutoCaLC-CPDRL] Evaluating intervention: mass
  Reward: 0.1259, CM: 2.2016
[AutoCaLC-CPDRL] Evaluating intervention: friction
  Reward: 0.3607, CM: 2.4843
[AutoCaLC-CPDRL] Evaluating intervention: visual
  Reward: -0.0737, CM: 2.5686
[AutoCaLC-CPDRL] Evaluating intervention: joints
WARNING:root:Applying intervention lead to infeasibility of the robot
WARNING:root:Invalid Intervention was just executed!
  Reward: -0.5154, CM: 2.4117
[AutoCaLC-CPDRL] Evaluating intervention: pose
  Reward: 0.3580, CM: 2.2908
[AutoCaLC-CPDRL] Evaluating intervention: random
  Reward: 0.0304, CM: 2.4086

[AutoCaLC-CPDRL] Adaptive weights: w_p=0.514, w_cm=0.486

[AutoCaLC-CPDRL] Intervention ranking (adaptive weights):
  1. friction | Score: 1.3920 | Reward: 0.3607 | CM: 2.4843
  2. pose | Score: 1.2966 | Reward: 0.3580 | CM: 2.2908
  3. visual | Score: 1.2095 | Reward: -0.0737 | CM: 2.5686
  4. random | Score: 1.1854 | Reward: 0.0304 | CM: 2.4086
  5. mass | Score: 1.1340 | Reward: 0.1259 | CM: 2.2016
  6. goal | Score: 1.0929 | Reward: -0.2112 | CM: 2.4740
  7. joints | Score: 0.9061 | Reward: -0.5154 | CM: 2.4117

[AutoCaLC-CPDRL] === Intervention 'friction' just got applied (rank 1) ===
Logging to autocalc_cpdrl_logs/PPO_3
[AutoCaLC-CPDRL] Episode 1: reward=0.0505 (Intervention: friction, Rank: 1)
[34m[1mwandb[0m: [33mWARNING[0m Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
[AutoCaLC-CPDRL] Episode 2: reward=-0.0218 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 3: reward=0.0662 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 4: reward=0.0687 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 5: reward=0.0619 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 6: reward=0.0469 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 7: reward=0.0784 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 8: reward=0.0179 (Intervention: friction, Rank: 1)
-----------------------------
| time/              |      |
|    fps             | 317  |
|    iterations      | 1    |
|    time_elapsed    | 6    |
|    total_timesteps | 2048 |
-----------------------------
[AutoCaLC-CPDRL] Episode 9: reward=0.0804 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 10: reward=0.0133 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 11: reward=0.0484 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 12: reward=0.0030 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 13: reward=-0.0081 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 14: reward=0.0579 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 15: reward=0.0842 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 16: reward=-0.0493 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 2           |
|    time_elapsed         | 19          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.037801716 |
|    clip_fraction        | 0.482       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.554      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.116      |
|    std                  | 2.66        |
|    value_loss           | 0.0426      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 17: reward=0.1087 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 18: reward=0.0382 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 19: reward=0.0034 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 20: reward=0.0496 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 21: reward=0.1006 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 22: reward=0.0472 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 23: reward=-0.0127 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 24: reward=-0.0063 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 3           |
|    time_elapsed         | 32          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.047504127 |
|    clip_fraction        | 0.537       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.556      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.113      |
|    std                  | 2.67        |
|    value_loss           | 0.0218      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 25: reward=0.0169 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 26: reward=0.0620 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 27: reward=0.0774 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 28: reward=0.1118 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 29: reward=0.0863 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 30: reward=-0.0146 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 31: reward=-0.0069 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 32: reward=-0.0523 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 181         |
|    iterations           | 4           |
|    time_elapsed         | 45          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.043570876 |
|    clip_fraction        | 0.531       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.533      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.109      |
|    std                  | 2.69        |
|    value_loss           | 0.035       |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 33: reward=-0.0239 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 34: reward=0.0228 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 35: reward=0.0191 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 36: reward=-0.0259 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 37: reward=0.0306 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 38: reward=0.0196 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 39: reward=0.0158 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 40: reward=-0.0050 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 174         |
|    iterations           | 5           |
|    time_elapsed         | 58          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.049657904 |
|    clip_fraction        | 0.512       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.525      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.107      |
|    std                  | 2.71        |
|    value_loss           | 0.0431      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 41: reward=-0.0123 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 42: reward=0.0128 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 43: reward=0.0873 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 44: reward=0.0877 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 45: reward=0.0573 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 46: reward=-0.0235 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 47: reward=0.0142 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 48: reward=0.0017 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 6           |
|    time_elapsed         | 71          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.042623565 |
|    clip_fraction        | 0.491       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.545      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0982     |
|    std                  | 2.75        |
|    value_loss           | 0.0266      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 49: reward=-0.0234 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 50: reward=-0.0628 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 51: reward=0.0426 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 52: reward=0.0239 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 53: reward=0.0423 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 54: reward=0.0462 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 55: reward=-0.0636 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 56: reward=0.0517 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 57: reward=0.0403 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 7           |
|    time_elapsed         | 86          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.037964568 |
|    clip_fraction        | 0.453       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.512      |
|    n_updates            | 1245        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 2.77        |
|    value_loss           | 0.0601      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 58: reward=-0.0368 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 59: reward=0.0310 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 60: reward=0.0400 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 61: reward=0.0159 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 62: reward=-0.0601 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 63: reward=0.0181 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 64: reward=-0.0626 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 65: reward=0.0130 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 8           |
|    time_elapsed         | 100         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.050668206 |
|    clip_fraction        | 0.504       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.542      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.107      |
|    std                  | 2.8         |
|    value_loss           | 0.0406      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 66: reward=-0.0356 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 67: reward=0.0384 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 68: reward=0.0427 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 69: reward=0.0434 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 70: reward=-0.1096 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 71: reward=0.0498 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 72: reward=-0.0360 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 73: reward=-0.0905 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 163        |
|    iterations           | 9          |
|    time_elapsed         | 112        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.05970875 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.567     |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.111     |
|    std                  | 2.81       |
|    value_loss           | 0.0214     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 74: reward=-0.0018 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 75: reward=-0.0486 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 76: reward=0.0320 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 77: reward=-0.0375 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 78: reward=0.0745 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 79: reward=0.0707 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 80: reward=-0.0984 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 81: reward=-0.0085 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 161        |
|    iterations           | 10         |
|    time_elapsed         | 126        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.04228936 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.559     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.83       |
|    value_loss           | 0.0222     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 82: reward=0.0952 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 83: reward=-0.0044 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 84: reward=-0.0112 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 85: reward=-0.0026 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 86: reward=0.0136 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 87: reward=-0.0958 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 88: reward=-0.1131 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 89: reward=0.0355 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 160        |
|    iterations           | 11         |
|    time_elapsed         | 140        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05764263 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.533     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.086     |
|    std                  | 2.87       |
|    value_loss           | 0.0518     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 90: reward=-0.0962 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 91: reward=0.0243 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 92: reward=-0.0116 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 93: reward=-0.1252 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 94: reward=0.0008 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 95: reward=0.0387 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 96: reward=0.0524 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 97: reward=-0.0394 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 12         |
|    time_elapsed         | 153        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04765893 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.552     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.88       |
|    value_loss           | 0.0274     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 98: reward=-0.0431 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 99: reward=-0.0256 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 100: reward=0.0348 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 101: reward=-0.1070 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 102: reward=0.0592 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 103: reward=-0.0440 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 104: reward=-0.0603 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 105: reward=0.0192 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 106: reward=-0.0471 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 159         |
|    iterations           | 13          |
|    time_elapsed         | 166         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.048455507 |
|    clip_fraction        | 0.53        |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.567      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.106      |
|    std                  | 2.93        |
|    value_loss           | 0.0275      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 107: reward=0.0313 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 108: reward=-0.0031 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 109: reward=-0.0097 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 110: reward=-0.0071 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 111: reward=-0.0664 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 112: reward=0.0417 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 113: reward=0.0119 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 114: reward=0.0812 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 14         |
|    time_elapsed         | 179        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06950668 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.562     |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.105     |
|    std                  | 2.95       |
|    value_loss           | 0.0549     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 115: reward=0.0283 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 116: reward=0.1282 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 117: reward=0.0080 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 118: reward=-0.0436 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 119: reward=0.0217 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 120: reward=-0.0663 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 121: reward=-0.0078 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 122: reward=-0.0151 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 15         |
|    time_elapsed         | 192        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.05555522 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.572     |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.101     |
|    std                  | 3          |
|    value_loss           | 0.0195     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 123: reward=-0.0053 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 124: reward=0.0112 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 125: reward=0.0088 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 126: reward=-0.1060 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 127: reward=-0.0657 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 128: reward=-0.0042 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 129: reward=-0.0290 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 130: reward=-0.0177 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 159         |
|    iterations           | 16          |
|    time_elapsed         | 206         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.048671223 |
|    clip_fraction        | 0.542       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.566      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.106      |
|    std                  | 3.03        |
|    value_loss           | 0.0279      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 131: reward=0.0576 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 132: reward=0.0231 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 133: reward=-0.0114 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 134: reward=-0.0168 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 135: reward=-0.0109 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 136: reward=-0.0155 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 137: reward=-0.0632 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 138: reward=-0.0576 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 17         |
|    time_elapsed         | 219        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07863188 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.55      |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.101     |
|    std                  | 3.04       |
|    value_loss           | 0.0552     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 139: reward=0.0390 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 140: reward=-0.0221 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 141: reward=0.0292 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 142: reward=0.0300 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 143: reward=0.0637 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 144: reward=-0.0199 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 145: reward=-0.0340 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 146: reward=0.0182 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 18          |
|    time_elapsed         | 232         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.050403178 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.9       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.562      |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.101      |
|    std                  | 3.1         |
|    value_loss           | 0.0419      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 147: reward=0.0323 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 148: reward=0.0759 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 149: reward=0.0074 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 150: reward=0.0375 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 151: reward=-0.0332 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 152: reward=0.0330 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 153: reward=0.0278 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 154: reward=0.0699 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 155: reward=0.0855 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 19         |
|    time_elapsed         | 245        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.05946914 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.57      |
|    n_updates            | 1425       |
|    policy_gradient_loss | -0.107     |
|    std                  | 3.14       |
|    value_loss           | 0.039      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 156: reward=-0.0048 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 157: reward=0.0278 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 158: reward=0.0094 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 159: reward=-0.0044 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 160: reward=-0.0347 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 161: reward=-0.0835 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 162: reward=0.0475 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 163: reward=0.0747 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 20         |
|    time_elapsed         | 258        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.06797266 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.583     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.111     |
|    std                  | 3.2        |
|    value_loss           | 0.0329     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 164: reward=-0.1041 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 165: reward=0.0031 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 166: reward=0.0153 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 167: reward=-0.0705 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 168: reward=-0.0613 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 169: reward=0.0157 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 170: reward=-0.0795 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 171: reward=-0.0045 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 21         |
|    time_elapsed         | 271        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06874631 |
|    clip_fraction        | 0.639      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.3      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.575     |
|    n_updates            | 1455       |
|    policy_gradient_loss | -0.112     |
|    std                  | 3.26       |
|    value_loss           | 0.0398     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 172: reward=-0.0491 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 173: reward=0.0164 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 174: reward=0.0252 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 175: reward=0.0373 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 176: reward=0.0360 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 177: reward=-0.0661 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 178: reward=-0.0354 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 179: reward=0.0654 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 22         |
|    time_elapsed         | 285        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.05217287 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.4      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.576     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.112     |
|    std                  | 3.27       |
|    value_loss           | 0.0434     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 180: reward=0.0126 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 181: reward=0.0341 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 182: reward=-0.1334 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 183: reward=0.0573 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 184: reward=0.0039 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 185: reward=0.0019 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 186: reward=-0.0053 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 187: reward=-0.0772 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 23         |
|    time_elapsed         | 298        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06626767 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.561     |
|    n_updates            | 1485       |
|    policy_gradient_loss | -0.0941    |
|    std                  | 3.34       |
|    value_loss           | 0.0475     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 188: reward=0.0238 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 189: reward=0.0352 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 190: reward=-0.0560 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 191: reward=-0.0156 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 192: reward=-0.0661 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 193: reward=0.0006 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 194: reward=0.0243 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 195: reward=0.0280 (Intervention: friction, Rank: 1)
--------------------------------------
| time/                   |          |
|    fps                  | 156      |
|    iterations           | 24       |
|    time_elapsed         | 313      |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.066824 |
|    clip_fraction        | 0.587    |
|    clip_range           | 0.2      |
|    entropy_loss         | -23.7    |
|    explained_variance   | 0.512    |
|    learning_rate        | 0.00025  |
|    loss                 | -0.584   |
|    n_updates            | 1500     |
|    policy_gradient_loss | -0.11    |
|    std                  | 3.4      |
|    value_loss           | 0.0472   |
--------------------------------------
[AutoCaLC-CPDRL] Episode 196: reward=-0.0458 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 197: reward=0.0685 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 198: reward=-0.0405 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 199: reward=-0.0959 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 200: reward=0.0266 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 201: reward=-0.0018 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 202: reward=-0.0167 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 203: reward=0.0827 (Intervention: friction, Rank: 1)
---------------------------------------
| time/                   |           |
|    fps                  | 156       |
|    iterations           | 25        |
|    time_elapsed         | 326       |
|    total_timesteps      | 51200     |
| train/                  |           |
|    approx_kl            | 0.0606414 |
|    clip_fraction        | 0.572     |
|    clip_range           | 0.2       |
|    entropy_loss         | -23.9     |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.591    |
|    n_updates            | 1515      |
|    policy_gradient_loss | -0.106    |
|    std                  | 3.5       |
|    value_loss           | 0.0304    |
---------------------------------------
[AutoCaLC-CPDRL] Episode 204: reward=0.0811 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 205: reward=0.0966 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 206: reward=0.0659 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 207: reward=0.0242 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 208: reward=-0.0564 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 209: reward=0.0214 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 210: reward=-0.0074 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 211: reward=-0.0233 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 212: reward=-0.0051 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 26          |
|    time_elapsed         | 339         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.048922792 |
|    clip_fraction        | 0.489       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.569      |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.088      |
|    std                  | 3.55        |
|    value_loss           | 0.0405      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 213: reward=0.0486 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 214: reward=-0.0448 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 215: reward=0.0319 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 216: reward=-0.0084 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 217: reward=-0.0155 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 218: reward=-0.0428 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 219: reward=0.0503 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 220: reward=-0.0329 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 27         |
|    time_elapsed         | 352        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.26826736 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.582     |
|    n_updates            | 1545       |
|    policy_gradient_loss | -0.111     |
|    std                  | 3.6        |
|    value_loss           | 0.0695     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 221: reward=0.0084 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 222: reward=-0.0421 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 223: reward=0.0336 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 224: reward=0.0544 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 225: reward=0.0258 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 226: reward=-0.0369 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 227: reward=-0.0070 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 228: reward=-0.0462 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 28         |
|    time_elapsed         | 365        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.04843151 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.4      |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.572     |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 3.65       |
|    value_loss           | 0.068      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 229: reward=-0.0207 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 230: reward=0.0159 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 231: reward=0.1018 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 232: reward=0.0062 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 233: reward=0.0187 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 234: reward=-0.0392 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 235: reward=0.0158 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 236: reward=-0.0021 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 29          |
|    time_elapsed         | 378         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.059620287 |
|    clip_fraction        | 0.571       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.605      |
|    n_updates            | 1575        |
|    policy_gradient_loss | -0.109      |
|    std                  | 3.7         |
|    value_loss           | 0.0336      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 237: reward=0.0183 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 238: reward=0.0397 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 239: reward=-0.0105 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 240: reward=-0.0641 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 241: reward=0.0396 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 242: reward=-0.0492 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 243: reward=0.0238 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 244: reward=-0.0401 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 30          |
|    time_elapsed         | 391         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.081700295 |
|    clip_fraction        | 0.581       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.577      |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0815     |
|    std                  | 3.8         |
|    value_loss           | 0.0709      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 245: reward=-0.0968 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 246: reward=-0.0401 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 247: reward=-0.0060 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 248: reward=0.0064 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 249: reward=0.0218 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 250: reward=0.0410 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 251: reward=0.0731 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 252: reward=-0.0070 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 31          |
|    time_elapsed         | 405         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.052215613 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.59       |
|    n_updates            | 1605        |
|    policy_gradient_loss | -0.0923     |
|    std                  | 3.86        |
|    value_loss           | 0.0415      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 253: reward=0.0281 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 254: reward=0.0219 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 255: reward=0.0503 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 256: reward=-0.0766 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 257: reward=0.0457 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 258: reward=0.0482 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 259: reward=-0.0203 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 260: reward=0.0894 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 261: reward=-0.0160 (Intervention: friction, Rank: 1)
---------------------------------------
| time/                   |           |
|    fps                  | 156       |
|    iterations           | 32        |
|    time_elapsed         | 419       |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0582041 |
|    clip_fraction        | 0.576     |
|    clip_range           | 0.2       |
|    entropy_loss         | -25.1     |
|    explained_variance   | 0.475     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.617    |
|    n_updates            | 1620      |
|    policy_gradient_loss | -0.112    |
|    std                  | 3.95      |
|    value_loss           | 0.0408    |
---------------------------------------
[AutoCaLC-CPDRL] Episode 262: reward=-0.0671 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 263: reward=-0.0532 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 264: reward=-0.0143 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 265: reward=-0.0880 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 266: reward=0.0072 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 267: reward=0.0126 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 268: reward=0.0314 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 269: reward=0.0463 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 33         |
|    time_elapsed         | 431        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.06784485 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.2      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.617     |
|    n_updates            | 1635       |
|    policy_gradient_loss | -0.111     |
|    std                  | 4.02       |
|    value_loss           | 0.0401     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 270: reward=0.0098 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 271: reward=-0.0340 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 272: reward=-0.0216 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 273: reward=-0.0738 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 274: reward=0.0318 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 275: reward=0.0193 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 276: reward=-0.0684 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 277: reward=-0.0173 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 34         |
|    time_elapsed         | 445        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.05876916 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.629     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.112     |
|    std                  | 4.06       |
|    value_loss           | 0.033      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 278: reward=-0.0274 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 279: reward=0.0708 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 280: reward=0.0129 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 281: reward=-0.0054 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 282: reward=-0.0204 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 283: reward=-0.0058 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 284: reward=-0.0032 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 285: reward=-0.0585 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 35          |
|    time_elapsed         | 458         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.058369104 |
|    clip_fraction        | 0.552       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.4       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.61       |
|    n_updates            | 1665        |
|    policy_gradient_loss | -0.101      |
|    std                  | 4.1         |
|    value_loss           | 0.0481      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 286: reward=0.0034 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 287: reward=0.0573 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 288: reward=-0.0969 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 289: reward=-0.0750 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 290: reward=0.0055 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 291: reward=-0.0048 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 292: reward=-0.0346 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 293: reward=-0.0004 (Intervention: friction, Rank: 1)
---------------------------------------
| time/                   |           |
|    fps                  | 156       |
|    iterations           | 36        |
|    time_elapsed         | 470       |
|    total_timesteps      | 73728     |
| train/                  |           |
|    approx_kl            | 0.0642003 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -25.5     |
|    explained_variance   | 0.684     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.601    |
|    n_updates            | 1680      |
|    policy_gradient_loss | -0.1      |
|    std                  | 4.16      |
|    value_loss           | 0.0615    |
---------------------------------------
[AutoCaLC-CPDRL] Episode 294: reward=0.0114 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 295: reward=0.1027 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 296: reward=-0.0572 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 297: reward=-1.2029 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 298: reward=0.0234 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 299: reward=0.0202 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 300: reward=0.0819 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 301: reward=0.0224 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 37         |
|    time_elapsed         | 483        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.08139944 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.7      |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.638     |
|    n_updates            | 1695       |
|    policy_gradient_loss | -0.114     |
|    std                  | 4.24       |
|    value_loss           | 0.0367     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 302: reward=0.0713 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 303: reward=0.0259 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 304: reward=-0.0124 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 305: reward=-0.1079 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 306: reward=-0.0062 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 307: reward=-0.0165 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 308: reward=0.0469 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 309: reward=0.0046 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 310: reward=0.0379 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 38         |
|    time_elapsed         | 496        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07832567 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.8      |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.619     |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.105     |
|    std                  | 4.32       |
|    value_loss           | 0.0458     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 311: reward=0.0602 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 312: reward=0.0747 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 313: reward=0.0235 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 314: reward=0.0592 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 315: reward=0.0808 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 316: reward=0.0528 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 317: reward=-0.0663 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 318: reward=-0.0174 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 39         |
|    time_elapsed         | 509        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.07415526 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -26        |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.617     |
|    n_updates            | 1725       |
|    policy_gradient_loss | -0.105     |
|    std                  | 4.37       |
|    value_loss           | 0.0598     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 319: reward=0.0762 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 320: reward=-0.0348 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 321: reward=0.0509 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 322: reward=0.0136 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 323: reward=0.0859 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 324: reward=0.0400 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 325: reward=-0.0524 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 326: reward=0.0188 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 40          |
|    time_elapsed         | 522         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.061313774 |
|    clip_fraction        | 0.552       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.1       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.634      |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.105      |
|    std                  | 4.45        |
|    value_loss           | 0.0308      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 327: reward=0.0246 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 328: reward=0.0281 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 329: reward=0.0324 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 330: reward=0.1304 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 331: reward=-0.0228 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 332: reward=-0.0248 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 333: reward=0.0456 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 334: reward=-0.0541 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 41          |
|    time_elapsed         | 535         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.052967522 |
|    clip_fraction        | 0.548       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.3       |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.635      |
|    n_updates            | 1755        |
|    policy_gradient_loss | -0.106      |
|    std                  | 4.54        |
|    value_loss           | 0.0403      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 335: reward=0.0159 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 336: reward=-0.0173 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 337: reward=0.0001 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 338: reward=-0.1200 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 339: reward=-0.0144 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 340: reward=0.0280 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 341: reward=0.0445 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 342: reward=0.0371 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 42          |
|    time_elapsed         | 549         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.053685248 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.4       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.618      |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0997     |
|    std                  | 4.61        |
|    value_loss           | 0.0505      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 343: reward=0.0165 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 344: reward=-0.0165 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 345: reward=0.1216 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 346: reward=-0.0064 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 347: reward=0.0563 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 348: reward=0.0170 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 349: reward=-0.0307 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 350: reward=0.0572 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 43         |
|    time_elapsed         | 562        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.05441601 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.6      |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.642     |
|    n_updates            | 1785       |
|    policy_gradient_loss | -0.101     |
|    std                  | 4.68       |
|    value_loss           | 0.032      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 351: reward=0.0180 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 352: reward=-0.0778 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 353: reward=0.0966 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 354: reward=0.0358 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 355: reward=0.0493 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 356: reward=-0.0209 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 357: reward=0.0288 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 358: reward=0.0261 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 359: reward=0.0042 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 44         |
|    time_elapsed         | 575        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.07520168 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.6      |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.655     |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.114     |
|    std                  | 4.7        |
|    value_loss           | 0.045      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 360: reward=0.0099 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 361: reward=-0.0714 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 362: reward=0.0458 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 363: reward=0.0509 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 364: reward=0.0142 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 365: reward=-0.1732 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 366: reward=-0.0440 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 367: reward=0.0513 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 45          |
|    time_elapsed         | 589         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.062369067 |
|    clip_fraction        | 0.571       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.8       |
|    explained_variance   | 0.494       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.642      |
|    n_updates            | 1815        |
|    policy_gradient_loss | -0.105      |
|    std                  | 4.79        |
|    value_loss           | 0.0391      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 368: reward=-0.0299 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 369: reward=0.0347 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 370: reward=0.0529 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 371: reward=0.0231 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 372: reward=-0.2172 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 373: reward=-0.0553 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 374: reward=0.0185 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 375: reward=0.0090 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 46         |
|    time_elapsed         | 601        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.08677598 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.9      |
|    explained_variance   | 0.416      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.684     |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.127     |
|    std                  | 4.88       |
|    value_loss           | 0.0303     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 376: reward=-0.1032 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 377: reward=0.0758 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 378: reward=0.0173 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 379: reward=0.0513 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 380: reward=-0.0268 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 381: reward=-0.0013 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 382: reward=-0.0280 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 383: reward=0.0450 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 47         |
|    time_elapsed         | 614        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.06257975 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.1      |
|    explained_variance   | 0.385      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.641     |
|    n_updates            | 1845       |
|    policy_gradient_loss | -0.0993    |
|    std                  | 4.93       |
|    value_loss           | 0.0677     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 384: reward=-0.0157 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 385: reward=-0.0077 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 386: reward=-0.0252 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 387: reward=0.0441 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 388: reward=0.0438 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 389: reward=-0.0413 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 390: reward=0.0184 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 391: reward=0.0107 (Intervention: friction, Rank: 1)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 48         |
|    time_elapsed         | 626        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.07313208 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.2      |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.654     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.109     |
|    std                  | 5.03       |
|    value_loss           | 0.0492     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 392: reward=-0.0454 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 393: reward=-0.0397 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 394: reward=-0.0041 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 395: reward=-0.0016 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 396: reward=0.0551 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 397: reward=-0.0190 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 398: reward=0.0435 (Intervention: friction, Rank: 1)
[AutoCaLC-CPDRL] Episode 399: reward=-0.0277 (Intervention: friction, Rank: 1)
-----------------------------------------
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 49          |
|    time_elapsed         | 639         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.071229145 |
|    clip_fraction        | 0.599       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.0197      |
|    learning_rate        | 0.00025     |
|    loss                 | -0.647      |
|    n_updates            | 1875        |
|    policy_gradient_loss | -0.102      |
|    std                  | 5.19        |
|    value_loss           | 0.0588      |
-----------------------------------------
[AutoCaLC-CPDRL] Post-training CM score: 2.1582
[AutoCaLC-CPDRL] CM score not yet stabilized. Patience counter reset. Re-training on this intervention.

[AutoCaLC-CPDRL] === Intervention 'pose' just got applied (rank 2) ===
[34m[1mwandb[0m: [33mWARNING[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to autocalc_cpdrl_logs/PPO_4
[AutoCaLC-CPDRL] Episode 1: reward=-0.0085 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 2: reward=0.0623 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 3: reward=0.0285 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 4: reward=0.0068 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 5: reward=0.0947 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 6: reward=-0.0065 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 7: reward=0.0168 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 8: reward=0.0114 (Intervention: pose, Rank: 2)
-----------------------------
| time/              |      |
|    fps             | 286  |
|    iterations      | 1    |
|    time_elapsed    | 7    |
|    total_timesteps | 2048 |
-----------------------------
[AutoCaLC-CPDRL] Episode 9: reward=0.0417 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 10: reward=0.0278 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 11: reward=0.0187 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 12: reward=0.0210 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 13: reward=-0.0096 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 14: reward=-0.0305 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 15: reward=0.0201 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 16: reward=-0.0735 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 195         |
|    iterations           | 2           |
|    time_elapsed         | 20          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.045768134 |
|    clip_fraction        | 0.522       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.654      |
|    n_updates            | 1905        |
|    policy_gradient_loss | -0.103      |
|    std                  | 5.29        |
|    value_loss           | 0.0634      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 17: reward=-0.0304 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 18: reward=-0.0260 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 19: reward=-0.0282 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 20: reward=0.0616 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 21: reward=0.0640 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 22: reward=-0.0110 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 23: reward=0.0019 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 24: reward=-0.0472 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 3           |
|    time_elapsed         | 34          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.074464515 |
|    clip_fraction        | 0.583       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.668      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.107      |
|    std                  | 5.33        |
|    value_loss           | 0.0517      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 25: reward=-0.0394 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 26: reward=-0.0823 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 27: reward=0.0293 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 28: reward=-0.0230 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 29: reward=0.0303 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 30: reward=-0.0036 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 31: reward=-0.0073 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 32: reward=0.0371 (Intervention: pose, Rank: 2)
---------------------------------------
| time/                   |           |
|    fps                  | 169       |
|    iterations           | 4         |
|    time_elapsed         | 48        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0811307 |
|    clip_fraction        | 0.627     |
|    clip_range           | 0.2       |
|    entropy_loss         | -27.9     |
|    explained_variance   | 0.268     |
|    learning_rate        | 0.00025   |
|    loss                 | -0.676    |
|    n_updates            | 1935      |
|    policy_gradient_loss | -0.111    |
|    std                  | 5.48      |
|    value_loss           | 0.0384    |
---------------------------------------
[AutoCaLC-CPDRL] Episode 33: reward=-0.0331 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 34: reward=-0.0111 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 35: reward=-0.1183 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 36: reward=0.0242 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 37: reward=0.0239 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 38: reward=0.0597 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 39: reward=0.0094 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 40: reward=0.0249 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 164        |
|    iterations           | 5          |
|    time_elapsed         | 62         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.06250475 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.1      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.672     |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.104     |
|    std                  | 5.6        |
|    value_loss           | 0.0397     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 41: reward=0.0248 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 42: reward=-0.0228 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 43: reward=0.0450 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 44: reward=-0.0033 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 45: reward=0.0562 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 46: reward=-0.0452 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 47: reward=0.0286 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 48: reward=-0.0166 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 163        |
|    iterations           | 6          |
|    time_elapsed         | 75         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07276856 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.3      |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.679     |
|    n_updates            | 1965       |
|    policy_gradient_loss | -0.111     |
|    std                  | 5.68       |
|    value_loss           | 0.042      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 49: reward=0.0460 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 50: reward=-0.0711 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 51: reward=0.0334 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 52: reward=0.0967 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 53: reward=0.1123 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 54: reward=0.0461 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 55: reward=0.0257 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 56: reward=0.0977 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 57: reward=0.0233 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 162         |
|    iterations           | 7           |
|    time_elapsed         | 88          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.064538516 |
|    clip_fraction        | 0.542       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.658      |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0891     |
|    std                  | 5.77        |
|    value_loss           | 0.05        |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 58: reward=0.0171 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 59: reward=0.0254 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 60: reward=-0.0740 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 61: reward=0.0127 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 62: reward=0.0280 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 63: reward=0.0109 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 64: reward=-0.0516 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 65: reward=0.0008 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 162        |
|    iterations           | 8          |
|    time_elapsed         | 100        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.09296673 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.5      |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.696     |
|    n_updates            | 1995       |
|    policy_gradient_loss | -0.119     |
|    std                  | 5.83       |
|    value_loss           | 0.0477     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 66: reward=-0.0587 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 67: reward=0.0466 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 68: reward=0.1074 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 69: reward=0.0004 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 70: reward=-0.0557 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 71: reward=-0.1278 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 72: reward=-0.0152 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 73: reward=-0.0637 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 160        |
|    iterations           | 9          |
|    time_elapsed         | 114        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06598544 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.7      |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.693     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.113     |
|    std                  | 5.92       |
|    value_loss           | 0.0406     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 74: reward=0.0711 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 75: reward=-0.0110 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 76: reward=-0.0475 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 77: reward=-0.1130 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 78: reward=0.0105 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 79: reward=0.0000 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 80: reward=-0.0736 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 81: reward=-0.0404 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 160        |
|    iterations           | 10         |
|    time_elapsed         | 127        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.06769573 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.8      |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.68      |
|    n_updates            | 2025       |
|    policy_gradient_loss | -0.107     |
|    std                  | 6.02       |
|    value_loss           | 0.0329     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 82: reward=-0.0047 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 83: reward=0.0453 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 84: reward=0.0216 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 85: reward=0.0701 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 86: reward=-0.0309 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 87: reward=-0.0388 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 88: reward=0.0571 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 89: reward=0.0391 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 11         |
|    time_elapsed         | 141        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.06756631 |
|    clip_fraction        | 0.598      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29        |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.695     |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.108     |
|    std                  | 6.15       |
|    value_loss           | 0.0251     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 90: reward=0.0692 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 91: reward=0.0834 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 92: reward=-0.0097 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 93: reward=-0.1062 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 94: reward=0.0447 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 95: reward=0.0367 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 96: reward=0.0130 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 97: reward=0.0097 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 12         |
|    time_elapsed         | 154        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.07635717 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.1      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.685     |
|    n_updates            | 2055       |
|    policy_gradient_loss | -0.104     |
|    std                  | 6.26       |
|    value_loss           | 0.0341     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 98: reward=0.0217 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 99: reward=-0.0274 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 100: reward=0.0319 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 101: reward=-0.0623 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 102: reward=0.0906 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 103: reward=0.0109 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 104: reward=-0.0470 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 105: reward=0.0261 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 106: reward=0.0039 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 13         |
|    time_elapsed         | 167        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.06191369 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.2      |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.684     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.102     |
|    std                  | 6.34       |
|    value_loss           | 0.0233     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 107: reward=0.0398 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 108: reward=0.0923 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 109: reward=0.0513 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 110: reward=-0.0110 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 111: reward=0.0513 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 112: reward=-0.0347 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 113: reward=0.0035 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 114: reward=0.0086 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 14          |
|    time_elapsed         | 181         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.059632268 |
|    clip_fraction        | 0.576       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.702      |
|    n_updates            | 2085        |
|    policy_gradient_loss | -0.111      |
|    std                  | 6.38        |
|    value_loss           | 0.0268      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 115: reward=0.0633 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 116: reward=-0.0092 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 117: reward=0.0413 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 118: reward=-0.1103 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 119: reward=0.0188 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 120: reward=-0.0185 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 121: reward=-0.0413 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 122: reward=-0.0152 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 15          |
|    time_elapsed         | 195         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.098548844 |
|    clip_fraction        | 0.65        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.719      |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.116      |
|    std                  | 6.54        |
|    value_loss           | 0.0361      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 123: reward=0.0043 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 124: reward=0.0795 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 125: reward=0.0016 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 126: reward=-0.0083 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 127: reward=0.0556 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 128: reward=-0.0367 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 129: reward=-0.0237 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 130: reward=0.0585 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 16         |
|    time_elapsed         | 209        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.05622782 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.6      |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.706     |
|    n_updates            | 2115       |
|    policy_gradient_loss | -0.102     |
|    std                  | 6.64       |
|    value_loss           | 0.0301     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 131: reward=-0.0013 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 132: reward=0.0313 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 133: reward=-0.0315 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 134: reward=0.0208 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 135: reward=0.0167 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 136: reward=0.0085 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 137: reward=0.0302 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 138: reward=-0.0426 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 17         |
|    time_elapsed         | 223        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07420994 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.8      |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.713     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.112     |
|    std                  | 6.76       |
|    value_loss           | 0.031      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 139: reward=-0.0716 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 140: reward=-0.0009 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 141: reward=0.0634 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 142: reward=0.0843 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 143: reward=0.0482 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 144: reward=-0.0412 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 145: reward=-0.0524 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 146: reward=0.0691 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 18          |
|    time_elapsed         | 236         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.051105015 |
|    clip_fraction        | 0.552       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.703      |
|    n_updates            | 2145        |
|    policy_gradient_loss | -0.1        |
|    std                  | 6.86        |
|    value_loss           | 0.0334      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 147: reward=0.0201 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 148: reward=-0.0281 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 149: reward=-0.0155 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 150: reward=-0.0098 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 151: reward=0.1221 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 152: reward=-0.0094 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 153: reward=-0.0042 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 154: reward=0.1077 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 155: reward=0.0188 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 19         |
|    time_elapsed         | 249        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04919818 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.708     |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.109     |
|    std                  | 6.96       |
|    value_loss           | 0.0314     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 156: reward=-0.0123 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 157: reward=0.0548 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 158: reward=-0.0263 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 159: reward=0.1505 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 160: reward=0.0169 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 161: reward=-0.0737 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 162: reward=0.0417 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 163: reward=-0.0336 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 20          |
|    time_elapsed         | 262         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.055916257 |
|    clip_fraction        | 0.502       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.2       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.695      |
|    n_updates            | 2175        |
|    policy_gradient_loss | -0.0892     |
|    std                  | 7.03        |
|    value_loss           | 0.0503      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 164: reward=0.0150 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 165: reward=-0.0599 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 166: reward=0.0492 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 167: reward=-0.0672 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 168: reward=-0.0277 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 169: reward=0.0530 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 170: reward=-0.0501 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 171: reward=0.0317 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 21         |
|    time_elapsed         | 276        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06798342 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.3      |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.722     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.103     |
|    std                  | 7.13       |
|    value_loss           | 0.0277     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 172: reward=-0.0533 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 173: reward=-0.0831 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 174: reward=0.0094 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 175: reward=0.0373 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 176: reward=0.0320 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 177: reward=-0.0008 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 178: reward=0.0151 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 179: reward=0.0683 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 22          |
|    time_elapsed         | 290         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.052508406 |
|    clip_fraction        | 0.537       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.723      |
|    n_updates            | 2205        |
|    policy_gradient_loss | -0.104      |
|    std                  | 7.24        |
|    value_loss           | 0.0223      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 180: reward=0.0226 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 181: reward=0.0417 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 182: reward=-0.0362 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 183: reward=0.0510 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 184: reward=0.0741 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 185: reward=0.0571 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 186: reward=-0.0504 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 187: reward=-0.0485 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 23          |
|    time_elapsed         | 304         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.056331046 |
|    clip_fraction        | 0.556       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.732      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.101      |
|    std                  | 7.39        |
|    value_loss           | 0.0284      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 188: reward=-0.0322 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 189: reward=0.0367 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 190: reward=-0.0238 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 191: reward=-0.0565 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 192: reward=-0.0640 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 193: reward=0.0503 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 194: reward=0.0108 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 195: reward=-0.0319 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 24         |
|    time_elapsed         | 318        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06006855 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.7      |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.727     |
|    n_updates            | 2235       |
|    policy_gradient_loss | -0.109     |
|    std                  | 7.52       |
|    value_loss           | 0.0245     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 196: reward=0.1280 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 197: reward=-0.0618 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 198: reward=0.0356 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 199: reward=-0.1302 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 200: reward=0.0050 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 201: reward=0.0572 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 202: reward=0.0445 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 203: reward=0.0781 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 25          |
|    time_elapsed         | 331         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.063158154 |
|    clip_fraction        | 0.569       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.729      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.103      |
|    std                  | 7.6         |
|    value_loss           | 0.0192      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 204: reward=0.0416 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 205: reward=0.0271 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 206: reward=0.0643 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 207: reward=-0.0141 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 208: reward=-0.0453 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 209: reward=0.0732 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 210: reward=-0.0461 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 211: reward=0.0788 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 212: reward=0.0230 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 26         |
|    time_elapsed         | 345        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07106338 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.9      |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.739     |
|    n_updates            | 2265       |
|    policy_gradient_loss | -0.117     |
|    std                  | 7.66       |
|    value_loss           | 0.0197     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 213: reward=-0.0296 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 214: reward=-0.0340 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 215: reward=-0.0222 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 216: reward=0.0299 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 217: reward=-0.0288 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 218: reward=0.1118 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 219: reward=0.0985 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 220: reward=0.0172 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 27         |
|    time_elapsed         | 359        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06971168 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31        |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.73      |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.101     |
|    std                  | 7.73       |
|    value_loss           | 0.0318     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 221: reward=0.0381 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 222: reward=-0.0162 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 223: reward=-0.0853 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 224: reward=0.0419 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 225: reward=0.0415 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 226: reward=-0.0708 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 227: reward=-0.0092 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 228: reward=-0.0462 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 28         |
|    time_elapsed         | 373        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.06442596 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.1      |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.748     |
|    n_updates            | 2295       |
|    policy_gradient_loss | -0.114     |
|    std                  | 7.85       |
|    value_loss           | 0.0385     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 229: reward=-0.0257 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 230: reward=-0.0019 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 231: reward=0.0319 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 232: reward=-0.0036 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 233: reward=0.0153 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 234: reward=-0.0308 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 235: reward=0.0173 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 236: reward=-0.0259 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 29         |
|    time_elapsed         | 386        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.05939546 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.395      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.744     |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.117     |
|    std                  | 7.98       |
|    value_loss           | 0.0347     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 237: reward=0.0604 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 238: reward=-0.0144 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 239: reward=0.1173 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 240: reward=-0.1436 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 241: reward=0.0071 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 242: reward=-0.0445 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 243: reward=0.3426 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 244: reward=-0.0596 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 30         |
|    time_elapsed         | 400        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.06073589 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.3      |
|    explained_variance   | 0.274      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.742     |
|    n_updates            | 2325       |
|    policy_gradient_loss | -0.111     |
|    std                  | 8.03       |
|    value_loss           | 0.0291     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 245: reward=-0.0752 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 246: reward=0.0063 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 247: reward=0.0050 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 248: reward=0.0221 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 249: reward=0.0399 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 250: reward=-0.1034 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 251: reward=0.0357 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 252: reward=0.1119 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 31          |
|    time_elapsed         | 413         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.060843028 |
|    clip_fraction        | 0.565       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.4       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.744      |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.11       |
|    std                  | 8.07        |
|    value_loss           | 0.0405      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 253: reward=0.0699 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 254: reward=0.0445 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 255: reward=0.0329 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 256: reward=-0.0122 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 257: reward=0.0607 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 258: reward=-0.0185 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 259: reward=0.0091 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 260: reward=0.0587 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 261: reward=0.1282 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 32         |
|    time_elapsed         | 427        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.06915668 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.5      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.731     |
|    n_updates            | 2355       |
|    policy_gradient_loss | -0.0938    |
|    std                  | 8.15       |
|    value_loss           | 0.0355     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 262: reward=-0.0182 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 263: reward=-0.0523 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 264: reward=-0.0317 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 265: reward=-0.0760 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 266: reward=-0.0206 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 267: reward=-0.0014 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 268: reward=0.0118 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 269: reward=0.1432 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 33          |
|    time_elapsed         | 440         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.075424016 |
|    clip_fraction        | 0.627       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.6       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.766      |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.121      |
|    std                  | 8.32        |
|    value_loss           | 0.0215      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 270: reward=0.0444 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 271: reward=0.0850 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 272: reward=-0.0260 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 273: reward=-0.0226 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 274: reward=-0.0057 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 275: reward=0.0106 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 276: reward=-0.0248 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 277: reward=-0.0137 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 34          |
|    time_elapsed         | 453         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.048946314 |
|    clip_fraction        | 0.523       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.7       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.744      |
|    n_updates            | 2385        |
|    policy_gradient_loss | -0.101      |
|    std                  | 8.42        |
|    value_loss           | 0.04        |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 278: reward=-0.0064 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 279: reward=-0.0516 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 280: reward=0.0212 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 281: reward=0.0683 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 282: reward=0.0116 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 283: reward=0.0349 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 284: reward=-0.0024 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 285: reward=-0.0468 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 35         |
|    time_elapsed         | 467        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.06691498 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.8      |
|    explained_variance   | 0.476      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.756     |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.109     |
|    std                  | 8.55       |
|    value_loss           | 0.0277     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 286: reward=-0.0137 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 287: reward=0.0708 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 288: reward=0.0360 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 289: reward=0.0182 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 290: reward=-0.0059 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 291: reward=0.0170 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 292: reward=-0.0020 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 293: reward=0.0338 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 36         |
|    time_elapsed         | 480        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.06112402 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.9      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.759     |
|    n_updates            | 2415       |
|    policy_gradient_loss | -0.115     |
|    std                  | 8.65       |
|    value_loss           | 0.0347     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 294: reward=-0.1051 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 295: reward=0.0638 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 296: reward=0.0458 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 297: reward=-0.1290 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 298: reward=0.0284 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 299: reward=0.0027 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 300: reward=-0.0256 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 301: reward=-0.0122 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 37          |
|    time_elapsed         | 492         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.070609376 |
|    clip_fraction        | 0.619       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32         |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.767      |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.118      |
|    std                  | 8.72        |
|    value_loss           | 0.0242      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 302: reward=-0.0048 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 303: reward=-0.0562 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 304: reward=0.0088 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 305: reward=-0.0034 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 306: reward=-0.0042 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 307: reward=0.0300 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 308: reward=-0.0100 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 309: reward=-0.0868 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 310: reward=0.0135 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 38         |
|    time_elapsed         | 505        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.06021659 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.1      |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.734     |
|    n_updates            | 2445       |
|    policy_gradient_loss | -0.0977    |
|    std                  | 8.86       |
|    value_loss           | 0.0768     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 311: reward=0.0267 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 312: reward=0.0393 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 313: reward=0.1128 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 314: reward=0.0373 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 315: reward=0.0508 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 316: reward=0.1430 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 317: reward=0.0401 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 318: reward=-0.0113 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 39         |
|    time_elapsed         | 518        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.05836852 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.3      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.759     |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.11      |
|    std                  | 8.99       |
|    value_loss           | 0.0479     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 319: reward=0.0274 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 320: reward=0.0703 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 321: reward=0.0705 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 322: reward=-0.0010 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 323: reward=0.0221 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 324: reward=-0.0252 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 325: reward=-0.0226 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 326: reward=-0.0367 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 40          |
|    time_elapsed         | 531         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.048357945 |
|    clip_fraction        | 0.528       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.4       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.758      |
|    n_updates            | 2475        |
|    policy_gradient_loss | -0.107      |
|    std                  | 9.12        |
|    value_loss           | 0.0352      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 327: reward=0.0008 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 328: reward=0.0318 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 329: reward=0.0113 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 330: reward=0.0366 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 331: reward=0.1994 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 332: reward=-0.0949 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 333: reward=-0.0058 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 334: reward=-0.0213 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 41         |
|    time_elapsed         | 544        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.06259682 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.5      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.77      |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.112     |
|    std                  | 9.25       |
|    value_loss           | 0.028      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 335: reward=0.0855 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 336: reward=-0.0404 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 337: reward=0.0046 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 338: reward=0.0184 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 339: reward=-0.0166 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 340: reward=-0.0076 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 341: reward=0.0078 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 342: reward=-0.0208 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 42         |
|    time_elapsed         | 557        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.04922681 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.6      |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.738     |
|    n_updates            | 2505       |
|    policy_gradient_loss | -0.101     |
|    std                  | 9.33       |
|    value_loss           | 0.0529     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 343: reward=-0.1069 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 344: reward=-0.0284 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 345: reward=0.1026 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 346: reward=0.0080 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 347: reward=0.0106 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 348: reward=-0.0138 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 349: reward=-0.0067 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 350: reward=0.0242 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 43         |
|    time_elapsed         | 570        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.08160066 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.284      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.763     |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.102     |
|    std                  | 9.5        |
|    value_loss           | 0.0288     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 351: reward=-0.0421 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 352: reward=-0.0372 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 353: reward=0.0502 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 354: reward=0.0674 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 355: reward=0.0183 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 356: reward=0.0592 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 357: reward=0.0117 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 358: reward=-0.0033 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 359: reward=-0.0524 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 44         |
|    time_elapsed         | 583        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.06238908 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.8      |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.784     |
|    n_updates            | 2535       |
|    policy_gradient_loss | -0.11      |
|    std                  | 9.59       |
|    value_loss           | 0.0229     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 360: reward=-0.0478 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 361: reward=0.0491 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 362: reward=0.0748 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 363: reward=0.0391 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 364: reward=0.0844 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 365: reward=0.0188 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 366: reward=0.0449 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 367: reward=0.0562 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 45          |
|    time_elapsed         | 595         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.053436972 |
|    clip_fraction        | 0.511       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33         |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.762      |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.0963     |
|    std                  | 9.75        |
|    value_loss           | 0.0227      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 368: reward=-0.0680 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 369: reward=0.0151 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 370: reward=-0.0560 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 371: reward=-0.0106 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 372: reward=-0.0084 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 373: reward=-0.0646 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 374: reward=-0.0481 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 375: reward=-0.0384 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 46          |
|    time_elapsed         | 608         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.062115967 |
|    clip_fraction        | 0.561       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.1       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.778      |
|    n_updates            | 2565        |
|    policy_gradient_loss | -0.112      |
|    std                  | 9.88        |
|    value_loss           | 0.0377      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 376: reward=0.0073 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 377: reward=0.1321 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 378: reward=-0.0145 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 379: reward=-0.1225 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 380: reward=-0.0381 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 381: reward=-0.0149 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 382: reward=0.0243 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 383: reward=0.0176 (Intervention: pose, Rank: 2)
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 47         |
|    time_elapsed         | 621        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.08150102 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.2      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.785     |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.108     |
|    std                  | 10.1       |
|    value_loss           | 0.0266     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 384: reward=-0.0335 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 385: reward=0.0103 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 386: reward=0.0091 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 387: reward=-0.0362 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 388: reward=0.0463 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 389: reward=0.0520 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 390: reward=0.0341 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 391: reward=-0.0324 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 48          |
|    time_elapsed         | 633         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.073989734 |
|    clip_fraction        | 0.585       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.777      |
|    n_updates            | 2595        |
|    policy_gradient_loss | -0.109      |
|    std                  | 10.2        |
|    value_loss           | 0.0387      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 392: reward=-0.0363 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 393: reward=-0.0332 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 394: reward=-0.0057 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 395: reward=0.0204 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 396: reward=0.1028 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 397: reward=0.0066 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 398: reward=0.0183 (Intervention: pose, Rank: 2)
[AutoCaLC-CPDRL] Episode 399: reward=-0.0560 (Intervention: pose, Rank: 2)
-----------------------------------------
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 49          |
|    time_elapsed         | 646         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.057771027 |
|    clip_fraction        | 0.567       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.5       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.785      |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.108      |
|    std                  | 10.4        |
|    value_loss           | 0.0377      |
-----------------------------------------
[AutoCaLC-CPDRL] Post-training CM score: 2.1612
[AutoCaLC-CPDRL] CM score not yet stabilized. Patience counter reset. Re-training on this intervention.

[AutoCaLC-CPDRL] === Intervention 'visual' just got applied (rank 3) ===
[34m[1mwandb[0m: [33mWARNING[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to autocalc_cpdrl_logs/PPO_5
[AutoCaLC-CPDRL] Episode 1: reward=0.0407 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 2: reward=-0.0047 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 3: reward=0.0004 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 4: reward=0.0821 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 5: reward=0.0090 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 6: reward=0.0480 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 7: reward=0.0497 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 8: reward=-0.0035 (Intervention: visual, Rank: 3)
-----------------------------
| time/              |      |
|    fps             | 287  |
|    iterations      | 1    |
|    time_elapsed    | 7    |
|    total_timesteps | 2048 |
-----------------------------
[AutoCaLC-CPDRL] Episode 9: reward=0.0649 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 10: reward=0.0679 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 11: reward=0.0292 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 12: reward=-0.0784 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 13: reward=0.0135 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 14: reward=0.0425 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 15: reward=0.0606 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 16: reward=0.0299 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 2           |
|    time_elapsed         | 20          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.059118498 |
|    clip_fraction        | 0.553       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.788      |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.102      |
|    std                  | 10.8        |
|    value_loss           | 0.0406      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 17: reward=0.0017 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 18: reward=-0.0551 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 19: reward=-0.0316 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 20: reward=0.0543 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 21: reward=-0.1969 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 22: reward=-0.0583 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 23: reward=-0.0357 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 24: reward=-0.0252 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 3           |
|    time_elapsed         | 32          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.061326366 |
|    clip_fraction        | 0.588       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34         |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.793      |
|    n_updates            | 2655        |
|    policy_gradient_loss | -0.112      |
|    std                  | 10.9        |
|    value_loss           | 0.0394      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 25: reward=-0.0529 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 26: reward=0.0086 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 27: reward=0.0057 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 28: reward=0.0474 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 29: reward=-0.0179 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 30: reward=-0.0070 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 31: reward=0.0030 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 32: reward=-0.0223 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 181         |
|    iterations           | 4           |
|    time_elapsed         | 45          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.064951085 |
|    clip_fraction        | 0.618       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.1       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.808      |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.121      |
|    std                  | 11.1        |
|    value_loss           | 0.0403      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 33: reward=-0.0495 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 34: reward=0.0641 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 35: reward=0.0613 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 36: reward=-0.0294 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 37: reward=0.0105 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 38: reward=-0.0295 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 39: reward=-0.1565 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 40: reward=0.0076 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 176         |
|    iterations           | 5           |
|    time_elapsed         | 57          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.064491674 |
|    clip_fraction        | 0.562       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.2       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.792      |
|    n_updates            | 2685        |
|    policy_gradient_loss | -0.0981     |
|    std                  | 11.3        |
|    value_loss           | 0.0491      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 41: reward=-0.0308 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 42: reward=0.0060 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 43: reward=-0.0456 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 44: reward=0.0546 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 45: reward=0.0767 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 46: reward=-0.0088 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 47: reward=0.0045 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 48: reward=-0.0658 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 176         |
|    iterations           | 6           |
|    time_elapsed         | 69          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.053335086 |
|    clip_fraction        | 0.506       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.774      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.0933     |
|    std                  | 11.5        |
|    value_loss           | 0.0633      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 49: reward=-0.0676 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 50: reward=-0.0540 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 51: reward=-0.0268 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 52: reward=0.0482 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 53: reward=-0.0031 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 54: reward=0.0103 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 55: reward=-0.0859 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 56: reward=-0.1098 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 57: reward=-0.0272 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 7           |
|    time_elapsed         | 82          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.054316062 |
|    clip_fraction        | 0.521       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.4       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.797      |
|    n_updates            | 2715        |
|    policy_gradient_loss | -0.107      |
|    std                  | 11.5        |
|    value_loss           | 0.0471      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 58: reward=-0.0542 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 59: reward=-0.0117 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 60: reward=-0.0224 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 61: reward=0.0433 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 62: reward=-0.0359 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 63: reward=-0.0026 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 64: reward=-0.0273 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 65: reward=-0.0285 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 169        |
|    iterations           | 8          |
|    time_elapsed         | 96         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.07113261 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.5      |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.792     |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0951    |
|    std                  | 11.6       |
|    value_loss           | 0.062      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 66: reward=-0.2003 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 67: reward=0.0736 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 68: reward=-0.0291 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 69: reward=0.0365 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 70: reward=-0.1007 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 71: reward=-0.0357 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 72: reward=-0.0394 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 73: reward=-0.0742 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 9           |
|    time_elapsed         | 110         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.059252203 |
|    clip_fraction        | 0.564       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.6       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.811      |
|    n_updates            | 2745        |
|    policy_gradient_loss | -0.106      |
|    std                  | 11.8        |
|    value_loss           | 0.0259      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 74: reward=-0.0389 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 75: reward=-0.0607 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 76: reward=0.0082 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 77: reward=-0.0020 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 78: reward=0.0418 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 79: reward=-0.0884 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 80: reward=-0.0874 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 81: reward=0.0180 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 10          |
|    time_elapsed         | 123         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.049881563 |
|    clip_fraction        | 0.568       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.7       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.819      |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.116      |
|    std                  | 11.9        |
|    value_loss           | 0.0217      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 82: reward=0.0306 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 83: reward=0.0246 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 84: reward=-0.0144 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 85: reward=-0.0729 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 86: reward=0.0017 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 87: reward=-0.0338 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 88: reward=0.0455 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 89: reward=0.0121 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 11          |
|    time_elapsed         | 134         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.059822183 |
|    clip_fraction        | 0.572       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.7       |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.81       |
|    n_updates            | 2775        |
|    policy_gradient_loss | -0.106      |
|    std                  | 12          |
|    value_loss           | 0.0323      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 90: reward=-0.0526 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 91: reward=0.0151 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 92: reward=-0.0019 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 93: reward=-0.0785 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 94: reward=0.1047 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 95: reward=0.0035 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 96: reward=0.0039 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 97: reward=-0.0203 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 168        |
|    iterations           | 12         |
|    time_elapsed         | 145        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.06372877 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.9      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.833     |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.122     |
|    std                  | 12.2       |
|    value_loss           | 0.0348     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 98: reward=-0.0424 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 99: reward=-0.0117 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 100: reward=0.0341 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 101: reward=-0.0211 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 102: reward=0.0565 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 103: reward=-0.0152 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 104: reward=-0.0187 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 105: reward=0.0700 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 106: reward=-0.0832 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 13          |
|    time_elapsed         | 158         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.057932794 |
|    clip_fraction        | 0.542       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.805      |
|    n_updates            | 2805        |
|    policy_gradient_loss | -0.106      |
|    std                  | 12.3        |
|    value_loss           | 0.0312      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 107: reward=0.0070 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 108: reward=-0.0019 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 109: reward=0.0229 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 110: reward=-0.0629 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 111: reward=-0.0740 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 112: reward=-0.0092 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 113: reward=-0.0945 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 114: reward=0.0800 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 14         |
|    time_elapsed         | 171        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06523368 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.447      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.811     |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.111     |
|    std                  | 12.5       |
|    value_loss           | 0.0351     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 115: reward=0.0741 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 116: reward=0.1058 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 117: reward=-0.0463 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 118: reward=-0.0697 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 119: reward=-0.0124 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 120: reward=-0.0254 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 121: reward=0.0127 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 122: reward=-0.0275 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 15         |
|    time_elapsed         | 183        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06712662 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.2      |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.813     |
|    n_updates            | 2835       |
|    policy_gradient_loss | -0.105     |
|    std                  | 12.7       |
|    value_loss           | 0.0335     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 123: reward=0.0031 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 124: reward=-0.0597 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 125: reward=-0.0173 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 126: reward=-0.1117 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 127: reward=-0.0108 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 128: reward=0.0443 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 129: reward=-0.0407 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 130: reward=-0.1590 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 16         |
|    time_elapsed         | 195        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04681953 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.4      |
|    explained_variance   | 0.449      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.808     |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0947    |
|    std                  | 12.9       |
|    value_loss           | 0.0375     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 131: reward=-0.0353 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 132: reward=0.0332 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 133: reward=0.0286 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 134: reward=0.0026 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 135: reward=-0.0042 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 136: reward=-0.0089 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 137: reward=-0.0488 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 138: reward=-0.0197 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 17          |
|    time_elapsed         | 207         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.054646716 |
|    clip_fraction        | 0.535       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.5       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.825      |
|    n_updates            | 2865        |
|    policy_gradient_loss | -0.103      |
|    std                  | 13.2        |
|    value_loss           | 0.0331      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 139: reward=-0.0408 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 140: reward=-0.1130 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 141: reward=-0.0605 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 142: reward=0.0354 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 143: reward=0.0306 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 144: reward=-0.0308 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 145: reward=-0.0573 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 146: reward=0.0694 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 18          |
|    time_elapsed         | 219         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.049384706 |
|    clip_fraction        | 0.53        |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.811      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.104      |
|    std                  | 13.4        |
|    value_loss           | 0.0304      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 147: reward=0.0357 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 148: reward=0.0161 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 149: reward=-0.0007 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 150: reward=-0.0056 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 151: reward=-0.0046 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 152: reward=0.0466 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 153: reward=0.0146 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 154: reward=0.0009 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 155: reward=-0.0648 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 19         |
|    time_elapsed         | 231        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06610992 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.8      |
|    explained_variance   | 0.374      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.797     |
|    n_updates            | 2895       |
|    policy_gradient_loss | -0.0975    |
|    std                  | 13.6       |
|    value_loss           | 0.0973     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 156: reward=-0.0107 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 157: reward=0.0063 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 158: reward=0.0607 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 159: reward=0.0654 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 160: reward=0.0026 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 161: reward=-0.0482 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 162: reward=-0.0519 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 163: reward=-0.0656 (Intervention: visual, Rank: 3)
--------------------------------------
| time/                   |          |
|    fps                  | 167      |
|    iterations           | 20       |
|    time_elapsed         | 244      |
|    total_timesteps      | 40960    |
| train/                  |          |
|    approx_kl            | 0.061311 |
|    clip_fraction        | 0.536    |
|    clip_range           | 0.2      |
|    entropy_loss         | -35.8    |
|    explained_variance   | 0.124    |
|    learning_rate        | 0.00025  |
|    loss                 | -0.847   |
|    n_updates            | 2910     |
|    policy_gradient_loss | -0.11    |
|    std                  | 13.6     |
|    value_loss           | 0.0322   |
--------------------------------------
[AutoCaLC-CPDRL] Episode 164: reward=-0.0107 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 165: reward=0.0954 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 166: reward=-0.0162 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 167: reward=0.0457 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 168: reward=-0.0359 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 169: reward=0.1048 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 170: reward=-0.0888 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 171: reward=-0.0377 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 21          |
|    time_elapsed         | 255         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.050420687 |
|    clip_fraction        | 0.506       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.9       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.824      |
|    n_updates            | 2925        |
|    policy_gradient_loss | -0.104      |
|    std                  | 13.7        |
|    value_loss           | 0.0491      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 172: reward=-0.0259 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 173: reward=0.0233 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 174: reward=-0.0245 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 175: reward=-0.0039 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 176: reward=0.0395 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 177: reward=-0.0547 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 178: reward=0.0037 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 179: reward=0.0288 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 22          |
|    time_elapsed         | 267         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.055538695 |
|    clip_fraction        | 0.561       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36         |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.83       |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.107      |
|    std                  | 13.9        |
|    value_loss           | 0.0407      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 180: reward=-0.0707 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 181: reward=0.0046 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 182: reward=-0.1442 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 183: reward=0.0297 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 184: reward=0.0115 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 185: reward=-0.0729 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 186: reward=-0.0234 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 187: reward=-0.0308 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 168        |
|    iterations           | 23         |
|    time_elapsed         | 278        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06238722 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.1      |
|    explained_variance   | 0.499      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.836     |
|    n_updates            | 2955       |
|    policy_gradient_loss | -0.106     |
|    std                  | 14         |
|    value_loss           | 0.0506     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 188: reward=-0.0074 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 189: reward=0.0188 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 190: reward=-0.0883 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 191: reward=-0.0006 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 192: reward=-0.0348 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 193: reward=0.0682 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 194: reward=0.0769 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 195: reward=0.0498 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 169        |
|    iterations           | 24         |
|    time_elapsed         | 289        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07353085 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.3      |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.847     |
|    n_updates            | 2970       |
|    policy_gradient_loss | -0.107     |
|    std                  | 14.4       |
|    value_loss           | 0.027      |
----------------------------------------
[AutoCaLC-CPDRL] Episode 196: reward=0.0269 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 197: reward=0.0194 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 198: reward=0.0157 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 199: reward=-0.0970 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 200: reward=0.0889 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 201: reward=0.0087 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 202: reward=-0.0096 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 203: reward=0.0315 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 170         |
|    iterations           | 25          |
|    time_elapsed         | 300         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.053241357 |
|    clip_fraction        | 0.558       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.4       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.841      |
|    n_updates            | 2985        |
|    policy_gradient_loss | -0.107      |
|    std                  | 14.5        |
|    value_loss           | 0.0576      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 204: reward=0.0636 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 205: reward=0.0866 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 206: reward=0.0408 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 207: reward=0.0121 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 208: reward=0.0317 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 209: reward=0.0232 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 210: reward=-0.0142 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 211: reward=0.1268 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 212: reward=0.0441 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 170         |
|    iterations           | 26          |
|    time_elapsed         | 312         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.061150327 |
|    clip_fraction        | 0.521       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.5       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.83       |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.104      |
|    std                  | 14.7        |
|    value_loss           | 0.0612      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 213: reward=-0.0335 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 214: reward=-0.2157 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 215: reward=-0.0369 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 216: reward=0.0092 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 217: reward=0.0371 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 218: reward=0.0234 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 219: reward=0.0022 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 220: reward=-0.0145 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 27          |
|    time_elapsed         | 323         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.047269877 |
|    clip_fraction        | 0.542       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.6       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.844      |
|    n_updates            | 3015        |
|    policy_gradient_loss | -0.103      |
|    std                  | 15          |
|    value_loss           | 0.0518      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 221: reward=-0.0886 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 222: reward=-0.0190 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 223: reward=-0.0107 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 224: reward=-0.0443 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 225: reward=0.0573 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 226: reward=-0.1827 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 227: reward=-0.0569 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 228: reward=-0.0370 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 28          |
|    time_elapsed         | 335         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.072708085 |
|    clip_fraction        | 0.59        |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.861      |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.117      |
|    std                  | 15.2        |
|    value_loss           | 0.0508      |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 229: reward=-0.0091 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 230: reward=0.0641 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 231: reward=0.0202 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 232: reward=0.0942 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 233: reward=-0.0782 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 234: reward=-0.0576 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 235: reward=-0.0452 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 236: reward=0.0177 (Intervention: visual, Rank: 3)
----------------------------------------
| time/                   |            |
|    fps                  | 171        |
|    iterations           | 29         |
|    time_elapsed         | 346        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.06269419 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.8      |
|    explained_variance   | 0.395      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.848     |
|    n_updates            | 3045       |
|    policy_gradient_loss | -0.105     |
|    std                  | 15.4       |
|    value_loss           | 0.0491     |
----------------------------------------
[AutoCaLC-CPDRL] Episode 237: reward=0.0636 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 238: reward=-0.0137 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 239: reward=-0.0572 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 240: reward=-0.0706 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 241: reward=0.0564 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 242: reward=0.0317 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 243: reward=-0.0734 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 244: reward=-0.0123 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 30          |
|    time_elapsed         | 357         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.052137878 |
|    clip_fraction        | 0.529       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37         |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.835      |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.105      |
|    std                  | 15.6        |
|    value_loss           | 0.056       |
-----------------------------------------
[AutoCaLC-CPDRL] Episode 245: reward=-0.0615 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 246: reward=-0.0587 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 247: reward=0.0007 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 248: reward=-0.0277 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 249: reward=-0.0277 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 250: reward=0.0338 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 251: reward=0.0215 (Intervention: visual, Rank: 3)
[AutoCaLC-CPDRL] Episode 252: reward=0.0107 (Intervention: visual, Rank: 3)
-----------------------------------------
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 31          |
|    time_elapsed         | 369         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.054163843 |
|    clip_fraction        | 0.533       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.1       |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.859      |
|    n_updates            | 3075        |
|    policy_gradient_loss | -0.106      |
|    std                  | 15.8        |
|    value_loss           | 0.0343      |
-----------------------------------------
Traceback (most recent call last):
  File "autocalc_cpdrl.py", line 476, in <module>
  File "autocalc_cpdrl.py", line 441, in main
    print(f"[AutoCaLC-CPDRL] PPO training phase {phase} for intervention '{res['type']}'...")
  File "autocalc_cpdrl.py", line 316, in ppo_train_on_intervention
    temp_path = os.path.join(log_dir, 'temp_transfer_model.zip')
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 270, in learn
    self.train()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 199, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/policies.py", line 646, in evaluate_actions
    log_prob = distribution.log_prob(actions)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/distributions.py", line 163, in log_prob
    log_prob = self.distribution.log_prob(actions)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/distributions/normal.py", line 79, in log_prob
    self._validate_sample(value)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/torch/distributions/distribution.py", line 270, in _validate_sample
    if not isinstance(value, torch.Tensor):
KeyboardInterrupt
