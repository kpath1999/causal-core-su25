
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 30
   student training steps: 50000
==================================================
2025-07-16 16:19:46,819 3266759 INFO Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.771, success=True
episode 2: length=27, reward=-0.771, success=True
episode 3: length=27, reward=-0.771, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.771
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 45.214
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 16.304
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 14.194
total data points collected: 40025
average episode length: 8005.0
average episode reward: 18.592
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
meta_teacher_student.py:252: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4530', '7.8095', '7.3438', '7.0046', '7.5950']
Training reward models...
Reward model losses: ['0.3488', '0.1874', '0.0210', '0.9681', '0.0037']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9971', '1.0240', '1.0652', '1.0113', '0.9587']
Training action VAE models...
Action VAE losses: ['1.0915', '1.1447', '1.0703', '1.0606', '0.9321']
CM score components:
transition disagreement: 0.3868
reward disagreement: 0.3493
state disagreement: 0.4907
action disagreement: 0.5201
total CM score: 1.7469
goal is complete. CM score: 1.7469
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 29 steps, reward: -0.761
Reset #2: mass intervention applied (success: True)
episode 2: 34 steps, reward: -0.422
Reset #3: mass intervention applied (success: True)
episode 3: 30 steps, reward: -0.534
total data points collected: 151
average episode length: 30.2
average episode reward: -0.741
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([151, 56]), actions: torch.Size([151, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3180', '6.9642', '7.7626', '7.5738', '7.8932']
Training reward models...
Reward model losses: ['0.1668', '0.0219', '0.0115', '0.0837', '0.0423']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8087', '1.7376', '1.7994', '1.9091', '1.7669']
Training action VAE models...
Action VAE losses: ['1.3928', '1.3632', '1.4283', '1.4441', '1.3337']
CM score components:
transition disagreement: 0.4351
reward disagreement: 0.1166
state disagreement: 0.5346
action disagreement: 0.5420
total CM score: 1.6283
mass is complete. CM score: 1.6283
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 27 steps, reward: -0.764
Reset #2: friction intervention applied (success: True)
episode 2: 27 steps, reward: -0.771
Reset #3: friction intervention applied (success: True)
episode 3: 27 steps, reward: -0.771
total data points collected: 136
average episode length: 27.2
average episode reward: -0.790
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([136, 56]), actions: torch.Size([136, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8421', '7.4845', '7.6449', '7.9799', '6.7912']
Training reward models...
Reward model losses: ['0.0217', '0.1581', '0.5899', '0.0356', '0.0690']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3773', '1.5370', '1.7074', '1.5879', '1.5768']
Training action VAE models...
Action VAE losses: ['1.4240', '1.3458', '1.5365', '1.3256', '1.4250']
CM score components:
transition disagreement: 0.3655
reward disagreement: 0.2209
state disagreement: 0.5122
action disagreement: 0.5682
total CM score: 1.6668
friction is complete. CM score: 1.6668
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 27 steps, reward: -0.771
Reset #2: visual intervention applied (success: True)
episode 2: 27 steps, reward: -0.771
Reset #3: visual intervention applied (success: True)
episode 3: 27 steps, reward: -0.771
total data points collected: 135
average episode length: 27.0
average episode reward: -0.771
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([135, 56]), actions: torch.Size([135, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7998', '7.7304', '7.8474', '7.5528', '7.4615']
Training reward models...
Reward model losses: ['0.0465', '0.0097', '0.0692', '0.0378', '0.0049']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6547', '1.8368', '1.5178', '1.5826', '1.7961']
Training action VAE models...
Action VAE losses: ['1.4371', '1.4053', '1.4527', '1.4541', '1.4050']
CM score components:
transition disagreement: 0.3661
reward disagreement: 0.0685
state disagreement: 0.4895
action disagreement: 0.5668
total CM score: 1.4909
visual is complete. CM score: 1.4909
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 104.537
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 18.875
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 12.484
total data points collected: 40030
average episode length: 8006.0
average episode reward: 30.308
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40030, 56]), actions: torch.Size([40030, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3780', '7.3473', '7.6298', '7.7960', '7.4765']
Training reward models...
Reward model losses: ['0.0141', '0.0530', '0.0155', '0.0131', '0.0166']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9747', '0.9201', '0.9646', '0.9782', '1.0084']
Training action VAE models...
Action VAE losses: ['1.2062', '1.0083', '1.0961', '1.1192', '1.3217']
CM score components:
transition disagreement: 0.4235
reward disagreement: 0.1052
state disagreement: 0.4664
action disagreement: 0.5390
total CM score: 1.5341
pose is complete. CM score: 1.5341
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 18.717
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.825
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.332
total data points collected: 50005
average episode length: 10001.0
average episode reward: 28.136
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3097', '6.8237', '7.3788', '7.5246', '7.5979']
Training reward models...
Reward model losses: ['0.0046', '0.1462', '0.0785', '0.0061', '1.3262']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1516', '1.3164', '1.2524', '1.1430', '1.3041']
Training action VAE models...
Action VAE losses: ['1.3559', '1.3617', '1.3519', '1.3987', '1.4783']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.3924
state disagreement: 0.5085
action disagreement: 0.5451
total CM score: 1.8716
random is complete. CM score: 1.8716
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 254       |
|    ep_rew_mean     | 2.2132688 |
| time/              |           |
|    fps             | 461       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 254         |
|    ep_rew_mean          | 2.2132688   |
| time/                   |             |
|    fps                  | 290         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.035862505 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.519      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0915     |
|    std                  | 2.65        |
|    value_loss           | 0.0527      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 254         |
|    ep_rew_mean          | 2.2132688   |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 3           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.029173575 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.517      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.071      |
|    std                  | 2.66        |
|    value_loss           | 0.0384      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 352         |
|    ep_rew_mean          | 2.285199    |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 4           |
|    time_elapsed         | 66          |
|    total_timesteps      | 5062656     |
| train/                  |             |
|    approx_kl            | 0.022583915 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.479      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0459     |
|    std                  | 2.69        |
|    value_loss           | 0.0241      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 352         |
|    ep_rew_mean          | 2.285199    |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 5           |
|    time_elapsed         | 85          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.030001668 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.538      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0901     |
|    std                  | 2.72        |
|    value_loss           | 0.024       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 449         |
|    ep_rew_mean          | 2.9759078   |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 6           |
|    time_elapsed         | 105         |
|    total_timesteps      | 5070848     |
| train/                  |             |
|    approx_kl            | 0.034329638 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.555      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0969     |
|    std                  | 2.74        |
|    value_loss           | 0.0147      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 449        |
|    ep_rew_mean          | 2.9759078  |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 7          |
|    time_elapsed         | 124        |
|    total_timesteps      | 5074944    |
| train/                  |            |
|    approx_kl            | 0.04215638 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.537     |
|    n_updates            | 1245       |
|    policy_gradient_loss | -0.0842    |
|    std                  | 2.76       |
|    value_loss           | 0.0223     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 547         |
|    ep_rew_mean          | 2.9819613   |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 8           |
|    time_elapsed         | 143         |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.046823226 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.571      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.108      |
|    std                  | 2.78        |
|    value_loss           | 0.00819     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 558         |
|    ep_rew_mean          | 3.044444    |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 9           |
|    time_elapsed         | 163         |
|    total_timesteps      | 5083136     |
| train/                  |             |
|    approx_kl            | 0.052595314 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.559      |
|    n_updates            | 1275        |
|    policy_gradient_loss | -0.105      |
|    std                  | 2.82        |
|    value_loss           | 0.00389     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 558        |
|    ep_rew_mean          | 3.044444   |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 10         |
|    time_elapsed         | 181        |
|    total_timesteps      | 5087232    |
| train/                  |            |
|    approx_kl            | 0.04730013 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.574     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.102     |
|    std                  | 2.84       |
|    value_loss           | 0.0121     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 654        |
|    ep_rew_mean          | 3.4917626  |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 11         |
|    time_elapsed         | 200        |
|    total_timesteps      | 5091328    |
| train/                  |            |
|    approx_kl            | 0.03797015 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.00025    |
|    loss                 | -0.565     |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 2.88       |
|    value_loss           | 0.00487    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 673        |
|    ep_rew_mean          | 3.5982306  |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 12         |
|    time_elapsed         | 219        |
|    total_timesteps      | 5095424    |
| train/                  |            |
|    approx_kl            | 0.04562588 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.542     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0757    |
|    std                  | 2.94       |
|    value_loss           | 0.0103     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 673         |
|    ep_rew_mean          | 3.5982306   |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 13          |
|    time_elapsed         | 239         |
|    total_timesteps      | 5099520     |
| train/                  |             |
|    approx_kl            | 0.054817595 |
|    clip_fraction        | 0.479       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.541      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.0884     |
|    std                  | 2.97        |
|    value_loss           | 0.0161      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_0.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=-5.788, success=False
episode 2: length=10001, reward=-5.788, success=False
episode 3: length=10001, reward=-5.788, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -5.788
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 32.797
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -4.442
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 59.364
total data points collected: 50005
average episode length: 10001.0
average episode reward: 16.415
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6941', '8.2222', '7.3892', '7.7608', '8.1667']
Training reward models...
Reward model losses: ['0.0062', '0.1796', '0.0252', '1.2898', '2.0538']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3687', '1.3061', '1.0904', '1.3522', '1.2533']
Training action VAE models...
Action VAE losses: ['1.3162', '1.2916', '1.2430', '1.3873', '1.3377']
CM score components:
transition disagreement: 0.4416
reward disagreement: 0.5064
state disagreement: 0.5218
action disagreement: 0.5381
total CM score: 2.0079
goal is complete. CM score: 2.0079
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.881
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.183
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -1.569
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.689
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7529', '7.2748', '7.9162', '7.6451', '6.8259']
Training reward models...
Reward model losses: ['0.5648', '0.3951', '0.0179', '0.1775', '0.1967']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3786', '1.1544', '1.0985', '1.1547', '1.2717']
Training action VAE models...
Action VAE losses: ['1.2619', '1.3100', '1.2488', '1.3636', '1.3184']
CM score components:
transition disagreement: 0.4419
reward disagreement: 0.2921
state disagreement: 0.4818
action disagreement: 0.5290
total CM score: 1.7448
mass is complete. CM score: 1.7448
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 98.382
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.436
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 88.286
total data points collected: 40143
average episode length: 8028.6
average episode reward: 55.244
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40143, 56]), actions: torch.Size([40143, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4768', '7.3328', '7.2265', '6.8358', '8.0935']
Training reward models...
Reward model losses: ['0.6858', '0.0291', '0.0542', '0.0884', '0.2257']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2015', '1.2825', '1.1388', '1.1745', '1.1527']
Training action VAE models...
Action VAE losses: ['1.3756', '1.4566', '1.3440', '1.4531', '1.4621']
CM score components:
transition disagreement: 0.4142
reward disagreement: 0.2548
state disagreement: 0.4971
action disagreement: 0.5383
total CM score: 1.7044
friction is complete. CM score: 1.7044
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -5.788
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -5.788
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -5.788
total data points collected: 50005
average episode length: 10001.0
average episode reward: -5.788
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8179', '7.4912', '7.7537', '7.5726', '7.3305']
Training reward models...
Reward model losses: ['0.3307', '0.7182', '0.0059', '0.0020', '0.2118']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7225', '0.8995', '0.7539', '0.7329', '0.7103']
Training action VAE models...
Action VAE losses: ['1.0113', '1.0080', '1.2113', '0.9570', '0.9114']
CM score components:
transition disagreement: 0.4114
reward disagreement: 0.2204
state disagreement: 0.4376
action disagreement: 0.5723
total CM score: 1.6417
visual is complete. CM score: 1.6417
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: -3.642
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 21.361
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -3.529
total data points collected: 40486
average episode length: 8097.2
average episode reward: 6.340
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40486, 56]), actions: torch.Size([40486, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9687', '7.2350', '8.2653', '8.1007', '6.7037']
Training reward models...
Reward model losses: ['0.4562', '0.0125', '0.1832', '0.0159', '0.0295']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3499', '1.4039', '1.1343', '1.1977', '1.2635']
Training action VAE models...
Action VAE losses: ['1.3649', '1.4465', '1.3129', '1.4893', '1.4513']
CM score components:
transition disagreement: 0.4401
reward disagreement: 0.1452
state disagreement: 0.5178
action disagreement: 0.5146
total CM score: 1.6176
pose is complete. CM score: 1.6176
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.928
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.950
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -2.430
total data points collected: 50005
average episode length: 10001.0
average episode reward: 10.634
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7797', '7.6487', '6.9178', '7.8685', '7.8869']
Training reward models...
Reward model losses: ['0.9332', '0.3959', '0.1069', '0.9247', '0.0060']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1874', '1.3184', '1.2892', '1.1878', '1.2260']
Training action VAE models...
Action VAE losses: ['1.4712', '1.5055', '1.4898', '1.3125', '1.3265']
CM score components:
transition disagreement: 0.4157
reward disagreement: 0.4790
state disagreement: 0.4875
action disagreement: 0.5584
total CM score: 1.9406
random is complete. CM score: 1.9406
2025-07-16 16:39:39,235 3266759 INFO Meta-Episode 1/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_0.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 32.797
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -4.442
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 59.364
total data points collected: 50005
average episode length: 10001.0
average episode reward: 16.415
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3750', '7.2335', '6.8366', '7.0218', '7.6629']
Training reward models...
Reward model losses: ['0.1818', '0.2181', '0.2570', '0.0040', '0.0176']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1362', '1.3063', '1.2660', '1.2336', '1.1902']
Training action VAE models...
Action VAE losses: ['1.4703', '1.3383', '1.3608', '1.2465', '1.2210']
CM score components:
transition disagreement: 0.4165
reward disagreement: 0.0743
state disagreement: 0.4629
action disagreement: 0.5592
total CM score: 1.5129
goal is complete. CM score: 1.5129
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.881
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.183
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -1.569
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.689
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0873', '7.7257', '6.9919', '6.8720', '8.3271']
Training reward models...
Reward model losses: ['0.0080', '0.1393', '0.0117', '0.5823', '0.4622']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3068', '1.1171', '1.1611', '1.2143', '1.3609']
Training action VAE models...
Action VAE losses: ['1.3162', '1.3377', '1.2512', '1.3308', '1.2513']
CM score components:
transition disagreement: 0.4192
reward disagreement: 0.1788
state disagreement: 0.5182
action disagreement: 0.5462
total CM score: 1.6624
mass is complete. CM score: 1.6624
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 98.382
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.436
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 88.286
total data points collected: 40143
average episode length: 8028.6
average episode reward: 55.244
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40143, 56]), actions: torch.Size([40143, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9144', '7.3561', '6.8612', '7.6742', '7.0962']
Training reward models...
Reward model losses: ['0.0706', '0.9211', '0.4216', '0.0202', '0.3683']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1981', '1.2185', '1.2117', '1.2828', '1.1207']
Training action VAE models...
Action VAE losses: ['1.3559', '1.4213', '1.2664', '1.2947', '1.4839']
CM score components:
transition disagreement: 0.4231
reward disagreement: 0.3880
state disagreement: 0.4951
action disagreement: 0.5223
total CM score: 1.8284
friction is complete. CM score: 1.8284
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -5.788
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -5.788
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -5.788
total data points collected: 50005
average episode length: 10001.0
average episode reward: -5.788
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3693', '7.9003', '7.0978', '7.2871', '7.6120']
Training reward models...
Reward model losses: ['0.0846', '0.4747', '0.0341', '0.0423', '0.0482']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8187', '0.7138', '0.7715', '0.8519', '0.7449']
Training action VAE models...
Action VAE losses: ['1.0165', '0.8395', '0.9925', '1.0839', '1.1495']
CM score components:
transition disagreement: 0.4586
reward disagreement: 0.1801
state disagreement: 0.4415
action disagreement: 0.5155
total CM score: 1.5957
visual is complete. CM score: 1.5957
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: -3.642
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 21.361
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -3.529
total data points collected: 40486
average episode length: 8097.2
average episode reward: 6.340
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40486, 56]), actions: torch.Size([40486, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7741', '7.8320', '7.7720', '6.6605', '7.5150']
Training reward models...
Reward model losses: ['3.1284', '1.0610', '0.1245', '0.0055', '0.0512']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3676', '1.2498', '1.1478', '1.1855', '1.2452']
Training action VAE models...
Action VAE losses: ['1.3143', '1.5033', '1.4517', '1.4206', '1.3515']
CM score components:
transition disagreement: 0.4436
reward disagreement: 0.7499
state disagreement: 0.5305
action disagreement: 0.5540
total CM score: 2.2780
pose is complete. CM score: 2.2780
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.928
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.950
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -2.430
total data points collected: 50005
average episode length: 10001.0
average episode reward: 10.634
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4733', '7.3294', '8.3438', '7.4607', '7.3668']
Training reward models...
Reward model losses: ['0.0495', '0.0414', '0.0308', '0.1529', '0.1634']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3107', '1.2036', '1.2465', '1.3113', '1.2512']
Training action VAE models...
Action VAE losses: ['1.3984', '1.5014', '1.4167', '1.4915', '1.5057']
CM score components:
transition disagreement: 0.3766
reward disagreement: 0.1068
state disagreement: 0.4924
action disagreement: 0.5826
total CM score: 1.5584
random is complete. CM score: 1.5584
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 693      |
|    ep_rew_mean     | 3.754646 |
| time/              |          |
|    fps             | 475      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 5103616  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 693         |
|    ep_rew_mean          | 3.754646    |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5107712     |
| train/                  |             |
|    approx_kl            | 0.029965531 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.507      |
|    n_updates            | 1365        |
|    policy_gradient_loss | -0.0501     |
|    std                  | 2.99        |
|    value_loss           | 0.043       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 693        |
|    ep_rew_mean          | 3.754646   |
| time/                   |            |
|    fps                  | 427        |
|    iterations           | 3          |
|    time_elapsed         | 28         |
|    total_timesteps      | 5111808    |
| train/                  |            |
|    approx_kl            | 0.03376682 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.485     |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0318    |
|    std                  | 3.01       |
|    value_loss           | 0.0255     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 791         |
|    ep_rew_mean          | 3.7352862   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5115904     |
| train/                  |             |
|    approx_kl            | 0.021323156 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.48       |
|    n_updates            | 1395        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 3.02        |
|    value_loss           | 0.0156      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 791        |
|    ep_rew_mean          | 3.7352862  |
| time/                   |            |
|    fps                  | 421        |
|    iterations           | 5          |
|    time_elapsed         | 48         |
|    total_timesteps      | 5120000    |
| train/                  |            |
|    approx_kl            | 0.02436065 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.476     |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0327    |
|    std                  | 3.04       |
|    value_loss           | 0.0264     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 888         |
|    ep_rew_mean          | 3.7634628   |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 6           |
|    time_elapsed         | 58          |
|    total_timesteps      | 5124096     |
| train/                  |             |
|    approx_kl            | 0.017945737 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.473      |
|    n_updates            | 1425        |
|    policy_gradient_loss | -0.013      |
|    std                  | 3.07        |
|    value_loss           | 0.00754     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 888         |
|    ep_rew_mean          | 3.7634628   |
| time/                   |             |
|    fps                  | 420         |
|    iterations           | 7           |
|    time_elapsed         | 68          |
|    total_timesteps      | 5128192     |
| train/                  |             |
|    approx_kl            | 0.027853895 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -22.9       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.502      |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0413     |
|    std                  | 3.09        |
|    value_loss           | 0.0166      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 888        |
|    ep_rew_mean          | 3.7634628  |
| time/                   |            |
|    fps                  | 420        |
|    iterations           | 8          |
|    time_elapsed         | 77         |
|    total_timesteps      | 5132288    |
| train/                  |            |
|    approx_kl            | 0.01594057 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.496     |
|    n_updates            | 1455       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 3.11       |
|    value_loss           | 0.00389    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 3.7224092   |
| time/                   |             |
|    fps                  | 421         |
|    iterations           | 9           |
|    time_elapsed         | 87          |
|    total_timesteps      | 5136384     |
| train/                  |             |
|    approx_kl            | 0.019356286 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23         |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.504      |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 3.13        |
|    value_loss           | 0.00276     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 3.7224092   |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 10          |
|    time_elapsed         | 97          |
|    total_timesteps      | 5140480     |
| train/                  |             |
|    approx_kl            | 0.023490965 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.1       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.511      |
|    n_updates            | 1485        |
|    policy_gradient_loss | -0.0394     |
|    std                  | 3.16        |
|    value_loss           | 0.0349      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.08e+03    |
|    ep_rew_mean          | 3.6595216   |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 11          |
|    time_elapsed         | 107         |
|    total_timesteps      | 5144576     |
| train/                  |             |
|    approx_kl            | 0.017495519 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.1       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.47       |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0131     |
|    std                  | 3.17        |
|    value_loss           | 0.00325     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.08e+03   |
|    ep_rew_mean          | 3.6595216  |
| time/                   |            |
|    fps                  | 417        |
|    iterations           | 12         |
|    time_elapsed         | 117        |
|    total_timesteps      | 5148672    |
| train/                  |            |
|    approx_kl            | 0.02672504 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.517     |
|    n_updates            | 1515       |
|    policy_gradient_loss | -0.0464    |
|    std                  | 3.19       |
|    value_loss           | 0.0209     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.08e+03    |
|    ep_rew_mean          | 3.6595216   |
| time/                   |             |
|    fps                  | 414         |
|    iterations           | 13          |
|    time_elapsed         | 128         |
|    total_timesteps      | 5152768     |
| train/                  |             |
|    approx_kl            | 0.022797797 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.3       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.525      |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0397     |
|    std                  | 3.22        |
|    value_loss           | 0.0134      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_1.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=1.195, success=False
episode 2: length=10001, reward=1.195, success=False
episode 3: length=10001, reward=1.195, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 1.195
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 54.660
Reset #2: goal intervention applied (success: True)
episode 2: 213 steps, reward: 0.691
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -3.410
total data points collected: 40217
average episode length: 8043.4
average episode reward: 30.346
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40217, 56]), actions: torch.Size([40217, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1789', '7.0628', '7.8762', '7.8676', '8.8767']
Training reward models...
Reward model losses: ['0.2609', '0.0120', '0.1868', '0.0086', '0.0088']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1577', '1.2335', '1.1942', '1.1735', '1.0446']
Training action VAE models...
Action VAE losses: ['1.3228', '1.4032', '1.2946', '1.3333', '1.3199']
CM score components:
transition disagreement: 0.4619
reward disagreement: 0.1224
state disagreement: 0.4760
action disagreement: 0.5749
total CM score: 1.6352
goal is complete. CM score: 1.6352
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 55.548
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 38.093
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 80.505
total data points collected: 40237
average episode length: 8047.4
average episode reward: 34.254
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40237, 56]), actions: torch.Size([40237, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4885', '7.3181', '7.6680', '8.6074', '8.0094']
Training reward models...
Reward model losses: ['0.0120', '0.1026', '1.8093', '0.2481', '0.1802']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5085', '1.6988', '1.7237', '1.5100', '1.5453']
Training action VAE models...
Action VAE losses: ['1.1852', '1.1719', '1.1842', '1.1404', '1.2175']
CM score components:
transition disagreement: 0.4254
reward disagreement: 0.4492
state disagreement: 0.5410
action disagreement: 0.5416
total CM score: 1.9571
mass is complete. CM score: 1.9571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 16.376
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.537
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 45.620
total data points collected: 50005
average episode length: 10001.0
average episode reward: 13.932
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2646', '7.6844', '6.9648', '7.6555', '6.3767']
Training reward models...
Reward model losses: ['0.0091', '0.0175', '0.1497', '0.0467', '0.2459']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2422', '1.2071', '1.2644', '1.3099', '1.1839']
Training action VAE models...
Action VAE losses: ['1.4644', '1.4082', '1.3321', '1.2572', '1.4847']
CM score components:
transition disagreement: 0.4403
reward disagreement: 0.1400
state disagreement: 0.4934
action disagreement: 0.5447
total CM score: 1.6184
friction is complete. CM score: 1.6184
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.195
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.195
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.195
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.195
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9739', '7.6131', '7.2724', '6.9255', '7.6381']
Training reward models...
Reward model losses: ['0.0064', '0.0435', '0.3859', '0.4043', '0.0682']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0585', '1.0675', '1.1226', '1.1763', '1.0645']
Training action VAE models...
Action VAE losses: ['1.3877', '1.2536', '1.1971', '1.2041', '1.1911']
CM score components:
transition disagreement: 0.4391
reward disagreement: 0.1922
state disagreement: 0.5004
action disagreement: 0.5752
total CM score: 1.7068
visual is complete. CM score: 1.7068
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 81 steps, reward: -0.353
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 22.137
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 8.327
total data points collected: 34501
average episode length: 6900.2
average episode reward: 16.300
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([34501, 56]), actions: torch.Size([34501, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4536', '7.3215', '7.5538', '7.3488', '7.3160']
Training reward models...
Reward model losses: ['0.2606', '0.0343', '0.3292', '0.0212', '0.0178']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1991', '1.2921', '1.3618', '1.3824', '1.2172']
Training action VAE models...
Action VAE losses: ['1.2224', '1.3137', '1.2782', '1.2700', '1.3421']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.1737
state disagreement: 0.4812
action disagreement: 0.5185
total CM score: 1.5990
pose is complete. CM score: 1.5990
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.734
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.621
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.007
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.517
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6995', '7.6734', '8.3549', '7.7129', '7.5642']
Training reward models...
Reward model losses: ['0.0155', '0.0110', '0.2173', '0.0240', '0.2400']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2112', '1.2620', '1.3671', '1.2486', '1.1904']
Training action VAE models...
Action VAE losses: ['1.4736', '1.5198', '1.3774', '1.3761', '1.4871']
CM score components:
transition disagreement: 0.4165
reward disagreement: 0.1393
state disagreement: 0.4909
action disagreement: 0.5660
total CM score: 1.6127
random is complete. CM score: 1.6127
2025-07-16 17:01:56,459 3266759 INFO Meta-Episode 2/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_1.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 54.660
Reset #2: goal intervention applied (success: True)
episode 2: 213 steps, reward: 0.691
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -3.410
total data points collected: 40217
average episode length: 8043.4
average episode reward: 30.346
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40217, 56]), actions: torch.Size([40217, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3275', '7.2195', '6.7752', '7.0155', '7.6446']
Training reward models...
Reward model losses: ['0.1457', '0.2287', '0.2508', '0.0032', '0.0173']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0548', '1.1588', '1.1995', '1.1320', '1.1149']
Training action VAE models...
Action VAE losses: ['1.4321', '1.4197', '1.4521', '1.2972', '1.3322']
CM score components:
transition disagreement: 0.4150
reward disagreement: 0.0502
state disagreement: 0.4529
action disagreement: 0.5644
total CM score: 1.4825
goal is complete. CM score: 1.4825
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 55.548
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 38.093
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 80.505
total data points collected: 40237
average episode length: 8047.4
average episode reward: 34.254
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40237, 56]), actions: torch.Size([40237, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1271', '7.7317', '7.0330', '6.8323', '8.3986']
Training reward models...
Reward model losses: ['0.0072', '0.1597', '0.0102', '0.5728', '0.4575']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5152', '1.3611', '1.4225', '1.4730', '1.7451']
Training action VAE models...
Action VAE losses: ['1.3838', '1.3218', '1.1892', '1.2247', '1.1825']
CM score components:
transition disagreement: 0.4163
reward disagreement: 0.1680
state disagreement: 0.5493
action disagreement: 0.5521
total CM score: 1.6857
mass is complete. CM score: 1.6857
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 16.376
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.537
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 45.620
total data points collected: 50005
average episode length: 10001.0
average episode reward: 13.932
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9682', '7.3387', '6.7850', '7.6886', '7.1042']
Training reward models...
Reward model losses: ['0.0642', '0.8151', '0.4628', '0.0258', '0.3133']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2438', '1.2720', '1.2779', '1.2998', '1.1713']
Training action VAE models...
Action VAE losses: ['1.2676', '1.4077', '1.2593', '1.2685', '1.4182']
CM score components:
transition disagreement: 0.4226
reward disagreement: 0.3577
state disagreement: 0.5033
action disagreement: 0.5237
total CM score: 1.8073
friction is complete. CM score: 1.8073
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.195
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.195
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.195
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.195
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3550', '7.9374', '7.1473', '7.2420', '7.6530']
Training reward models...
Reward model losses: ['0.0385', '0.3454', '0.0340', '0.0219', '0.0536']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1335', '1.0248', '1.1748', '1.1672', '1.1444']
Training action VAE models...
Action VAE losses: ['1.2416', '1.2008', '1.1966', '1.2256', '1.3022']
CM score components:
transition disagreement: 0.4432
reward disagreement: 0.1715
state disagreement: 0.4694
action disagreement: 0.5330
total CM score: 1.6171
visual is complete. CM score: 1.6171
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 81 steps, reward: -0.353
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 22.137
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 8.327
total data points collected: 34501
average episode length: 6900.2
average episode reward: 16.300
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([34501, 56]), actions: torch.Size([34501, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7667', '7.7676', '7.6332', '6.6129', '7.4211']
Training reward models...
Reward model losses: ['2.9145', '0.9161', '0.1345', '0.0115', '0.0458']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3327', '1.4415', '1.3004', '1.3534', '1.2515']
Training action VAE models...
Action VAE losses: ['1.2306', '1.4081', '1.3530', '1.3530', '1.2999']
CM score components:
transition disagreement: 0.4388
reward disagreement: 0.6938
state disagreement: 0.5031
action disagreement: 0.5568
total CM score: 2.1926
pose is complete. CM score: 2.1926
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.734
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.621
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.007
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.517
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4814', '7.3266', '8.3344', '7.4481', '7.2799']
Training reward models...
Reward model losses: ['0.0480', '0.0178', '0.0219', '0.1893', '0.1427']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2557', '1.2769', '1.2417', '1.2215', '1.2588']
Training action VAE models...
Action VAE losses: ['1.4261', '1.3849', '1.4612', '1.4836', '1.4358']
CM score components:
transition disagreement: 0.3770
reward disagreement: 0.0893
state disagreement: 0.4907
action disagreement: 0.5802
total CM score: 1.5372
random is complete. CM score: 1.5372
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.08e+03  |
|    ep_rew_mean     | 3.6428869 |
| time/              |           |
|    fps             | 434       |
|    iterations      | 1         |
|    time_elapsed    | 9         |
|    total_timesteps | 5156864   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.08e+03    |
|    ep_rew_mean          | 3.6428869   |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 2           |
|    time_elapsed         | 19          |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.019796696 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.00027     |
|    loss                 | -0.503      |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0324     |
|    std                  | 3.26        |
|    value_loss           | 0.0176      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | 3.527388    |
| time/                   |             |
|    fps                  | 416         |
|    iterations           | 3           |
|    time_elapsed         | 29          |
|    total_timesteps      | 5165056     |
| train/                  |             |
|    approx_kl            | 0.019514453 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.00027     |
|    loss                 | -0.499      |
|    n_updates            | 1575        |
|    policy_gradient_loss | -0.0215     |
|    std                  | 3.29        |
|    value_loss           | 0.0055      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | 3.527388    |
| time/                   |             |
|    fps                  | 415         |
|    iterations           | 4           |
|    time_elapsed         | 39          |
|    total_timesteps      | 5169152     |
| train/                  |             |
|    approx_kl            | 0.019504916 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.00027     |
|    loss                 | -0.509      |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0354     |
|    std                  | 3.31        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 3.4778414   |
| time/                   |             |
|    fps                  | 421         |
|    iterations           | 5           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5173248     |
| train/                  |             |
|    approx_kl            | 0.018872809 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.00027     |
|    loss                 | -0.508      |
|    n_updates            | 1605        |
|    policy_gradient_loss | -0.0275     |
|    std                  | 3.32        |
|    value_loss           | 0.00755     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.27e+03   |
|    ep_rew_mean          | 3.4778414  |
| time/                   |            |
|    fps                  | 426        |
|    iterations           | 6          |
|    time_elapsed         | 57         |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.02637957 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.6      |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.00027    |
|    loss                 | -0.502     |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0278    |
|    std                  | 3.33       |
|    value_loss           | 0.0176     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.27e+03    |
|    ep_rew_mean          | 3.4778414   |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 7           |
|    time_elapsed         | 67          |
|    total_timesteps      | 5181440     |
| train/                  |             |
|    approx_kl            | 0.025643744 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.6       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.00027     |
|    loss                 | -0.513      |
|    n_updates            | 1635        |
|    policy_gradient_loss | -0.0398     |
|    std                  | 3.36        |
|    value_loss           | 0.0274      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.37e+03   |
|    ep_rew_mean          | 3.6352482  |
| time/                   |            |
|    fps                  | 423        |
|    iterations           | 8          |
|    time_elapsed         | 77         |
|    total_timesteps      | 5185536    |
| train/                  |            |
|    approx_kl            | 0.01769663 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.00027    |
|    loss                 | -0.491     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0144    |
|    std                  | 3.37       |
|    value_loss           | 0.00392    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.37e+03   |
|    ep_rew_mean          | 3.6352482  |
| time/                   |            |
|    fps                  | 426        |
|    iterations           | 9          |
|    time_elapsed         | 86         |
|    total_timesteps      | 5189632    |
| train/                  |            |
|    approx_kl            | 0.01871343 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.00027    |
|    loss                 | -0.52      |
|    n_updates            | 1665       |
|    policy_gradient_loss | -0.0364    |
|    std                  | 3.39       |
|    value_loss           | 0.0173     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.46e+03   |
|    ep_rew_mean          | 3.6442204  |
| time/                   |            |
|    fps                  | 427        |
|    iterations           | 10         |
|    time_elapsed         | 95         |
|    total_timesteps      | 5193728    |
| train/                  |            |
|    approx_kl            | 0.01748044 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.8      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.00027    |
|    loss                 | -0.529     |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0371    |
|    std                  | 3.41       |
|    value_loss           | 0.008      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 3.6442204   |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 11          |
|    time_elapsed         | 105         |
|    total_timesteps      | 5197824     |
| train/                  |             |
|    approx_kl            | 0.020269053 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.00027     |
|    loss                 | -0.52       |
|    n_updates            | 1695        |
|    policy_gradient_loss | -0.0332     |
|    std                  | 3.43        |
|    value_loss           | 0.00865     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.46e+03    |
|    ep_rew_mean          | 3.6442204   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 12          |
|    time_elapsed         | 115         |
|    total_timesteps      | 5201920     |
| train/                  |             |
|    approx_kl            | 0.021410868 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.00027     |
|    loss                 | -0.518      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0322     |
|    std                  | 3.46        |
|    value_loss           | 0.0118      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.56e+03    |
|    ep_rew_mean          | 3.6322904   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 13          |
|    time_elapsed         | 125         |
|    total_timesteps      | 5206016     |
| train/                  |             |
|    approx_kl            | 0.020616803 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.00027     |
|    loss                 | -0.517      |
|    n_updates            | 1725        |
|    policy_gradient_loss | -0.0265     |
|    std                  | 3.49        |
|    value_loss           | 0.00399     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_2.zip

evaluating student performance (5 episodes)...
episode 1: length=25, reward=-0.306, success=True
episode 2: length=25, reward=-0.306, success=True
episode 3: length=25, reward=-0.306, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.306
average episode length: 25.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 26 steps, reward: 2.014
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 38.959
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 15.880
total data points collected: 30048
average episode length: 6009.6
average episode reward: 17.179
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30048, 56]), actions: torch.Size([30048, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1921', '7.0561', '7.8046', '7.7917', '8.8219']
Training reward models...
Reward model losses: ['0.2900', '0.0139', '0.2270', '0.0048', '0.0125']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9860', '1.0039', '1.0581', '0.9857', '0.9489']
Training action VAE models...
Action VAE losses: ['1.4631', '1.3959', '1.2587', '1.3741', '1.3709']
CM score components:
transition disagreement: 0.4501
reward disagreement: 0.1466
state disagreement: 0.4578
action disagreement: 0.5856
total CM score: 1.6401
goal is complete. CM score: 1.6401
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 49.559
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 100.964
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 81.391
total data points collected: 40620
average episode length: 8124.0
average episode reward: 59.169
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40620, 56]), actions: torch.Size([40620, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4479', '7.2530', '7.6668', '8.4657', '7.9692']
Training reward models...
Reward model losses: ['0.0123', '0.0873', '1.7380', '0.2426', '0.0969']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0634', '1.0505', '1.1191', '0.9383', '0.9794']
Training action VAE models...
Action VAE losses: ['1.3037', '1.3522', '1.1874', '1.2763', '1.2894']
CM score components:
transition disagreement: 0.4257
reward disagreement: 0.4548
state disagreement: 0.4923
action disagreement: 0.5473
total CM score: 1.9202
mass is complete. CM score: 1.9202
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 25 steps, reward: -0.306
Reset #2: friction intervention applied (success: True)
episode 2: 25 steps, reward: -0.307
Reset #3: friction intervention applied (success: True)
episode 3: 25 steps, reward: -0.308
total data points collected: 125
average episode length: 25.0
average episode reward: -0.307
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([125, 56]), actions: torch.Size([125, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2285', '7.6346', '7.0572', '7.7600', '6.4565']
Training reward models...
Reward model losses: ['0.0134', '0.0195', '0.1469', '0.0337', '0.2645']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5322', '1.4311', '1.4886', '1.4414', '1.6241']
Training action VAE models...
Action VAE losses: ['1.5150', '1.4409', '1.3448', '1.2817', '1.5575']
CM score components:
transition disagreement: 0.4483
reward disagreement: 0.1436
state disagreement: 0.5354
action disagreement: 0.5594
total CM score: 1.6866
friction is complete. CM score: 1.6866
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 25 steps, reward: -0.306
Reset #2: visual intervention applied (success: True)
episode 2: 25 steps, reward: -0.306
Reset #3: visual intervention applied (success: True)
episode 3: 25 steps, reward: -0.306
total data points collected: 125
average episode length: 25.0
average episode reward: -0.306
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([125, 56]), actions: torch.Size([125, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9669', '7.6387', '7.2794', '6.8567', '7.6379']
Training reward models...
Reward model losses: ['0.0067', '0.0762', '0.2948', '0.2759', '0.1865']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5701', '1.4768', '1.3655', '1.6591', '1.5385']
Training action VAE models...
Action VAE losses: ['1.5871', '1.4446', '1.4688', '1.4771', '1.4083']
CM score components:
transition disagreement: 0.4339
reward disagreement: 0.1619
state disagreement: 0.5379
action disagreement: 0.5929
total CM score: 1.7266
visual is complete. CM score: 1.7266
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 103.950
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 12.652
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 7.217
total data points collected: 40025
average episode length: 8005.0
average episode reward: 28.486
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4716', '7.3175', '7.5489', '7.3819', '7.3200']
Training reward models...
Reward model losses: ['0.2613', '0.0357', '0.3137', '0.0218', '0.0224']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2604', '1.2787', '1.2483', '1.2708', '1.1510']
Training action VAE models...
Action VAE losses: ['1.2812', '1.3747', '1.3778', '1.3749', '1.4021']
CM score components:
transition disagreement: 0.4274
reward disagreement: 0.1765
state disagreement: 0.4938
action disagreement: 0.5156
total CM score: 1.6133
pose is complete. CM score: 1.6133
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -0.325
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.119
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -2.290
total data points collected: 50005
average episode length: 10001.0
average episode reward: 5.391
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6934', '7.6615', '8.3778', '7.7190', '7.5004']
Training reward models...
Reward model losses: ['0.0173', '0.0128', '0.2704', '0.0193', '0.1978']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1599', '1.2880', '1.2919', '1.2249', '1.1820']
Training action VAE models...
Action VAE losses: ['1.4557', '1.5549', '1.4612', '1.4009', '1.5511']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.1457
state disagreement: 0.4893
action disagreement: 0.5684
total CM score: 1.6237
random is complete. CM score: 1.6237
2025-07-16 17:17:52,454 3266759 INFO Meta-Episode 3/30: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_2.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 26 steps, reward: 2.014
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 38.959
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 15.880
total data points collected: 30048
average episode length: 6009.6
average episode reward: 17.179
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30048, 56]), actions: torch.Size([30048, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2840', '7.1748', '6.7713', '7.0542', '7.6435']
Training reward models...
Reward model losses: ['0.1718', '0.2430', '0.2285', '0.0037', '0.0198']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9926', '0.9312', '1.0063', '0.9985', '1.0365']
Training action VAE models...
Action VAE losses: ['1.4134', '1.3636', '1.4129', '1.2759', '1.3015']
CM score components:
transition disagreement: 0.4045
reward disagreement: 0.0529
state disagreement: 0.4464
action disagreement: 0.5570
total CM score: 1.4609
goal is complete. CM score: 1.4609
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 49.559
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 100.964
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 81.391
total data points collected: 40620
average episode length: 8124.0
average episode reward: 59.169
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40620, 56]), actions: torch.Size([40620, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0918', '7.6750', '7.1037', '6.8293', '8.2465']
Training reward models...
Reward model losses: ['0.0033', '0.3023', '0.0102', '0.3475', '0.6383']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0812', '0.9164', '1.0002', '1.0404', '1.0799']
Training action VAE models...
Action VAE losses: ['1.3267', '1.3662', '1.2918', '1.2611', '1.2056']
CM score components:
transition disagreement: 0.4209
reward disagreement: 0.2004
state disagreement: 0.5003
action disagreement: 0.5410
total CM score: 1.6625
mass is complete. CM score: 1.6625
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 25 steps, reward: -0.306
Reset #2: friction intervention applied (success: True)
episode 2: 25 steps, reward: -0.307
Reset #3: friction intervention applied (success: True)
episode 3: 25 steps, reward: -0.308
total data points collected: 125
average episode length: 25.0
average episode reward: -0.307
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([125, 56]), actions: torch.Size([125, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9423', '7.2992', '6.9142', '7.7164', '7.1505']
Training reward models...
Reward model losses: ['0.0771', '1.1596', '0.4023', '0.0261', '0.2086']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4203', '1.4516', '1.3609', '1.6211', '1.5604']
Training action VAE models...
Action VAE losses: ['1.3020', '1.3499', '1.2957', '1.2385', '1.4162']
CM score components:
transition disagreement: 0.4257
reward disagreement: 0.4140
state disagreement: 0.5332
action disagreement: 0.5282
total CM score: 1.9011
friction is complete. CM score: 1.9011
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 25 steps, reward: -0.306
Reset #2: visual intervention applied (success: True)
episode 2: 25 steps, reward: -0.306
Reset #3: visual intervention applied (success: True)
episode 3: 25 steps, reward: -0.306
total data points collected: 125
average episode length: 25.0
average episode reward: -0.306
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([125, 56]), actions: torch.Size([125, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1759', '7.8065', '7.0775', '7.2494', '7.5339']
Training reward models...
Reward model losses: ['0.0832', '0.3320', '0.0713', '0.0129', '0.0788']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4049', '1.3898', '1.5125', '1.3674', '1.3620']
Training action VAE models...
Action VAE losses: ['1.3597', '1.2812', '1.3082', '1.4353', '1.4928']
CM score components:
transition disagreement: 0.4456
reward disagreement: 0.1514
state disagreement: 0.4965
action disagreement: 0.5353
total CM score: 1.6289
visual is complete. CM score: 1.6289
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 103.950
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 12.652
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 7.217
total data points collected: 40025
average episode length: 8005.0
average episode reward: 28.486
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7595', '7.7726', '7.6421', '6.6203', '7.3954']
Training reward models...
Reward model losses: ['3.0638', '1.0083', '0.1567', '0.0099', '0.0466']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2474', '1.2768', '1.2104', '1.2503', '1.1651']
Training action VAE models...
Action VAE losses: ['1.3861', '1.4853', '1.3793', '1.4813', '1.3595']
CM score components:
transition disagreement: 0.4391
reward disagreement: 0.7306
state disagreement: 0.4889
action disagreement: 0.5616
total CM score: 2.2203
pose is complete. CM score: 2.2203
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -0.325
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.119
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -2.290
total data points collected: 50005
average episode length: 10001.0
average episode reward: 5.391
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4419', '7.2986', '8.2912', '7.3944', '7.2821']
Training reward models...
Reward model losses: ['0.0512', '0.0281', '0.0201', '0.2354', '0.1586']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2426', '1.2541', '1.2351', '1.2043', '1.2367']
Training action VAE models...
Action VAE losses: ['1.4892', '1.4158', '1.4094', '1.4639', '1.4207']
CM score components:
transition disagreement: 0.3697
reward disagreement: 0.1254
state disagreement: 0.4943
action disagreement: 0.5799
total CM score: 1.5693
random is complete. CM score: 1.5693
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.56e+03  |
|    ep_rew_mean     | 3.6322904 |
| time/              |           |
|    fps             | 469       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5210112   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.56e+03   |
|    ep_rew_mean          | 3.6322904  |
| time/                   |            |
|    fps                  | 452        |
|    iterations           | 2          |
|    time_elapsed         | 18         |
|    total_timesteps      | 5214208    |
| train/                  |            |
|    approx_kl            | 0.02339517 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.000243   |
|    loss                 | -0.523     |
|    n_updates            | 1755       |
|    policy_gradient_loss | -0.0402    |
|    std                  | 3.57       |
|    value_loss           | 0.0469     |
----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | 3.5739398   |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5218304     |
| train/                  |             |
|    approx_kl            | 0.021011157 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.2       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.000243    |
|    loss                 | -0.523      |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0365     |
|    std                  | 3.59        |
|    value_loss           | 0.0327      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.66e+03  |
|    ep_rew_mean          | 3.5739398 |
| time/                   |           |
|    fps                  | 445       |
|    iterations           | 4         |
|    time_elapsed         | 36        |
|    total_timesteps      | 5222400   |
| train/                  |           |
|    approx_kl            | 0.0176153 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.3     |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.000243  |
|    loss                 | -0.534    |
|    n_updates            | 1785      |
|    policy_gradient_loss | -0.0393   |
|    std                  | 3.6       |
|    value_loss           | 0.0165    |
---------------------------------------
Reset #3: visual intervention applied (success: True)
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.75e+03   |
|    ep_rew_mean          | 3.5281491  |
| time/                   |            |
|    fps                  | 444        |
|    iterations           | 5          |
|    time_elapsed         | 46         |
|    total_timesteps      | 5226496    |
| train/                  |            |
|    approx_kl            | 0.01580999 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.3      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.000243   |
|    loss                 | -0.542     |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0338    |
|    std                  | 3.62       |
|    value_loss           | 0.00764    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 3.5281491   |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 6           |
|    time_elapsed         | 55          |
|    total_timesteps      | 5230592     |
| train/                  |             |
|    approx_kl            | 0.016704729 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.53       |
|    n_updates            | 1815        |
|    policy_gradient_loss | -0.032      |
|    std                  | 3.65        |
|    value_loss           | 0.0142      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.75e+03    |
|    ep_rew_mean          | 3.5281491   |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 7           |
|    time_elapsed         | 64          |
|    total_timesteps      | 5234688     |
| train/                  |             |
|    approx_kl            | 0.016425733 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.531      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0333     |
|    std                  | 3.65        |
|    value_loss           | 0.00867     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.85e+03    |
|    ep_rew_mean          | 3.528416    |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 8           |
|    time_elapsed         | 74          |
|    total_timesteps      | 5238784     |
| train/                  |             |
|    approx_kl            | 0.017625097 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.541      |
|    n_updates            | 1845        |
|    policy_gradient_loss | -0.0311     |
|    std                  | 3.68        |
|    value_loss           | 0.00508     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.85e+03   |
|    ep_rew_mean          | 3.528416   |
| time/                   |            |
|    fps                  | 442        |
|    iterations           | 9          |
|    time_elapsed         | 83         |
|    total_timesteps      | 5242880    |
| train/                  |            |
|    approx_kl            | 0.02326081 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.5      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.000243   |
|    loss                 | -0.545     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0464    |
|    std                  | 3.7        |
|    value_loss           | 0.0281     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.95e+03   |
|    ep_rew_mean          | 3.4768662  |
| time/                   |            |
|    fps                  | 442        |
|    iterations           | 10         |
|    time_elapsed         | 92         |
|    total_timesteps      | 5246976    |
| train/                  |            |
|    approx_kl            | 0.01789015 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.6      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.000243   |
|    loss                 | -0.552     |
|    n_updates            | 1875       |
|    policy_gradient_loss | -0.0413    |
|    std                  | 3.72       |
|    value_loss           | 0.0146     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3.4768662   |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 11          |
|    time_elapsed         | 101         |
|    total_timesteps      | 5251072     |
| train/                  |             |
|    approx_kl            | 0.020091552 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.544      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0415     |
|    std                  | 3.75        |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.95e+03    |
|    ep_rew_mean          | 3.4768662   |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 12          |
|    time_elapsed         | 111         |
|    total_timesteps      | 5255168     |
| train/                  |             |
|    approx_kl            | 0.019042559 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.541      |
|    n_updates            | 1905        |
|    policy_gradient_loss | -0.0393     |
|    std                  | 3.78        |
|    value_loss           | 0.0152      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 3.4354186   |
| time/                   |             |
|    fps                  | 443         |
|    iterations           | 13          |
|    time_elapsed         | 120         |
|    total_timesteps      | 5259264     |
| train/                  |             |
|    approx_kl            | 0.016905032 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.8       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.000243    |
|    loss                 | -0.538      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0348     |
|    std                  | 3.81        |
|    value_loss           | 0.00483     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_3.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=65.252, success=False
episode 2: length=10001, reward=65.252, success=False
episode 3: length=10001, reward=65.252, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 65.252
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 63.719
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 76.937
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 11.794
total data points collected: 50005
average episode length: 10001.0
average episode reward: 39.724
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1477', '7.0869', '7.8509', '7.8865', '8.8983']
Training reward models...
Reward model losses: ['0.2264', '0.0153', '0.2134', '0.0093', '0.0134']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2594', '1.2916', '1.1920', '1.2612', '1.0595']
Training action VAE models...
Action VAE losses: ['1.5078', '1.4727', '1.3288', '1.4430', '1.3743']
CM score components:
transition disagreement: 0.4616
reward disagreement: 0.1147
state disagreement: 0.4818
action disagreement: 0.5826
total CM score: 1.6407
goal is complete. CM score: 1.6407
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 55.022
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 60.906
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 70.781
total data points collected: 50005
average episode length: 10001.0
average episode reward: 43.265
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4717', '7.2688', '7.6066', '8.6317', '7.9732']
Training reward models...
Reward model losses: ['0.0127', '0.1032', '1.7585', '0.2422', '0.1833']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2474', '1.3861', '1.2672', '1.1441', '1.1564']
Training action VAE models...
Action VAE losses: ['1.3926', '1.4256', '1.3042', '1.3677', '1.3748']
CM score components:
transition disagreement: 0.4242
reward disagreement: 0.4461
state disagreement: 0.5157
action disagreement: 0.5459
total CM score: 1.9319
mass is complete. CM score: 1.9319
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -1.778
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 35.580
Reset #3: friction intervention applied (success: True)
episode 3: 51 steps, reward: -1.751
total data points collected: 40055
average episode length: 8011.0
average episode reward: 6.208
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40055, 56]), actions: torch.Size([40055, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2355', '7.6942', '6.9814', '7.6400', '6.3654']
Training reward models...
Reward model losses: ['0.0158', '0.0198', '0.1424', '0.0459', '0.2570']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4532', '1.4134', '1.3463', '1.6886', '1.3530']
Training action VAE models...
Action VAE losses: ['1.4406', '1.3883', '1.3469', '1.2648', '1.5076']
CM score components:
transition disagreement: 0.4379
reward disagreement: 0.1314
state disagreement: 0.5164
action disagreement: 0.5395
total CM score: 1.6252
friction is complete. CM score: 1.6252
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 65.252
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 65.252
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 65.252
total data points collected: 50005
average episode length: 10001.0
average episode reward: 65.252
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9388', '7.5568', '7.2615', '6.8891', '7.6120']
Training reward models...
Reward model losses: ['0.0051', '0.0548', '0.3723', '0.3370', '0.1154']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0581', '1.0933', '1.1353', '1.2138', '1.0726']
Training action VAE models...
Action VAE losses: ['1.6054', '1.4388', '1.4625', '1.3860', '1.4454']
CM score components:
transition disagreement: 0.4341
reward disagreement: 0.1830
state disagreement: 0.4998
action disagreement: 0.5822
total CM score: 1.6990
visual is complete. CM score: 1.6990
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 104.028
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 8.183
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 5.935
total data points collected: 40028
average episode length: 8005.6
average episode reward: 26.537
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5287', '7.3663', '7.5664', '7.3902', '7.3819']
Training reward models...
Reward model losses: ['0.2487', '0.0408', '0.3130', '0.0171', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2645', '1.3292', '1.3511', '1.4469', '1.2422']
Training action VAE models...
Action VAE losses: ['1.2436', '1.3852', '1.3877', '1.4476', '1.4535']
CM score components:
transition disagreement: 0.4285
reward disagreement: 0.1562
state disagreement: 0.4945
action disagreement: 0.5234
total CM score: 1.6026
pose is complete. CM score: 1.6026
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -3.320
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -4.841
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.202
total data points collected: 50005
average episode length: 10001.0
average episode reward: 12.459
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6735', '7.7176', '8.4033', '7.6925', '7.5950']
Training reward models...
Reward model losses: ['0.0183', '0.0119', '0.2802', '0.0231', '0.2559']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1351', '1.2378', '1.2225', '1.1717', '1.1195']
Training action VAE models...
Action VAE losses: ['1.4499', '1.5051', '1.3656', '1.3627', '1.4712']
CM score components:
transition disagreement: 0.4275
reward disagreement: 0.1577
state disagreement: 0.4839
action disagreement: 0.5629
total CM score: 1.6319
random is complete. CM score: 1.6319
2025-07-16 17:34:48,091 3266759 INFO Meta-Episode 4/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_3.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 63.719
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 76.937
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 11.794
total data points collected: 50005
average episode length: 10001.0
average episode reward: 39.724
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2894', '7.1865', '6.7584', '7.0259', '7.6575']
Training reward models...
Reward model losses: ['0.1726', '0.2100', '0.2785', '0.0061', '0.0174']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1482', '1.2159', '1.2394', '1.2347', '1.1661']
Training action VAE models...
Action VAE losses: ['1.5478', '1.4001', '1.4491', '1.3186', '1.3426']
CM score components:
transition disagreement: 0.4158
reward disagreement: 0.0692
state disagreement: 0.4622
action disagreement: 0.5602
total CM score: 1.5074
goal is complete. CM score: 1.5074
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 55.022
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 60.906
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 70.781
total data points collected: 50005
average episode length: 10001.0
average episode reward: 43.265
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0865', '7.7311', '7.0105', '6.8454', '8.3373']
Training reward models...
Reward model losses: ['0.0161', '0.1621', '0.0125', '0.5506', '0.5077']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2247', '1.0583', '1.0916', '1.1708', '1.3561']
Training action VAE models...
Action VAE losses: ['1.4091', '1.4674', '1.3240', '1.4493', '1.3578']
CM score components:
transition disagreement: 0.4209
reward disagreement: 0.1818
state disagreement: 0.5111
action disagreement: 0.5466
total CM score: 1.6604
mass is complete. CM score: 1.6604
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -1.778
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 35.580
Reset #3: friction intervention applied (success: True)
episode 3: 51 steps, reward: -1.751
total data points collected: 40055
average episode length: 8011.0
average episode reward: 6.208
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40055, 56]), actions: torch.Size([40055, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9660', '7.3404', '6.8044', '7.7058', '7.1211']
Training reward models...
Reward model losses: ['0.0742', '0.7587', '0.4172', '0.0334', '0.3072']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4047', '1.4265', '1.4933', '1.4112', '1.3682']
Training action VAE models...
Action VAE losses: ['1.2720', '1.3412', '1.2750', '1.2543', '1.4121']
CM score components:
transition disagreement: 0.4265
reward disagreement: 0.3334
state disagreement: 0.5196
action disagreement: 0.5240
total CM score: 1.8035
friction is complete. CM score: 1.8035
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 65.252
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 65.252
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 65.252
total data points collected: 50005
average episode length: 10001.0
average episode reward: 65.252
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2207', '7.8128', '7.1343', '7.1455', '7.5500']
Training reward models...
Reward model losses: ['0.0455', '0.4620', '0.0527', '0.0219', '0.0738']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1478', '1.1271', '1.1397', '1.2171', '1.1151']
Training action VAE models...
Action VAE losses: ['1.4247', '1.3666', '1.3233', '1.4028', '1.5083']
CM score components:
transition disagreement: 0.4411
reward disagreement: 0.2021
state disagreement: 0.4807
action disagreement: 0.5374
total CM score: 1.6613
visual is complete. CM score: 1.6613
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 104.028
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 8.183
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 5.935
total data points collected: 40028
average episode length: 8005.6
average episode reward: 26.537
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7552', '7.8038', '7.7059', '6.6431', '7.4792']
Training reward models...
Reward model losses: ['2.9199', '0.9684', '0.1324', '0.0106', '0.0306']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3618', '1.5109', '1.3711', '1.3718', '1.2446']
Training action VAE models...
Action VAE losses: ['1.3648', '1.4900', '1.4005', '1.4976', '1.3772']
CM score components:
transition disagreement: 0.4368
reward disagreement: 0.7072
state disagreement: 0.5059
action disagreement: 0.5537
total CM score: 2.2036
pose is complete. CM score: 2.2036
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -3.320
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -4.841
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.202
total data points collected: 50005
average episode length: 10001.0
average episode reward: 12.459
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4887', '7.3083', '8.3489', '7.4018', '7.3210']
Training reward models...
Reward model losses: ['0.0325', '0.0265', '0.0264', '0.1611', '0.1541']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1404', '1.1153', '1.1680', '1.1529', '1.2004']
Training action VAE models...
Action VAE losses: ['1.4311', '1.3433', '1.4450', '1.4590', '1.3987']
CM score components:
transition disagreement: 0.3799
reward disagreement: 0.1002
state disagreement: 0.4845
action disagreement: 0.5833
total CM score: 1.5479
random is complete. CM score: 1.5479
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.04e+03  |
|    ep_rew_mean     | 3.4072592 |
| time/              |           |
|    fps             | 495       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5263360   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.04e+03    |
|    ep_rew_mean          | 3.4072592   |
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 2           |
|    time_elapsed         | 17          |
|    total_timesteps      | 5267456     |
| train/                  |             |
|    approx_kl            | 0.019437663 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.537      |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0384     |
|    std                  | 3.86        |
|    value_loss           | 0.0264      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3.329187    |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 3           |
|    time_elapsed         | 26          |
|    total_timesteps      | 5271552     |
| train/                  |             |
|    approx_kl            | 0.019780383 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.54       |
|    n_updates            | 1965        |
|    policy_gradient_loss | -0.0346     |
|    std                  | 3.89        |
|    value_loss           | 0.0171      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.13e+03    |
|    ep_rew_mean          | 3.329187    |
| time/                   |             |
|    fps                  | 458         |
|    iterations           | 4           |
|    time_elapsed         | 35          |
|    total_timesteps      | 5275648     |
| train/                  |             |
|    approx_kl            | 0.017696638 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.000219    |
|    loss                 | -0.542      |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0375     |
|    std                  | 3.91        |
|    value_loss           | 0.0215      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.23e+03   |
|    ep_rew_mean          | 3.3252096  |
| time/                   |            |
|    fps                  | 453        |
|    iterations           | 5          |
|    time_elapsed         | 45         |
|    total_timesteps      | 5279744    |
| train/                  |            |
|    approx_kl            | 0.01437509 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.1      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.000219   |
|    loss                 | -0.528     |
|    n_updates            | 1995       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 3.94       |
|    value_loss           | 0.00778    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 3.3234475   |
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 6           |
|    time_elapsed         | 54          |
|    total_timesteps      | 5283840     |
| train/                  |             |
|    approx_kl            | 0.016130924 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.1       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.535      |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0277     |
|    std                  | 3.96        |
|    value_loss           | 0.00411     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.23e+03    |
|    ep_rew_mean          | 3.3234475   |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 7           |
|    time_elapsed         | 64          |
|    total_timesteps      | 5287936     |
| train/                  |             |
|    approx_kl            | 0.016483221 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.000219    |
|    loss                 | -0.538      |
|    n_updates            | 2025        |
|    policy_gradient_loss | -0.0316     |
|    std                  | 3.99        |
|    value_loss           | 0.00857     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.33e+03    |
|    ep_rew_mean          | 3.2909756   |
| time/                   |             |
|    fps                  | 443         |
|    iterations           | 8           |
|    time_elapsed         | 73          |
|    total_timesteps      | 5292032     |
| train/                  |             |
|    approx_kl            | 0.013202697 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.532      |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0242     |
|    std                  | 4.01        |
|    value_loss           | 0.00409     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.33e+03     |
|    ep_rew_mean          | 3.2909756    |
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 9            |
|    time_elapsed         | 83           |
|    total_timesteps      | 5296128      |
| train/                  |              |
|    approx_kl            | 0.0143510485 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    entropy_loss         | -25.3        |
|    explained_variance   | 0.695        |
|    learning_rate        | 0.000219     |
|    loss                 | -0.543       |
|    n_updates            | 2055         |
|    policy_gradient_loss | -0.0313      |
|    std                  | 4.03         |
|    value_loss           | 0.0189       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 3.260923    |
| time/                   |             |
|    fps                  | 441         |
|    iterations           | 10          |
|    time_elapsed         | 92          |
|    total_timesteps      | 5300224     |
| train/                  |             |
|    approx_kl            | 0.013452409 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.547      |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0299     |
|    std                  | 4.05        |
|    value_loss           | 0.00433     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 3.260923    |
| time/                   |             |
|    fps                  | 441         |
|    iterations           | 11          |
|    time_elapsed         | 102         |
|    total_timesteps      | 5304320     |
| train/                  |             |
|    approx_kl            | 0.016689634 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.4       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.545      |
|    n_updates            | 2085        |
|    policy_gradient_loss | -0.0297     |
|    std                  | 4.08        |
|    value_loss           | 0.00644     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.42e+03    |
|    ep_rew_mean          | 3.260923    |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 12          |
|    time_elapsed         | 111         |
|    total_timesteps      | 5308416     |
| train/                  |             |
|    approx_kl            | 0.014822353 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.5       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.543      |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0285     |
|    std                  | 4.12        |
|    value_loss           | 0.00803     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 3.303642    |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 13          |
|    time_elapsed         | 120         |
|    total_timesteps      | 5312512     |
| train/                  |             |
|    approx_kl            | 0.013595171 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.5       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.000219    |
|    loss                 | -0.543      |
|    n_updates            | 2115        |
|    policy_gradient_loss | -0.025      |
|    std                  | 4.14        |
|    value_loss           | 0.0157      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_4.zip

evaluating student performance (5 episodes)...
episode 1: length=167, reward=2.006, success=True
episode 2: length=167, reward=2.006, success=True
episode 3: length=167, reward=2.006, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 2.006
average episode length: 167.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 53.646
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 25.176
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 21.236
total data points collected: 50005
average episode length: 10001.0
average episode reward: 25.868
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1901', '7.1168', '7.8517', '7.8238', '8.8845']
Training reward models...
Reward model losses: ['0.2446', '0.0129', '0.2591', '0.0068', '0.0147']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2657', '1.3520', '1.3059', '1.2821', '1.0900']
Training action VAE models...
Action VAE losses: ['1.4562', '1.5155', '1.3350', '1.4942', '1.4226']
CM score components:
transition disagreement: 0.4513
reward disagreement: 0.1464
state disagreement: 0.4872
action disagreement: 0.5844
total CM score: 1.6694
goal is complete. CM score: 1.6694
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 0.255
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 0.145
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.369
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.222
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4650', '7.2442', '7.6685', '8.5316', '7.9472']
Training reward models...
Reward model losses: ['0.0109', '0.0921', '1.6745', '0.2256', '0.1252']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1930', '1.3241', '1.2511', '1.1082', '1.1764']
Training action VAE models...
Action VAE losses: ['1.3290', '1.3734', '1.2654', '1.2695', '1.2971']
CM score components:
transition disagreement: 0.4259
reward disagreement: 0.4378
state disagreement: 0.5081
action disagreement: 0.5503
total CM score: 1.9221
mass is complete. CM score: 1.9221
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 34.162
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 55.095
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 43.826
total data points collected: 50005
average episode length: 10001.0
average episode reward: 53.545
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2333', '7.5826', '6.9359', '7.6453', '6.3970']
Training reward models...
Reward model losses: ['0.0082', '0.0197', '0.1614', '0.0290', '0.2175']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2141', '1.2087', '1.2595', '1.3391', '1.2325']
Training action VAE models...
Action VAE losses: ['1.5259', '1.3968', '1.3763', '1.2563', '1.5115']
CM score components:
transition disagreement: 0.4370
reward disagreement: 0.1353
state disagreement: 0.4908
action disagreement: 0.5409
total CM score: 1.6040
friction is complete. CM score: 1.6040
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 167 steps, reward: 2.006
Reset #2: visual intervention applied (success: True)
episode 2: 167 steps, reward: 2.006
Reset #3: visual intervention applied (success: True)
episode 3: 167 steps, reward: 2.006
total data points collected: 835
average episode length: 167.0
average episode reward: 2.006
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([835, 56]), actions: torch.Size([835, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9284', '7.5497', '7.2775', '6.8091', '7.5815']
Training reward models...
Reward model losses: ['0.0041', '0.0537', '0.2679', '0.2394', '0.1788']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5113', '1.5680', '1.7006', '1.7659', '1.5284']
Training action VAE models...
Action VAE losses: ['1.6008', '1.4432', '1.4730', '1.4629', '1.4579']
CM score components:
transition disagreement: 0.4241
reward disagreement: 0.1505
state disagreement: 0.5580
action disagreement: 0.5853
total CM score: 1.7179
visual is complete. CM score: 1.7179
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: -3.318
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 0.874
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -2.887
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.586
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5126', '7.4036', '7.5687', '7.3755', '7.3639']
Training reward models...
Reward model losses: ['0.2249', '0.0418', '0.2475', '0.0157', '0.0160']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1092', '1.2178', '1.2728', '1.2521', '1.1393']
Training action VAE models...
Action VAE losses: ['1.2257', '1.3304', '1.3586', '1.4017', '1.4111']
CM score components:
transition disagreement: 0.4261
reward disagreement: 0.1377
state disagreement: 0.4810
action disagreement: 0.5238
total CM score: 1.5686
pose is complete. CM score: 1.5686
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 22.163
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.322
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.267
total data points collected: 40082
average episode length: 8016.4
average episode reward: 4.499
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40082, 56]), actions: torch.Size([40082, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7206', '7.6615', '8.3781', '7.7429', '7.5341']
Training reward models...
Reward model losses: ['0.0165', '0.0143', '0.2732', '0.0186', '0.2278']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2427', '1.3108', '1.3338', '1.2727', '1.2485']
Training action VAE models...
Action VAE losses: ['1.4754', '1.5363', '1.3912', '1.4053', '1.5023']
CM score components:
transition disagreement: 0.4238
reward disagreement: 0.1480
state disagreement: 0.4945
action disagreement: 0.5672
total CM score: 1.6335
random is complete. CM score: 1.6335
2025-07-16 17:52:52,446 3266759 INFO Meta-Episode 5/30: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_4.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 53.646
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 25.176
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 21.236
total data points collected: 50005
average episode length: 10001.0
average episode reward: 25.868
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3087', '7.1615', '6.8095', '7.1002', '7.6247']
Training reward models...
Reward model losses: ['0.2003', '0.2263', '0.2348', '0.0112', '0.0176']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1767', '1.2195', '1.3208', '1.2247', '1.2307']
Training action VAE models...
Action VAE losses: ['1.5292', '1.4305', '1.5572', '1.3035', '1.3516']
CM score components:
transition disagreement: 0.4123
reward disagreement: 0.0733
state disagreement: 0.4612
action disagreement: 0.5605
total CM score: 1.5073
goal is complete. CM score: 1.5073
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 0.255
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 0.145
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.369
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.222
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0738', '7.6736', '7.0539', '6.8162', '8.2852']
Training reward models...
Reward model losses: ['0.0036', '0.2371', '0.0114', '0.3832', '0.5914']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2673', '1.0621', '1.0901', '1.1883', '1.2815']
Training action VAE models...
Action VAE losses: ['1.3531', '1.4113', '1.3288', '1.3247', '1.2501']
CM score components:
transition disagreement: 0.4191
reward disagreement: 0.1904
state disagreement: 0.5131
action disagreement: 0.5483
total CM score: 1.6708
mass is complete. CM score: 1.6708
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 34.162
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 55.095
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 43.826
total data points collected: 50005
average episode length: 10001.0
average episode reward: 53.545
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9178', '7.2889', '6.7929', '7.7099', '7.0737']
Training reward models...
Reward model losses: ['0.0647', '0.9832', '0.4499', '0.0179', '0.2253']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2983', '1.2396', '1.2777', '1.3324', '1.1904']
Training action VAE models...
Action VAE losses: ['1.2892', '1.4291', '1.3371', '1.3176', '1.4725']
CM score components:
transition disagreement: 0.4185
reward disagreement: 0.3967
state disagreement: 0.5089
action disagreement: 0.5293
total CM score: 1.8535
friction is complete. CM score: 1.8535
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 167 steps, reward: 2.006
Reset #2: visual intervention applied (success: True)
episode 2: 167 steps, reward: 2.006
Reset #3: visual intervention applied (success: True)
episode 3: 167 steps, reward: 2.006
total data points collected: 835
average episode length: 167.0
average episode reward: 2.006
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([835, 56]), actions: torch.Size([835, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1547', '7.8014', '7.0707', '7.1541', '7.4952']
Training reward models...
Reward model losses: ['0.0484', '0.3710', '0.0744', '0.0186', '0.0629']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7990', '1.5479', '1.5970', '1.7455', '1.6368']
Training action VAE models...
Action VAE losses: ['1.3777', '1.3077', '1.3408', '1.4552', '1.5202']
CM score components:
transition disagreement: 0.4414
reward disagreement: 0.1747
state disagreement: 0.5344
action disagreement: 0.5389
total CM score: 1.6894
visual is complete. CM score: 1.6894
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: -3.318
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 0.874
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -2.887
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.586
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7539', '7.7904', '7.7105', '6.6170', '7.5132']
Training reward models...
Reward model losses: ['2.7853', '0.9310', '0.1088', '0.0090', '0.0301']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2075', '1.2696', '1.1702', '1.1612', '1.1551']
Training action VAE models...
Action VAE losses: ['1.3402', '1.4668', '1.3740', '1.4457', '1.3205']
CM score components:
transition disagreement: 0.4374
reward disagreement: 0.6877
state disagreement: 0.4904
action disagreement: 0.5630
total CM score: 2.1784
pose is complete. CM score: 2.1784
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 22.163
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.322
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.267
total data points collected: 40082
average episode length: 8016.4
average episode reward: 4.499
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40082, 56]), actions: torch.Size([40082, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4745', '7.3096', '8.3316', '7.3947', '7.2909']
Training reward models...
Reward model losses: ['0.0591', '0.0292', '0.0214', '0.2429', '0.1404']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2417', '1.2665', '1.2812', '1.2243', '1.3205']
Training action VAE models...
Action VAE losses: ['1.4194', '1.3950', '1.4348', '1.5153', '1.4565']
CM score components:
transition disagreement: 0.3765
reward disagreement: 0.1268
state disagreement: 0.4964
action disagreement: 0.5774
total CM score: 1.5770
random is complete. CM score: 1.5770
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.52e+03 |
|    ep_rew_mean     | 3.303642 |
| time/              |          |
|    fps             | 462      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 5316608  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.52e+03    |
|    ep_rew_mean          | 3.303642    |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5320704     |
| train/                  |             |
|    approx_kl            | 0.017150234 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.543      |
|    n_updates            | 2145        |
|    policy_gradient_loss | -0.0367     |
|    std                  | 4.18        |
|    value_loss           | 0.0375      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.62e+03    |
|    ep_rew_mean          | 3.2570682   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.016450543 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.7       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.556      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0358     |
|    std                  | 4.19        |
|    value_loss           | 0.0259      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.62e+03    |
|    ep_rew_mean          | 3.2570682   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 4           |
|    time_elapsed         | 37          |
|    total_timesteps      | 5328896     |
| train/                  |             |
|    approx_kl            | 0.016232261 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.7       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.556      |
|    n_updates            | 2175        |
|    policy_gradient_loss | -0.0373     |
|    std                  | 4.22        |
|    value_loss           | 0.0181      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.72e+03    |
|    ep_rew_mean          | 3.2443833   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 5           |
|    time_elapsed         | 46          |
|    total_timesteps      | 5332992     |
| train/                  |             |
|    approx_kl            | 0.016341913 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.8       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.555      |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0286     |
|    std                  | 4.24        |
|    value_loss           | 0.00459     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.72e+03    |
|    ep_rew_mean          | 3.2443833   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 6           |
|    time_elapsed         | 56          |
|    total_timesteps      | 5337088     |
| train/                  |             |
|    approx_kl            | 0.017060967 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.8       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.562      |
|    n_updates            | 2205        |
|    policy_gradient_loss | -0.0356     |
|    std                  | 4.27        |
|    value_loss           | 0.00754     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.72e+03    |
|    ep_rew_mean          | 3.2443833   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5341184     |
| train/                  |             |
|    approx_kl            | 0.015688736 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.8       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.000197    |
|    loss                 | -0.561      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0311     |
|    std                  | 4.29        |
|    value_loss           | 0.00929     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.81e+03    |
|    ep_rew_mean          | 3.1731253   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 8           |
|    time_elapsed         | 74          |
|    total_timesteps      | 5345280     |
| train/                  |             |
|    approx_kl            | 0.016157655 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.54       |
|    n_updates            | 2235        |
|    policy_gradient_loss | -0.0276     |
|    std                  | 4.31        |
|    value_loss           | 0.012       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.81e+03   |
|    ep_rew_mean          | 3.1731253  |
| time/                   |            |
|    fps                  | 438        |
|    iterations           | 9          |
|    time_elapsed         | 84         |
|    total_timesteps      | 5349376    |
| train/                  |            |
|    approx_kl            | 0.01685476 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.9      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.000197   |
|    loss                 | -0.569     |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0351    |
|    std                  | 4.33       |
|    value_loss           | 0.00986    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.91e+03    |
|    ep_rew_mean          | 3.1377158   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 5353472     |
| train/                  |             |
|    approx_kl            | 0.013167568 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26         |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.568      |
|    n_updates            | 2265        |
|    policy_gradient_loss | -0.028      |
|    std                  | 4.34        |
|    value_loss           | 0.00361     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 2.91e+03    |
|    ep_rew_mean          | 3.1377158   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 11          |
|    time_elapsed         | 102         |
|    total_timesteps      | 5357568     |
| train/                  |             |
|    approx_kl            | 0.019503972 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26         |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.561      |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0346     |
|    std                  | 4.36        |
|    value_loss           | 0.014       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 2.91e+03   |
|    ep_rew_mean          | 3.1377158  |
| time/                   |            |
|    fps                  | 441        |
|    iterations           | 12         |
|    time_elapsed         | 111        |
|    total_timesteps      | 5361664    |
| train/                  |            |
|    approx_kl            | 0.01685671 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.1      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.000197   |
|    loss                 | -0.567     |
|    n_updates            | 2295       |
|    policy_gradient_loss | -0.0316    |
|    std                  | 4.39       |
|    value_loss           | 0.0153     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.01e+03    |
|    ep_rew_mean          | 3.1070173   |
| time/                   |             |
|    fps                  | 441         |
|    iterations           | 13          |
|    time_elapsed         | 120         |
|    total_timesteps      | 5365760     |
| train/                  |             |
|    approx_kl            | 0.016415033 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.1       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.000197    |
|    loss                 | -0.558      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0298     |
|    std                  | 4.41        |
|    value_loss           | 0.00564     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_5.zip

evaluating student performance (5 episodes)...
episode 1: length=100, reward=1.418, success=True
episode 2: length=100, reward=1.418, success=True
episode 3: length=100, reward=1.418, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 1.418
average episode length: 100.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 71.905
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 46.397
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 8.139
total data points collected: 40031
average episode length: 8006.2
average episode reward: 30.789
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1699', '7.1085', '7.8250', '7.8094', '8.8641']
Training reward models...
Reward model losses: ['0.2510', '0.0138', '0.2651', '0.0029', '0.0172']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1199', '1.1859', '1.1389', '1.1253', '0.9676']
Training action VAE models...
Action VAE losses: ['1.5136', '1.4322', '1.3680', '1.4503', '1.4015']
CM score components:
transition disagreement: 0.4461
reward disagreement: 0.1497
state disagreement: 0.4725
action disagreement: 0.5867
total CM score: 1.6549
goal is complete. CM score: 1.6549
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 285 steps, reward: 2.702
Reset #2: mass intervention applied (success: True)
episode 2: 251 steps, reward: 1.544
Reset #3: mass intervention applied (success: True)
episode 3: 205 steps, reward: 1.843
total data points collected: 10928
average episode length: 2185.6
average episode reward: 21.859
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([10928, 56]), actions: torch.Size([10928, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4456', '7.2446', '7.6621', '8.5583', '7.9409']
Training reward models...
Reward model losses: ['0.0070', '0.1008', '1.6165', '0.2234', '0.1562']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7441', '0.6931', '0.7845', '0.6538', '0.6831']
Training action VAE models...
Action VAE losses: ['1.3399', '1.4134', '1.2647', '1.2930', '1.3188']
CM score components:
transition disagreement: 0.4212
reward disagreement: 0.4327
state disagreement: 0.4536
action disagreement: 0.5516
total CM score: 1.8592
mass is complete. CM score: 1.8592
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 110 steps, reward: 1.594
Reset #2: friction intervention applied (success: True)
episode 2: 137 steps, reward: 1.663
Reset #3: friction intervention applied (success: True)
episode 3: 131 steps, reward: 1.589
total data points collected: 658
average episode length: 131.6
average episode reward: 1.683
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([658, 56]), actions: torch.Size([658, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2180', '7.6194', '6.9669', '7.6718', '6.4219']
Training reward models...
Reward model losses: ['0.0152', '0.0281', '0.1607', '0.0297', '0.2482']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4380', '1.3653', '1.3058', '1.3624', '1.5474']
Training action VAE models...
Action VAE losses: ['1.4940', '1.4477', '1.3572', '1.2623', '1.4984']
CM score components:
transition disagreement: 0.4381
reward disagreement: 0.1567
state disagreement: 0.5297
action disagreement: 0.5465
total CM score: 1.6710
friction is complete. CM score: 1.6710
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 100 steps, reward: 1.418
Reset #2: visual intervention applied (success: True)
episode 2: 100 steps, reward: 1.418
Reset #3: visual intervention applied (success: True)
episode 3: 100 steps, reward: 1.418
total data points collected: 500
average episode length: 100.0
average episode reward: 1.418
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([500, 56]), actions: torch.Size([500, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9120', '7.5579', '7.2724', '6.7946', '7.5879']
Training reward models...
Reward model losses: ['0.0071', '0.0551', '0.2733', '0.2455', '0.1698']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4062', '1.4916', '1.2001', '1.4360', '1.3815']
Training action VAE models...
Action VAE losses: ['1.6006', '1.4684', '1.4753', '1.4660', '1.4580']
CM score components:
transition disagreement: 0.4256
reward disagreement: 0.1498
state disagreement: 0.5307
action disagreement: 0.5847
total CM score: 1.6908
visual is complete. CM score: 1.6908
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 80 steps, reward: 0.975
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 33.192
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 15.248
total data points collected: 30113
average episode length: 6022.6
average episode reward: 13.363
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30113, 56]), actions: torch.Size([30113, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4628', '7.3699', '7.5440', '7.3180', '7.3132']
Training reward models...
Reward model losses: ['0.2366', '0.0394', '0.2764', '0.0147', '0.0161']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0378', '1.1092', '1.1377', '1.1368', '1.0336']
Training action VAE models...
Action VAE losses: ['1.3001', '1.3745', '1.4017', '1.4107', '1.4726']
CM score components:
transition disagreement: 0.4252
reward disagreement: 0.1530
state disagreement: 0.4687
action disagreement: 0.5195
total CM score: 1.5664
pose is complete. CM score: 1.5664
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 16.106
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.300
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.193
total data points collected: 50005
average episode length: 10001.0
average episode reward: 4.747
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7158', '7.6468', '8.3793', '7.7477', '7.4917']
Training reward models...
Reward model losses: ['0.0207', '0.0161', '0.2790', '0.0250', '0.1760']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1629', '1.2714', '1.2661', '1.2395', '1.2300']
Training action VAE models...
Action VAE losses: ['1.4249', '1.5680', '1.4340', '1.4099', '1.4991']
CM score components:
transition disagreement: 0.4221
reward disagreement: 0.1470
state disagreement: 0.4908
action disagreement: 0.5653
total CM score: 1.6253
random is complete. CM score: 1.6253
2025-07-16 18:06:53,902 3266759 INFO Meta-Episode 6/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_5.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 71.905
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 46.397
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 8.139
total data points collected: 40031
average episode length: 8006.2
average episode reward: 30.789
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2491', '7.1176', '6.7893', '7.0930', '7.6334']
Training reward models...
Reward model losses: ['0.1816', '0.2241', '0.2394', '0.0027', '0.0140']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0115', '1.1027', '1.0733', '1.0652', '1.0628']
Training action VAE models...
Action VAE losses: ['1.4529', '1.4473', '1.5055', '1.3570', '1.3725']
CM score components:
transition disagreement: 0.4084
reward disagreement: 0.0556
state disagreement: 0.4446
action disagreement: 0.5613
total CM score: 1.4699
goal is complete. CM score: 1.4699
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 285 steps, reward: 2.702
Reset #2: mass intervention applied (success: True)
episode 2: 251 steps, reward: 1.544
Reset #3: mass intervention applied (success: True)
episode 3: 205 steps, reward: 1.843
total data points collected: 10928
average episode length: 2185.6
average episode reward: 21.859
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([10928, 56]), actions: torch.Size([10928, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0565', '7.6460', '7.0143', '6.8263', '8.2772']
Training reward models...
Reward model losses: ['0.0035', '0.2342', '0.0093', '0.3930', '0.5541']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7279', '0.6623', '0.6789', '0.7443', '0.7832']
Training action VAE models...
Action VAE losses: ['1.3727', '1.3880', '1.2861', '1.3367', '1.2373']
CM score components:
transition disagreement: 0.4216
reward disagreement: 0.1805
state disagreement: 0.4665
action disagreement: 0.5479
total CM score: 1.6164
mass is complete. CM score: 1.6164
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 110 steps, reward: 1.594
Reset #2: friction intervention applied (success: True)
episode 2: 137 steps, reward: 1.663
Reset #3: friction intervention applied (success: True)
episode 3: 131 steps, reward: 1.589
total data points collected: 658
average episode length: 131.6
average episode reward: 1.683
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([658, 56]), actions: torch.Size([658, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9082', '7.2848', '6.8303', '7.6937', '7.0856']
Training reward models...
Reward model losses: ['0.0595', '1.0320', '0.4829', '0.0204', '0.2588']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4310', '1.3643', '1.4191', '1.3655', '1.4039']
Training action VAE models...
Action VAE losses: ['1.2687', '1.3577', '1.3256', '1.2733', '1.4062']
CM score components:
transition disagreement: 0.4166
reward disagreement: 0.4178
state disagreement: 0.5084
action disagreement: 0.5199
total CM score: 1.8627
friction is complete. CM score: 1.8627
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 100 steps, reward: 1.418
Reset #2: visual intervention applied (success: True)
episode 2: 100 steps, reward: 1.418
Reset #3: visual intervention applied (success: True)
episode 3: 100 steps, reward: 1.418
total data points collected: 500
average episode length: 100.0
average episode reward: 1.418
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([500, 56]), actions: torch.Size([500, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1464', '7.7891', '7.0844', '7.1730', '7.5036']
Training reward models...
Reward model losses: ['0.0574', '0.3499', '0.0861', '0.0212', '0.0699']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3363', '1.2738', '1.4133', '1.1602', '1.2502']
Training action VAE models...
Action VAE losses: ['1.3577', '1.2977', '1.3274', '1.4525', '1.4902']
CM score components:
transition disagreement: 0.4409
reward disagreement: 0.1684
state disagreement: 0.4887
action disagreement: 0.5334
total CM score: 1.6315
visual is complete. CM score: 1.6315
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 80 steps, reward: 0.975
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 33.192
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 15.248
total data points collected: 30113
average episode length: 6022.6
average episode reward: 13.363
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30113, 56]), actions: torch.Size([30113, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7510', '7.7412', '7.6640', '6.5910', '7.4161']
Training reward models...
Reward model losses: ['2.8605', '0.9539', '0.1495', '0.0122', '0.0292']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1550', '1.2136', '1.1315', '1.1045', '1.0749']
Training action VAE models...
Action VAE losses: ['1.3856', '1.5083', '1.3934', '1.4750', '1.3592']
CM score components:
transition disagreement: 0.4354
reward disagreement: 0.6977
state disagreement: 0.4916
action disagreement: 0.5603
total CM score: 2.1850
pose is complete. CM score: 2.1850
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 16.106
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.300
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.193
total data points collected: 50005
average episode length: 10001.0
average episode reward: 4.747
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4862', '7.2994', '8.3047', '7.3897', '7.3036']
Training reward models...
Reward model losses: ['0.0461', '0.0296', '0.0273', '0.2591', '0.1459']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2300', '1.1897', '1.2018', '1.2169', '1.2506']
Training action VAE models...
Action VAE losses: ['1.4687', '1.3919', '1.4106', '1.4720', '1.4559']
CM score components:
transition disagreement: 0.3756
reward disagreement: 0.1231
state disagreement: 0.4879
action disagreement: 0.5792
total CM score: 1.5658
random is complete. CM score: 1.5658
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.01e+03  |
|    ep_rew_mean     | 3.1070173 |
| time/              |           |
|    fps             | 470       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5369856   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.01e+03    |
|    ep_rew_mean          | 3.1070173   |
| time/                   |             |
|    fps                  | 449         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5373952     |
| train/                  |             |
|    approx_kl            | 0.014363137 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.2       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.567      |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0323     |
|    std                  | 4.47        |
|    value_loss           | 0.017       |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.1e+03     |
|    ep_rew_mean          | 3.1967623   |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5378048     |
| train/                  |             |
|    approx_kl            | 0.015102911 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.3       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.58       |
|    n_updates            | 2355        |
|    policy_gradient_loss | -0.0275     |
|    std                  | 4.5         |
|    value_loss           | 0.00518     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.1e+03      |
|    ep_rew_mean          | 3.1967623    |
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 4            |
|    time_elapsed         | 37           |
|    total_timesteps      | 5382144      |
| train/                  |              |
|    approx_kl            | 0.0144726485 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -26.3        |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.000177     |
|    loss                 | -0.56        |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.0267      |
|    std                  | 4.52         |
|    value_loss           | 0.00802      |
------------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.2e+03     |
|    ep_rew_mean          | 3.2194183   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 5           |
|    time_elapsed         | 46          |
|    total_timesteps      | 5386240     |
| train/                  |             |
|    approx_kl            | 0.014276868 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.4       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.512      |
|    n_updates            | 2385        |
|    policy_gradient_loss | -0.0222     |
|    std                  | 4.54        |
|    value_loss           | 0.105       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.2e+03     |
|    ep_rew_mean          | 3.2194183   |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 6           |
|    time_elapsed         | 55          |
|    total_timesteps      | 5390336     |
| train/                  |             |
|    approx_kl            | 0.014467513 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.4       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.572      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0293     |
|    std                  | 4.57        |
|    value_loss           | 0.00959     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.2e+03     |
|    ep_rew_mean          | 3.2194183   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5394432     |
| train/                  |             |
|    approx_kl            | 0.013493512 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.4       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.574      |
|    n_updates            | 2415        |
|    policy_gradient_loss | -0.03       |
|    std                  | 4.57        |
|    value_loss           | 0.00437     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.3e+03     |
|    ep_rew_mean          | 3.1890655   |
| time/                   |             |
|    fps                  | 437         |
|    iterations           | 8           |
|    time_elapsed         | 74          |
|    total_timesteps      | 5398528     |
| train/                  |             |
|    approx_kl            | 0.015597657 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.572      |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0308     |
|    std                  | 4.6         |
|    value_loss           | 0.00277     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.3e+03     |
|    ep_rew_mean          | 3.1890655   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 9           |
|    time_elapsed         | 84          |
|    total_timesteps      | 5402624     |
| train/                  |             |
|    approx_kl            | 0.017317953 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.57       |
|    n_updates            | 2445        |
|    policy_gradient_loss | -0.0353     |
|    std                  | 4.63        |
|    value_loss           | 0.013       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 3.174409    |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.012983793 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.576      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.029      |
|    std                  | 4.64        |
|    value_loss           | 0.00464     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.4e+03     |
|    ep_rew_mean          | 3.174409    |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 11          |
|    time_elapsed         | 102         |
|    total_timesteps      | 5410816     |
| train/                  |             |
|    approx_kl            | 0.014870366 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.576      |
|    n_updates            | 2475        |
|    policy_gradient_loss | -0.0349     |
|    std                  | 4.67        |
|    value_loss           | 0.013       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.4e+03    |
|    ep_rew_mean          | 3.174409   |
| time/                   |            |
|    fps                  | 440        |
|    iterations           | 12         |
|    time_elapsed         | 111        |
|    total_timesteps      | 5414912    |
| train/                  |            |
|    approx_kl            | 0.01491913 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.6      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.000177   |
|    loss                 | -0.568     |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0293    |
|    std                  | 4.68       |
|    value_loss           | 0.00487    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.49e+03    |
|    ep_rew_mean          | 3.1295774   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 13          |
|    time_elapsed         | 120         |
|    total_timesteps      | 5419008     |
| train/                  |             |
|    approx_kl            | 0.014580436 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.000177    |
|    loss                 | -0.575      |
|    n_updates            | 2505        |
|    policy_gradient_loss | -0.0301     |
|    std                  | 4.69        |
|    value_loss           | 0.00459     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_6.zip

evaluating student performance (5 episodes)...
episode 1: length=2505, reward=26.132, success=True
episode 2: length=2505, reward=26.132, success=True
episode 3: length=2505, reward=26.132, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 26.132
average episode length: 2505.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 99.730
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 39.414
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 10.383
total data points collected: 40029
average episode length: 8005.8
average episode reward: 33.404
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1779', '7.1039', '7.8318', '7.8219', '8.8815']
Training reward models...
Reward model losses: ['0.2669', '0.0190', '0.2453', '0.0059', '0.0160']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2482', '1.3048', '1.2773', '1.2639', '1.0767']
Training action VAE models...
Action VAE losses: ['1.4957', '1.4635', '1.3362', '1.4825', '1.4299']
CM score components:
transition disagreement: 0.4486
reward disagreement: 0.1562
state disagreement: 0.4814
action disagreement: 0.5849
total CM score: 1.6710
goal is complete. CM score: 1.6710
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 57.111
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 56.028
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 83.584
total data points collected: 50005
average episode length: 10001.0
average episode reward: 67.678
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4476', '7.2536', '7.6469', '8.5608', '7.9375']
Training reward models...
Reward model losses: ['0.0084', '0.1222', '1.6611', '0.2223', '0.1574']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1649', '1.3510', '1.1855', '1.0115', '1.0899']
Training action VAE models...
Action VAE losses: ['1.4050', '1.4186', '1.3637', '1.3651', '1.4068']
CM score components:
transition disagreement: 0.4219
reward disagreement: 0.4423
state disagreement: 0.5060
action disagreement: 0.5523
total CM score: 1.9226
mass is complete. CM score: 1.9226
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 1654 steps, reward: 17.128
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 81.521
Reset #3: friction intervention applied (success: True)
episode 3: 368 steps, reward: 3.497
total data points collected: 12824
average episode length: 2564.8
average episode reward: 22.025
termination reasons: ['success', 'max_length', 'success', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([12824, 56]), actions: torch.Size([12824, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2094', '7.5983', '6.9561', '7.6568', '6.4197']
Training reward models...
Reward model losses: ['0.0116', '0.0186', '0.1568', '0.0242', '0.2455']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1307', '1.0721', '1.1120', '1.0697', '1.2628']
Training action VAE models...
Action VAE losses: ['1.4870', '1.4344', '1.3439', '1.2523', '1.4634']
CM score components:
transition disagreement: 0.4352
reward disagreement: 0.1512
state disagreement: 0.4933
action disagreement: 0.5405
total CM score: 1.6202
friction is complete. CM score: 1.6202
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 2505 steps, reward: 26.132
Reset #2: visual intervention applied (success: True)
episode 2: 2505 steps, reward: 26.132
Reset #3: visual intervention applied (success: True)
episode 3: 2505 steps, reward: 26.132
total data points collected: 12525
average episode length: 2505.0
average episode reward: 26.132
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([12525, 56]), actions: torch.Size([12525, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8808', '7.5313', '7.2657', '6.7677', '7.5727']
Training reward models...
Reward model losses: ['0.0056', '0.0433', '0.2784', '0.2583', '0.1690']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7698', '0.7484', '0.7097', '0.8082', '0.7587']
Training action VAE models...
Action VAE losses: ['1.6574', '1.4250', '1.4459', '1.4128', '1.5057']
CM score components:
transition disagreement: 0.4253
reward disagreement: 0.1531
state disagreement: 0.4575
action disagreement: 0.5878
total CM score: 1.6237
visual is complete. CM score: 1.6237
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 185 steps, reward: 1.988
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 23.707
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.587
total data points collected: 30212
average episode length: 6042.4
average episode reward: 8.325
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30212, 56]), actions: torch.Size([30212, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4529', '7.3837', '7.5495', '7.3070', '7.3184']
Training reward models...
Reward model losses: ['0.2107', '0.0418', '0.2437', '0.0114', '0.0146']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9453', '1.0346', '1.0227', '1.0991', '0.9619']
Training action VAE models...
Action VAE losses: ['1.3235', '1.3246', '1.3656', '1.3983', '1.4116']
CM score components:
transition disagreement: 0.4254
reward disagreement: 0.1293
state disagreement: 0.4600
action disagreement: 0.5192
total CM score: 1.5339
pose is complete. CM score: 1.5339
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 30.265
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.734
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.733
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.656
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7158', '7.6310', '8.3709', '7.7561', '7.4642']
Training reward models...
Reward model losses: ['0.0200', '0.0155', '0.3137', '0.0187', '0.1763']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2077', '1.2987', '1.2924', '1.2415', '1.1533']
Training action VAE models...
Action VAE losses: ['1.4194', '1.4960', '1.4555', '1.4039', '1.4847']
CM score components:
transition disagreement: 0.4239
reward disagreement: 0.1577
state disagreement: 0.4906
action disagreement: 0.5686
total CM score: 1.6408
random is complete. CM score: 1.6408
2025-07-16 18:20:17,007 3266759 INFO Meta-Episode 7/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_6.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 99.730
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 39.414
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 10.383
total data points collected: 40029
average episode length: 8005.8
average episode reward: 33.404
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2733', '7.1501', '6.8011', '7.0932', '7.6721']
Training reward models...
Reward model losses: ['0.2006', '0.1887', '0.2269', '0.0032', '0.0125']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1305', '1.2629', '1.2478', '1.2122', '1.1995']
Training action VAE models...
Action VAE losses: ['1.5337', '1.4132', '1.4693', '1.3765', '1.3989']
CM score components:
transition disagreement: 0.4072
reward disagreement: 0.0703
state disagreement: 0.4582
action disagreement: 0.5637
total CM score: 1.4994
goal is complete. CM score: 1.4994
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 57.111
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 56.028
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 83.584
total data points collected: 50005
average episode length: 10001.0
average episode reward: 67.678
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0550', '7.6555', '7.0145', '6.8285', '8.2806']
Training reward models...
Reward model losses: ['0.0038', '0.2533', '0.0094', '0.4001', '0.5713']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1816', '0.9896', '1.0286', '1.1280', '1.2268']
Training action VAE models...
Action VAE losses: ['1.4445', '1.4396', '1.3547', '1.3967', '1.2750']
CM score components:
transition disagreement: 0.4212
reward disagreement: 0.1918
state disagreement: 0.5026
action disagreement: 0.5497
total CM score: 1.6653
mass is complete. CM score: 1.6653
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 1654 steps, reward: 17.128
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 81.521
Reset #3: friction intervention applied (success: True)
episode 3: 368 steps, reward: 3.497
total data points collected: 12824
average episode length: 2564.8
average episode reward: 22.025
termination reasons: ['success', 'max_length', 'success', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([12824, 56]), actions: torch.Size([12824, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8843', '7.2926', '6.8083', '7.6557', '7.0827']
Training reward models...
Reward model losses: ['0.0598', '1.0407', '0.4837', '0.0188', '0.2697']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1431', '1.0715', '1.0875', '1.2218', '1.1372']
Training action VAE models...
Action VAE losses: ['1.2963', '1.3718', '1.3429', '1.3049', '1.4269']
CM score components:
transition disagreement: 0.4151
reward disagreement: 0.4189
state disagreement: 0.4881
action disagreement: 0.5212
total CM score: 1.8433
friction is complete. CM score: 1.8433
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 2505 steps, reward: 26.132
Reset #2: visual intervention applied (success: True)
episode 2: 2505 steps, reward: 26.132
Reset #3: visual intervention applied (success: True)
episode 3: 2505 steps, reward: 26.132
total data points collected: 12525
average episode length: 2505.0
average episode reward: 26.132
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([12525, 56]), actions: torch.Size([12525, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1463', '7.7914', '7.0930', '7.1422', '7.4873']
Training reward models...
Reward model losses: ['0.0520', '0.3969', '0.0669', '0.0145', '0.0554']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7250', '0.6767', '0.8162', '0.7143', '0.7157']
Training action VAE models...
Action VAE losses: ['1.3947', '1.3270', '1.3403', '1.4500', '1.5924']
CM score components:
transition disagreement: 0.4394
reward disagreement: 0.1794
state disagreement: 0.4362
action disagreement: 0.5391
total CM score: 1.5941
visual is complete. CM score: 1.5941
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 185 steps, reward: 1.988
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 23.707
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.587
total data points collected: 30212
average episode length: 6042.4
average episode reward: 8.325
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30212, 56]), actions: torch.Size([30212, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7497', '7.7458', '7.6953', '6.5752', '7.4368']
Training reward models...
Reward model losses: ['2.7720', '0.9660', '0.1545', '0.0119', '0.0240']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0477', '1.1438', '1.0631', '1.0843', '1.0274']
Training action VAE models...
Action VAE losses: ['1.3701', '1.4414', '1.3528', '1.4162', '1.3051']
CM score components:
transition disagreement: 0.4359
reward disagreement: 0.6843
state disagreement: 0.4881
action disagreement: 0.5589
total CM score: 2.1672
pose is complete. CM score: 2.1672
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 30.265
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -1.734
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.733
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.656
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4303', '7.3111', '8.2759', '7.3973', '7.3086']
Training reward models...
Reward model losses: ['0.0480', '0.0320', '0.0230', '0.2586', '0.1247']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2147', '1.2049', '1.2227', '1.1651', '1.2766']
Training action VAE models...
Action VAE losses: ['1.4869', '1.3801', '1.4072', '1.4946', '1.4145']
CM score components:
transition disagreement: 0.3723
reward disagreement: 0.1318
state disagreement: 0.4902
action disagreement: 0.5784
total CM score: 1.5727
random is complete. CM score: 1.5727
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.5e+03   |
|    ep_rew_mean     | 3.1763806 |
| time/              |           |
|    fps             | 466       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5423104   |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.5e+03   |
|    ep_rew_mean          | 3.1763806 |
| time/                   |           |
|    fps                  | 438       |
|    iterations           | 2         |
|    time_elapsed         | 18        |
|    total_timesteps      | 5427200   |
| train/                  |           |
|    approx_kl            | 0.013976  |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.2       |
|    entropy_loss         | -26.7     |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.000159  |
|    loss                 | -0.575    |
|    n_updates            | 2535      |
|    policy_gradient_loss | -0.0303   |
|    std                  | 4.73      |
|    value_loss           | 0.0101    |
---------------------------------------
Reset #3: visual intervention applied (success: True)
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 3.6e+03      |
|    ep_rew_mean          | 3.1530902    |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 3            |
|    time_elapsed         | 28           |
|    total_timesteps      | 5431296      |
| train/                  |              |
|    approx_kl            | 0.0145887425 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    entropy_loss         | -26.8        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.000159     |
|    loss                 | -0.571       |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.0249      |
|    std                  | 4.75         |
|    value_loss           | 0.00386      |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.6e+03    |
|    ep_rew_mean          | 3.1530902  |
| time/                   |            |
|    fps                  | 432        |
|    iterations           | 4          |
|    time_elapsed         | 37         |
|    total_timesteps      | 5435392    |
| train/                  |            |
|    approx_kl            | 0.01437243 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.8      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.000159   |
|    loss                 | -0.574     |
|    n_updates            | 2565       |
|    policy_gradient_loss | -0.0293    |
|    std                  | 4.78       |
|    value_loss           | 0.0122     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.6e+03     |
|    ep_rew_mean          | 3.1530902   |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5439488     |
| train/                  |             |
|    approx_kl            | 0.013250412 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.8       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.586      |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.0307     |
|    std                  | 4.79        |
|    value_loss           | 0.00627     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.69e+03    |
|    ep_rew_mean          | 3.062887    |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 6           |
|    time_elapsed         | 56          |
|    total_timesteps      | 5443584     |
| train/                  |             |
|    approx_kl            | 0.014031455 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.000159    |
|    loss                 | -0.579      |
|    n_updates            | 2595        |
|    policy_gradient_loss | -0.0312     |
|    std                  | 4.8         |
|    value_loss           | 0.00552     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.69e+03    |
|    ep_rew_mean          | 3.062887    |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 7           |
|    time_elapsed         | 66          |
|    total_timesteps      | 5447680     |
| train/                  |             |
|    approx_kl            | 0.016420806 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.578      |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.0357     |
|    std                  | 4.82        |
|    value_loss           | 0.0138      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.79e+03  |
|    ep_rew_mean          | 3.0304666 |
| time/                   |           |
|    fps                  | 432       |
|    iterations           | 8         |
|    time_elapsed         | 75        |
|    total_timesteps      | 5451776   |
| train/                  |           |
|    approx_kl            | 0.0145107 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.2       |
|    entropy_loss         | -26.9     |
|    explained_variance   | 0.936     |
|    learning_rate        | 0.000159  |
|    loss                 | -0.575    |
|    n_updates            | 2625      |
|    policy_gradient_loss | -0.0244   |
|    std                  | 4.84      |
|    value_loss           | 0.00498   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.79e+03   |
|    ep_rew_mean          | 3.0304666  |
| time/                   |            |
|    fps                  | 432        |
|    iterations           | 9          |
|    time_elapsed         | 85         |
|    total_timesteps      | 5455872    |
| train/                  |            |
|    approx_kl            | 0.01189851 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27        |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.000159   |
|    loss                 | -0.575     |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.025     |
|    std                  | 4.87       |
|    value_loss           | 0.0255     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.79e+03    |
|    ep_rew_mean          | 3.0304666   |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 10          |
|    time_elapsed         | 94          |
|    total_timesteps      | 5459968     |
| train/                  |             |
|    approx_kl            | 0.011932134 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.567      |
|    n_updates            | 2655        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 4.88        |
|    value_loss           | 0.00786     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.88e+03    |
|    ep_rew_mean          | 2.9895182   |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 11          |
|    time_elapsed         | 104         |
|    total_timesteps      | 5464064     |
| train/                  |             |
|    approx_kl            | 0.012689253 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.576      |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.0239     |
|    std                  | 4.88        |
|    value_loss           | 0.00416     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.88e+03    |
|    ep_rew_mean          | 2.9895182   |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 12          |
|    time_elapsed         | 114         |
|    total_timesteps      | 5468160     |
| train/                  |             |
|    approx_kl            | 0.016455816 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.587      |
|    n_updates            | 2685        |
|    policy_gradient_loss | -0.0302     |
|    std                  | 4.9         |
|    value_loss           | 0.00585     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.98e+03    |
|    ep_rew_mean          | 2.9506087   |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 13          |
|    time_elapsed         | 123         |
|    total_timesteps      | 5472256     |
| train/                  |             |
|    approx_kl            | 0.016713582 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.000159    |
|    loss                 | -0.577      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.0251     |
|    std                  | 4.92        |
|    value_loss           | 0.00206     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_7.zip

evaluating student performance (5 episodes)...
episode 1: length=69, reward=0.788, success=True
episode 2: length=69, reward=0.788, success=True
episode 3: length=69, reward=0.788, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 0.788
average episode length: 69.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 1.220
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 21.277
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 38.880
total data points collected: 40031
average episode length: 8006.2
average episode reward: 26.223
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1815', '7.0941', '7.8118', '7.8108', '8.8712']
Training reward models...
Reward model losses: ['0.2780', '0.0123', '0.2522', '0.0066', '0.0168']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2772', '1.2693', '1.1988', '1.2204', '1.1347']
Training action VAE models...
Action VAE losses: ['1.5192', '1.5279', '1.3216', '1.4312', '1.3902']
CM score components:
transition disagreement: 0.4513
reward disagreement: 0.1574
state disagreement: 0.4769
action disagreement: 0.5822
total CM score: 1.6678
goal is complete. CM score: 1.6678
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 73.339
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -4.031
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 66.975
total data points collected: 40042
average episode length: 8008.4
average episode reward: 45.194
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40042, 56]), actions: torch.Size([40042, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4471', '7.2559', '7.6693', '8.5548', '7.9256']
Training reward models...
Reward model losses: ['0.0109', '0.1056', '1.7288', '0.2645', '0.1214']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1680', '1.2561', '1.1662', '1.0339', '1.0587']
Training action VAE models...
Action VAE losses: ['1.3090', '1.2758', '1.1885', '1.2482', '1.2526']
CM score components:
transition disagreement: 0.4266
reward disagreement: 0.4550
state disagreement: 0.5031
action disagreement: 0.5357
total CM score: 1.9203
mass is complete. CM score: 1.9203
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 69 steps, reward: 0.788
Reset #2: friction intervention applied (success: True)
episode 2: 69 steps, reward: 0.788
Reset #3: friction intervention applied (success: True)
episode 3: 69 steps, reward: 0.788
total data points collected: 345
average episode length: 69.0
average episode reward: 0.788
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([345, 56]), actions: torch.Size([345, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2589', '7.7063', '7.0216', '7.7587', '6.5069']
Training reward models...
Reward model losses: ['0.0264', '0.0294', '0.1858', '0.0328', '0.2620']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4548', '1.3236', '1.2976', '1.3180', '1.5791']
Training action VAE models...
Action VAE losses: ['1.4165', '1.4927', '1.2931', '1.3081', '1.5304']
CM score components:
transition disagreement: 0.4423
reward disagreement: 0.1634
state disagreement: 0.5199
action disagreement: 0.5506
total CM score: 1.6762
friction is complete. CM score: 1.6762
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 69 steps, reward: 0.788
Reset #2: visual intervention applied (success: True)
episode 2: 69 steps, reward: 0.788
Reset #3: visual intervention applied (success: True)
episode 3: 69 steps, reward: 0.788
total data points collected: 345
average episode length: 69.0
average episode reward: 0.788
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([345, 56]), actions: torch.Size([345, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9495', '7.6039', '7.3356', '6.8516', '7.6623']
Training reward models...
Reward model losses: ['0.0145', '0.0617', '0.2746', '0.2660', '0.1562']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3941', '1.4803', '1.2144', '1.4514', '1.4191']
Training action VAE models...
Action VAE losses: ['1.6038', '1.4269', '1.4667', '1.4568', '1.4382']
CM score components:
transition disagreement: 0.4302
reward disagreement: 0.1485
state disagreement: 0.5365
action disagreement: 0.5870
total CM score: 1.7021
visual is complete. CM score: 1.7021
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 25 steps, reward: 0.673
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 25.598
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 37.392
total data points collected: 40029
average episode length: 8005.8
average episode reward: 38.407
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4578', '7.3833', '7.5607', '7.2993', '7.3000']
Training reward models...
Reward model losses: ['0.2492', '0.0401', '0.2488', '0.0135', '0.0087']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9599', '0.9861', '1.0379', '1.0865', '0.9819']
Training action VAE models...
Action VAE losses: ['1.1661', '1.2346', '1.2351', '1.2932', '1.3298']
CM score components:
transition disagreement: 0.4246
reward disagreement: 0.1420
state disagreement: 0.4596
action disagreement: 0.5237
total CM score: 1.5500
pose is complete. CM score: 1.5500
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 55.877
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.401
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.338
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.703
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7351', '7.6326', '8.3860', '7.7519', '7.4710']
Training reward models...
Reward model losses: ['0.0185', '0.0159', '0.2813', '0.0203', '0.1744']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2178', '1.2970', '1.3056', '1.2746', '1.2200']
Training action VAE models...
Action VAE losses: ['1.3554', '1.5408', '1.3716', '1.3915', '1.4424']
CM score components:
transition disagreement: 0.4228
reward disagreement: 0.1442
state disagreement: 0.4861
action disagreement: 0.5640
total CM score: 1.6171
random is complete. CM score: 1.6171
2025-07-16 18:34:48,136 3266759 INFO Meta-Episode 8/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_7.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 1.220
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 21.277
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 38.880
total data points collected: 40031
average episode length: 8006.2
average episode reward: 26.223
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2701', '7.1152', '6.7982', '7.0958', '7.6523']
Training reward models...
Reward model losses: ['0.1885', '0.2250', '0.2566', '0.0072', '0.0164']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1194', '1.2589', '1.3240', '1.1910', '1.1793']
Training action VAE models...
Action VAE losses: ['1.5157', '1.3614', '1.4666', '1.3750', '1.3443']
CM score components:
transition disagreement: 0.4089
reward disagreement: 0.0696
state disagreement: 0.4575
action disagreement: 0.5603
total CM score: 1.4962
goal is complete. CM score: 1.4962
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 73.339
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -4.031
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 66.975
total data points collected: 40042
average episode length: 8008.4
average episode reward: 45.194
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40042, 56]), actions: torch.Size([40042, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0700', '7.6713', '7.0205', '6.8402', '8.2678']
Training reward models...
Reward model losses: ['0.0030', '0.2376', '0.0093', '0.3982', '0.6282']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1899', '1.0151', '0.9576', '1.0700', '1.2102']
Training action VAE models...
Action VAE losses: ['1.3039', '1.3008', '1.2872', '1.2611', '1.1431']
CM score components:
transition disagreement: 0.4214
reward disagreement: 0.2069
state disagreement: 0.5027
action disagreement: 0.5449
total CM score: 1.6758
mass is complete. CM score: 1.6758
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 69 steps, reward: 0.788
Reset #2: friction intervention applied (success: True)
episode 2: 69 steps, reward: 0.788
Reset #3: friction intervention applied (success: True)
episode 3: 69 steps, reward: 0.788
total data points collected: 345
average episode length: 69.0
average episode reward: 0.788
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([345, 56]), actions: torch.Size([345, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0014', '7.3437', '6.9073', '7.7434', '7.1561']
Training reward models...
Reward model losses: ['0.0675', '1.0111', '0.4976', '0.0279', '0.2547']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3510', '1.3926', '1.3473', '1.3834', '1.4384']
Training action VAE models...
Action VAE losses: ['1.3175', '1.3882', '1.2995', '1.3250', '1.4091']
CM score components:
transition disagreement: 0.4179
reward disagreement: 0.4098
state disagreement: 0.5206
action disagreement: 0.5220
total CM score: 1.8703
friction is complete. CM score: 1.8703
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 69 steps, reward: 0.788
Reset #2: visual intervention applied (success: True)
episode 2: 69 steps, reward: 0.788
Reset #3: visual intervention applied (success: True)
episode 3: 69 steps, reward: 0.788
total data points collected: 345
average episode length: 69.0
average episode reward: 0.788
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([345, 56]), actions: torch.Size([345, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1128', '7.7969', '7.1766', '7.2280', '7.5847']
Training reward models...
Reward model losses: ['0.0653', '0.3334', '0.1038', '0.0200', '0.0790']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3553', '1.2764', '1.4182', '1.2275', '1.3099']
Training action VAE models...
Action VAE losses: ['1.4426', '1.2614', '1.4098', '1.4665', '1.5035']
CM score components:
transition disagreement: 0.4417
reward disagreement: 0.1599
state disagreement: 0.4889
action disagreement: 0.5356
total CM score: 1.6262
visual is complete. CM score: 1.6262
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 25 steps, reward: 0.673
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 25.598
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 37.392
total data points collected: 40029
average episode length: 8005.8
average episode reward: 38.407
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7609', '7.7375', '7.6795', '6.5876', '7.4220']
Training reward models...
Reward model losses: ['2.7979', '0.8870', '0.1513', '0.0146', '0.0305']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0307', '1.0933', '1.0619', '1.0545', '0.9596']
Training action VAE models...
Action VAE losses: ['1.2798', '1.4287', '1.3196', '1.3112', '1.2068']
CM score components:
transition disagreement: 0.4383
reward disagreement: 0.6792
state disagreement: 0.4816
action disagreement: 0.5558
total CM score: 2.1549
pose is complete. CM score: 2.1549
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 55.877
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.401
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.338
total data points collected: 50005
average episode length: 10001.0
average episode reward: 11.703
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4473', '7.3013', '8.2977', '7.3958', '7.3069']
Training reward models...
Reward model losses: ['0.0423', '0.0357', '0.0252', '0.2681', '0.1239']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2854', '1.2542', '1.2421', '1.2619', '1.3067']
Training action VAE models...
Action VAE losses: ['1.3890', '1.3262', '1.3429', '1.4322', '1.4130']
CM score components:
transition disagreement: 0.3729
reward disagreement: 0.1343
state disagreement: 0.4907
action disagreement: 0.5766
total CM score: 1.5745
random is complete. CM score: 1.5745
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.98e+03  |
|    ep_rew_mean     | 2.9506087 |
| time/              |           |
|    fps             | 462       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5476352   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.98e+03    |
|    ep_rew_mean          | 2.9506087   |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5480448     |
| train/                  |             |
|    approx_kl            | 0.012614177 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.585      |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.0294     |
|    std                  | 4.97        |
|    value_loss           | 0.0134      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.07e+03    |
|    ep_rew_mean          | 2.9310179   |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5484544     |
| train/                  |             |
|    approx_kl            | 0.011242155 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.581      |
|    n_updates            | 2745        |
|    policy_gradient_loss | -0.0228     |
|    std                  | 4.98        |
|    value_loss           | 0.00347     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.07e+03    |
|    ep_rew_mean          | 2.9310179   |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 4           |
|    time_elapsed         | 37          |
|    total_timesteps      | 5488640     |
| train/                  |             |
|    approx_kl            | 0.015053262 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.573      |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.0287     |
|    std                  | 4.99        |
|    value_loss           | 0.0118      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.17e+03     |
|    ep_rew_mean          | 2.88489      |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 5            |
|    time_elapsed         | 47           |
|    total_timesteps      | 5492736      |
| train/                  |              |
|    approx_kl            | 0.0125038475 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -27.2        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.000143     |
|    loss                 | -0.572       |
|    n_updates            | 2775         |
|    policy_gradient_loss | -0.021       |
|    std                  | 5            |
|    value_loss           | 0.00173      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.17e+03    |
|    ep_rew_mean          | 2.88489     |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 5496832     |
| train/                  |             |
|    approx_kl            | 0.012217412 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.3       |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.562      |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0239     |
|    std                  | 5.02        |
|    value_loss           | 0.0269      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.17e+03    |
|    ep_rew_mean          | 2.88489     |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 7           |
|    time_elapsed         | 66          |
|    total_timesteps      | 5500928     |
| train/                  |             |
|    approx_kl            | 0.012901782 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.3       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.574      |
|    n_updates            | 2805        |
|    policy_gradient_loss | -0.0222     |
|    std                  | 5.03        |
|    value_loss           | 0.00363     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.26e+03     |
|    ep_rew_mean          | 2.8564944    |
| time/                   |              |
|    fps                  | 426          |
|    iterations           | 8            |
|    time_elapsed         | 76           |
|    total_timesteps      | 5505024      |
| train/                  |              |
|    approx_kl            | 0.0138361575 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -27.3        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.000143     |
|    loss                 | -0.569       |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.0225      |
|    std                  | 5.06         |
|    value_loss           | 0.00204      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.26e+03    |
|    ep_rew_mean          | 2.8564944   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 9           |
|    time_elapsed         | 86          |
|    total_timesteps      | 5509120     |
| train/                  |             |
|    approx_kl            | 0.012664801 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.576      |
|    n_updates            | 2835        |
|    policy_gradient_loss | -0.0245     |
|    std                  | 5.08        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.36e+03    |
|    ep_rew_mean          | 2.8133705   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 10          |
|    time_elapsed         | 96          |
|    total_timesteps      | 5513216     |
| train/                  |             |
|    approx_kl            | 0.012649983 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.579      |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 5.1         |
|    value_loss           | 0.0039      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.36e+03    |
|    ep_rew_mean          | 2.8133705   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 11          |
|    time_elapsed         | 105         |
|    total_timesteps      | 5517312     |
| train/                  |             |
|    approx_kl            | 0.012376262 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.58       |
|    n_updates            | 2865        |
|    policy_gradient_loss | -0.0263     |
|    std                  | 5.12        |
|    value_loss           | 0.00514     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.36e+03    |
|    ep_rew_mean          | 2.8133705   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 12          |
|    time_elapsed         | 115         |
|    total_timesteps      | 5521408     |
| train/                  |             |
|    approx_kl            | 0.011059234 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.5       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.586      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 5.13        |
|    value_loss           | 0.00325     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.46e+03    |
|    ep_rew_mean          | 2.7769687   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 13          |
|    time_elapsed         | 124         |
|    total_timesteps      | 5525504     |
| train/                  |             |
|    approx_kl            | 0.012823112 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.583      |
|    n_updates            | 2895        |
|    policy_gradient_loss | -0.0217     |
|    std                  | 5.15        |
|    value_loss           | 0.00343     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_8.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=90.685, success=False
episode 2: length=10001, reward=90.685, success=False
episode 3: length=10001, reward=90.685, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 90.685
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 28 steps, reward: 2.646
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 4.499
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 3.167
total data points collected: 30053
average episode length: 6010.6
average episode reward: 2.597
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30053, 56]), actions: torch.Size([30053, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1768', '7.0744', '7.7917', '7.7970', '8.8207']
Training reward models...
Reward model losses: ['0.2487', '0.0096', '0.2414', '0.0027', '0.0124']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0925', '1.1598', '1.1320', '1.0646', '0.9201']
Training action VAE models...
Action VAE losses: ['1.4697', '1.3876', '1.2924', '1.4233', '1.3654']
CM score components:
transition disagreement: 0.4462
reward disagreement: 0.1342
state disagreement: 0.4647
action disagreement: 0.5871
total CM score: 1.6321
goal is complete. CM score: 1.6321
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 34 steps, reward: 1.006
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -1.600
Reset #3: mass intervention applied (success: True)
episode 3: 37 steps, reward: 1.256
total data points collected: 15630
average episode length: 3126.0
average episode reward: 11.152
termination reasons: ['success', 'max_length', 'success', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([15630, 56]), actions: torch.Size([15630, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4629', '7.2475', '7.6595', '8.5306', '7.9487']
Training reward models...
Reward model losses: ['0.0126', '0.1062', '1.7779', '0.2450', '0.0999']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2151', '1.0728', '1.3104', '1.2371', '1.1578']
Training action VAE models...
Action VAE losses: ['1.3747', '1.4178', '1.3441', '1.3888', '1.3213']
CM score components:
transition disagreement: 0.4273
reward disagreement: 0.4658
state disagreement: 0.5149
action disagreement: 0.5411
total CM score: 1.9491
mass is complete. CM score: 1.9491
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 91.908
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 89.974
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 91.282
total data points collected: 40033
average episode length: 8006.6
average episode reward: 73.276
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40033, 56]), actions: torch.Size([40033, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2461', '7.5948', '6.9364', '7.6408', '6.3916']
Training reward models...
Reward model losses: ['0.0118', '0.0163', '0.1644', '0.0264', '0.2284']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9267', '0.9092', '0.9632', '0.9889', '0.9216']
Training action VAE models...
Action VAE losses: ['1.3807', '1.4039', '1.3073', '1.2649', '1.5429']
CM score components:
transition disagreement: 0.4369
reward disagreement: 0.1481
state disagreement: 0.4615
action disagreement: 0.5427
total CM score: 1.5892
friction is complete. CM score: 1.5892
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 90.685
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 90.685
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 90.685
total data points collected: 50005
average episode length: 10001.0
average episode reward: 90.685
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9064', '7.5280', '7.2608', '6.7747', '7.5776']
Training reward models...
Reward model losses: ['0.0021', '0.0443', '0.2492', '0.2364', '0.1744']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8942', '0.9104', '0.9865', '1.0263', '0.8882']
Training action VAE models...
Action VAE losses: ['1.5584', '1.4660', '1.4795', '1.4396', '1.3900']
CM score components:
transition disagreement: 0.4207
reward disagreement: 0.1341
state disagreement: 0.4849
action disagreement: 0.5818
total CM score: 1.6214
visual is complete. CM score: 1.6214
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 27 steps, reward: 0.786
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.279
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -0.962
total data points collected: 40031
average episode length: 8006.2
average episode reward: 0.907
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4512', '7.3951', '7.5710', '7.3011', '7.2843']
Training reward models...
Reward model losses: ['0.2526', '0.0391', '0.2791', '0.0148', '0.0137']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1481', '1.2211', '1.2341', '1.2598', '1.1867']
Training action VAE models...
Action VAE losses: ['1.1600', '1.2461', '1.2163', '1.2980', '1.3422']
CM score components:
transition disagreement: 0.4249
reward disagreement: 0.1540
state disagreement: 0.4728
action disagreement: 0.5267
total CM score: 1.5785
pose is complete. CM score: 1.5785
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 47 steps, reward: 1.457
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.375
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.604
total data points collected: 40051
average episode length: 8010.2
average episode reward: 1.910
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40051, 56]), actions: torch.Size([40051, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7332', '7.6274', '8.3741', '7.7561', '7.4845']
Training reward models...
Reward model losses: ['0.0179', '0.0142', '0.2746', '0.0200', '0.1557']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1818', '1.2733', '1.2713', '1.2428', '1.1752']
Training action VAE models...
Action VAE losses: ['1.4144', '1.5086', '1.3738', '1.3678', '1.4150']
CM score components:
transition disagreement: 0.4175
reward disagreement: 0.1444
state disagreement: 0.4790
action disagreement: 0.5640
total CM score: 1.6049
random is complete. CM score: 1.6049
2025-07-16 18:51:16,110 3266759 INFO Meta-Episode 9/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_8.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 28 steps, reward: 2.646
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 4.499
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 3.167
total data points collected: 30053
average episode length: 6010.6
average episode reward: 2.597
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30053, 56]), actions: torch.Size([30053, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2620', '7.1393', '6.7458', '7.0740', '7.5864']
Training reward models...
Reward model losses: ['0.1944', '0.2166', '0.2338', '0.0036', '0.0204']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0192', '1.0562', '1.0333', '1.0307', '1.0618']
Training action VAE models...
Action VAE losses: ['1.3743', '1.3975', '1.4798', '1.3543', '1.3437']
CM score components:
transition disagreement: 0.4043
reward disagreement: 0.0598
state disagreement: 0.4479
action disagreement: 0.5609
total CM score: 1.4729
goal is complete. CM score: 1.4729
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 34 steps, reward: 1.006
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -1.600
Reset #3: mass intervention applied (success: True)
episode 3: 37 steps, reward: 1.256
total data points collected: 15630
average episode length: 3126.0
average episode reward: 11.152
termination reasons: ['success', 'max_length', 'success', 'success', 'success']
success rate: 4/5
tensor shapes - states: torch.Size([15630, 56]), actions: torch.Size([15630, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0858', '7.6885', '7.0540', '6.8557', '8.2582']
Training reward models...
Reward model losses: ['0.0052', '0.2743', '0.0102', '0.3355', '0.6665']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2178', '1.1242', '1.1345', '1.1897', '1.2120']
Training action VAE models...
Action VAE losses: ['1.4380', '1.3924', '1.3353', '1.3944', '1.2656']
CM score components:
transition disagreement: 0.4206
reward disagreement: 0.2202
state disagreement: 0.5174
action disagreement: 0.5535
total CM score: 1.7116
mass is complete. CM score: 1.7116
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 91.908
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 89.974
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 91.282
total data points collected: 40033
average episode length: 8006.6
average episode reward: 73.276
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40033, 56]), actions: torch.Size([40033, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9098', '7.2748', '6.7872', '7.7103', '7.0594']
Training reward models...
Reward model losses: ['0.0585', '1.0209', '0.4752', '0.0191', '0.2185']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9519', '0.9430', '0.9786', '0.9957', '0.8663']
Training action VAE models...
Action VAE losses: ['1.2594', '1.3600', '1.3277', '1.2869', '1.3105']
CM score components:
transition disagreement: 0.4170
reward disagreement: 0.4074
state disagreement: 0.4745
action disagreement: 0.5193
total CM score: 1.8181
friction is complete. CM score: 1.8181
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 90.685
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 90.685
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 90.685
total data points collected: 50005
average episode length: 10001.0
average episode reward: 90.685
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1621', '7.8220', '7.0708', '7.1685', '7.5028']
Training reward models...
Reward model losses: ['0.0373', '0.3579', '0.0646', '0.0177', '0.0464']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9850', '0.9010', '0.9693', '1.0420', '0.9605']
Training action VAE models...
Action VAE losses: ['1.4077', '1.2451', '1.4090', '1.4930', '1.4173']
CM score components:
transition disagreement: 0.4439
reward disagreement: 0.1669
state disagreement: 0.4629
action disagreement: 0.5342
total CM score: 1.6080
visual is complete. CM score: 1.6080
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 27 steps, reward: 0.786
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.279
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -0.962
total data points collected: 40031
average episode length: 8006.2
average episode reward: 0.907
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40031, 56]), actions: torch.Size([40031, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7763', '7.7235', '7.6636', '6.6012', '7.4251']
Training reward models...
Reward model losses: ['2.8386', '0.8601', '0.1459', '0.0168', '0.0403']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2115', '1.3192', '1.2777', '1.2842', '1.1364']
Training action VAE models...
Action VAE losses: ['1.2585', '1.4396', '1.3176', '1.3142', '1.2301']
CM score components:
transition disagreement: 0.4426
reward disagreement: 0.6809
state disagreement: 0.4973
action disagreement: 0.5571
total CM score: 2.1779
pose is complete. CM score: 2.1779
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 47 steps, reward: 1.457
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.375
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.604
total data points collected: 40051
average episode length: 8010.2
average episode reward: 1.910
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40051, 56]), actions: torch.Size([40051, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4335', '7.2946', '8.3130', '7.4183', '7.2928']
Training reward models...
Reward model losses: ['0.0554', '0.0357', '0.0232', '0.2695', '0.1293']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2663', '1.2151', '1.2493', '1.2803', '1.2721']
Training action VAE models...
Action VAE losses: ['1.3956', '1.3343', '1.3583', '1.4715', '1.4405']
CM score components:
transition disagreement: 0.3723
reward disagreement: 0.1254
state disagreement: 0.4954
action disagreement: 0.5782
total CM score: 1.5714
random is complete. CM score: 1.5714
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 4.46e+03  |
|    ep_rew_mean     | 2.7769687 |
| time/              |           |
|    fps             | 472       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5529600   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.46e+03    |
|    ep_rew_mean          | 2.7769687   |
| time/                   |             |
|    fps                  | 454         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5533696     |
| train/                  |             |
|    approx_kl            | 0.011886948 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.592      |
|    n_updates            | 2925        |
|    policy_gradient_loss | -0.0264     |
|    std                  | 5.19        |
|    value_loss           | 0.0072      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.56e+03   |
|    ep_rew_mean          | 2.7480984  |
| time/                   |            |
|    fps                  | 450        |
|    iterations           | 3          |
|    time_elapsed         | 27         |
|    total_timesteps      | 5537792    |
| train/                  |            |
|    approx_kl            | 0.01282398 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.6      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.000129   |
|    loss                 | -0.584     |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.0212    |
|    std                  | 5.22       |
|    value_loss           | 0.00267    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.56e+03    |
|    ep_rew_mean          | 2.7480984   |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 4           |
|    time_elapsed         | 36          |
|    total_timesteps      | 5541888     |
| train/                  |             |
|    approx_kl            | 0.014520337 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.587      |
|    n_updates            | 2955        |
|    policy_gradient_loss | -0.0301     |
|    std                  | 5.22        |
|    value_loss           | 0.0196      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.65e+03    |
|    ep_rew_mean          | 2.7370162   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 5           |
|    time_elapsed         | 46          |
|    total_timesteps      | 5545984     |
| train/                  |             |
|    approx_kl            | 0.012142573 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.583      |
|    n_updates            | 2970        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 5.23        |
|    value_loss           | 0.00288     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.65e+03    |
|    ep_rew_mean          | 2.7370162   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 6           |
|    time_elapsed         | 56          |
|    total_timesteps      | 5550080     |
| train/                  |             |
|    approx_kl            | 0.013277026 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.000129    |
|    loss                 | -0.584      |
|    n_updates            | 2985        |
|    policy_gradient_loss | -0.0243     |
|    std                  | 5.25        |
|    value_loss           | 0.00313     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.65e+03    |
|    ep_rew_mean          | 2.7370162   |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5554176     |
| train/                  |             |
|    approx_kl            | 0.010482147 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.581      |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.0216     |
|    std                  | 5.25        |
|    value_loss           | 0.00281     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.75e+03    |
|    ep_rew_mean          | 2.705756    |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 8           |
|    time_elapsed         | 75          |
|    total_timesteps      | 5558272     |
| train/                  |             |
|    approx_kl            | 0.010887593 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.578      |
|    n_updates            | 3015        |
|    policy_gradient_loss | -0.0175     |
|    std                  | 5.27        |
|    value_loss           | 0.00207     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.75e+03    |
|    ep_rew_mean          | 2.705756    |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 9           |
|    time_elapsed         | 85          |
|    total_timesteps      | 5562368     |
| train/                  |             |
|    approx_kl            | 0.010591323 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.593      |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.0236     |
|    std                  | 5.27        |
|    value_loss           | 0.0059      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.85e+03    |
|    ep_rew_mean          | 2.6933525   |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 10          |
|    time_elapsed         | 95          |
|    total_timesteps      | 5566464     |
| train/                  |             |
|    approx_kl            | 0.011286352 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.7       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.000129    |
|    loss                 | -0.58       |
|    n_updates            | 3045        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.29        |
|    value_loss           | 0.00578     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 4.85e+03     |
|    ep_rew_mean          | 2.6933525    |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 11           |
|    time_elapsed         | 104          |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 0.0120671615 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    entropy_loss         | -27.7        |
|    explained_variance   | 0.795        |
|    learning_rate        | 0.000129     |
|    loss                 | -0.592       |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.0288      |
|    std                  | 5.3          |
|    value_loss           | 0.0139       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.85e+03    |
|    ep_rew_mean          | 2.6933525   |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 12          |
|    time_elapsed         | 113         |
|    total_timesteps      | 5574656     |
| train/                  |             |
|    approx_kl            | 0.014636127 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.8       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.000129    |
|    loss                 | -0.577      |
|    n_updates            | 3075        |
|    policy_gradient_loss | -0.0253     |
|    std                  | 5.33        |
|    value_loss           | 0.0452      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.94e+03   |
|    ep_rew_mean          | 2.6676447  |
| time/                   |            |
|    fps                  | 432        |
|    iterations           | 13         |
|    time_elapsed         | 122        |
|    total_timesteps      | 5578752    |
| train/                  |            |
|    approx_kl            | 0.01573179 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.000129   |
|    loss                 | -0.57      |
|    n_updates            | 3090       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 5.36       |
|    value_loss           | 0.0473     |
----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_9.zip

evaluating student performance (5 episodes)...
episode 1: length=30, reward=1.123, success=True
episode 2: length=30, reward=1.123, success=True
episode 3: length=30, reward=1.123, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 1.123
average episode length: 30.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 38.048
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 8.947
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 7.790
total data points collected: 50005
average episode length: 10001.0
average episode reward: 14.142
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2013', '7.0870', '7.7980', '7.8002', '8.8579']
Training reward models...
Reward model losses: ['0.2614', '0.0107', '0.2600', '0.0027', '0.0156']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2456', '1.2814', '1.2427', '1.2038', '1.0397']
Training action VAE models...
Action VAE losses: ['1.4716', '1.4679', '1.2941', '1.5191', '1.4609']
CM score components:
transition disagreement: 0.4490
reward disagreement: 0.1531
state disagreement: 0.4872
action disagreement: 0.5930
total CM score: 1.6822
goal is complete. CM score: 1.6822
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 36 steps, reward: 1.135
Reset #2: mass intervention applied (success: True)
episode 2: 37 steps, reward: 1.225
Reset #3: mass intervention applied (success: True)
episode 3: 38 steps, reward: 1.286
total data points collected: 183
average episode length: 36.6
average episode reward: 1.232
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([183, 56]), actions: torch.Size([183, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4809', '7.3631', '7.7228', '8.6159', '7.9948']
Training reward models...
Reward model losses: ['0.0153', '0.1178', '1.7925', '0.2451', '0.1663']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7899', '1.6147', '2.0077', '1.7944', '1.8885']
Training action VAE models...
Action VAE losses: ['1.3808', '1.4397', '1.3583', '1.4029', '1.3476']
CM score components:
transition disagreement: 0.4336
reward disagreement: 0.4685
state disagreement: 0.5546
action disagreement: 0.5426
total CM score: 1.9993
mass is complete. CM score: 1.9993
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 33 steps, reward: 0.914
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: 1.316
Reset #3: friction intervention applied (success: True)
episode 3: 33 steps, reward: 0.919
total data points collected: 173
average episode length: 34.6
average episode reward: 1.025
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([173, 56]), actions: torch.Size([173, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2672', '7.6801', '7.0588', '7.7628', '6.4985']
Training reward models...
Reward model losses: ['0.0273', '0.0313', '0.1221', '0.0368', '0.2265']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7369', '1.7417', '1.8754', '1.6361', '2.1065']
Training action VAE models...
Action VAE losses: ['1.4386', '1.4876', '1.3230', '1.2731', '1.5315']
CM score components:
transition disagreement: 0.4498
reward disagreement: 0.1409
state disagreement: 0.5570
action disagreement: 0.5484
total CM score: 1.6960
friction is complete. CM score: 1.6960
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 30 steps, reward: 1.123
Reset #2: visual intervention applied (success: True)
episode 2: 30 steps, reward: 1.123
Reset #3: visual intervention applied (success: True)
episode 3: 30 steps, reward: 1.123
total data points collected: 150
average episode length: 30.0
average episode reward: 1.123
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([150, 56]), actions: torch.Size([150, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9955', '7.6346', '7.3159', '6.8878', '7.6811']
Training reward models...
Reward model losses: ['0.0098', '0.0709', '0.3090', '0.1956', '0.1320']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8342', '1.7564', '1.6160', '2.2008', '1.7790']
Training action VAE models...
Action VAE losses: ['1.5660', '1.4829', '1.4807', '1.4992', '1.4338']
CM score components:
transition disagreement: 0.4342
reward disagreement: 0.1406
state disagreement: 0.5724
action disagreement: 0.5911
total CM score: 1.7383
visual is complete. CM score: 1.7383
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 24 steps, reward: 0.676
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.255
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.669
total data points collected: 30053
average episode length: 6010.6
average episode reward: 0.089
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30053, 56]), actions: torch.Size([30053, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4723', '7.4610', '7.5927', '7.3368', '7.3020']
Training reward models...
Reward model losses: ['0.2075', '0.0443', '0.2338', '0.0078', '0.0055']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0284', '1.0651', '1.0766', '1.1132', '1.0132']
Training action VAE models...
Action VAE losses: ['1.1374', '1.2451', '1.2096', '1.3241', '1.2910']
CM score components:
transition disagreement: 0.4264
reward disagreement: 0.1148
state disagreement: 0.4654
action disagreement: 0.5198
total CM score: 1.5264
pose is complete. CM score: 1.5264
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 41.490
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.112
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.745
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.661
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7415', '7.6227', '8.3692', '7.7538', '7.4695']
Training reward models...
Reward model losses: ['0.0168', '0.0129', '0.2832', '0.0241', '0.1471']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2065', '1.3686', '1.3365', '1.2676', '1.2429']
Training action VAE models...
Action VAE losses: ['1.4717', '1.4528', '1.4120', '1.3434', '1.4334']
CM score components:
transition disagreement: 0.4146
reward disagreement: 0.1479
state disagreement: 0.4900
action disagreement: 0.5648
total CM score: 1.6172
random is complete. CM score: 1.6172
2025-07-16 19:04:38,772 3266759 INFO Meta-Episode 10/30: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_9.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 38.048
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 8.947
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 7.790
total data points collected: 50005
average episode length: 10001.0
average episode reward: 14.142
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2740', '7.1330', '6.7832', '7.1058', '7.6455']
Training reward models...
Reward model losses: ['0.2015', '0.2206', '0.2408', '0.0062', '0.0167']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0873', '1.1924', '1.1773', '1.1323', '1.1831']
Training action VAE models...
Action VAE losses: ['1.5888', '1.4088', '1.4440', '1.3588', '1.3211']
CM score components:
transition disagreement: 0.4063
reward disagreement: 0.0719
state disagreement: 0.4559
action disagreement: 0.5710
total CM score: 1.5052
goal is complete. CM score: 1.5052
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 36 steps, reward: 1.135
Reset #2: mass intervention applied (success: True)
episode 2: 37 steps, reward: 1.225
Reset #3: mass intervention applied (success: True)
episode 3: 38 steps, reward: 1.286
total data points collected: 183
average episode length: 36.6
average episode reward: 1.232
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([183, 56]), actions: torch.Size([183, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1551', '7.7914', '7.0978', '6.9333', '8.3330']
Training reward models...
Reward model losses: ['0.0205', '0.3131', '0.0100', '0.3431', '0.4981']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7525', '1.7379', '1.6887', '1.8429', '2.3052']
Training action VAE models...
Action VAE losses: ['1.5057', '1.3896', '1.3720', '1.4233', '1.3849']
CM score components:
transition disagreement: 0.4234
reward disagreement: 0.1773
state disagreement: 0.5831
action disagreement: 0.5448
total CM score: 1.7286
mass is complete. CM score: 1.7286
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 33 steps, reward: 0.914
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: 1.316
Reset #3: friction intervention applied (success: True)
episode 3: 33 steps, reward: 0.919
total data points collected: 173
average episode length: 34.6
average episode reward: 1.025
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([173, 56]), actions: torch.Size([173, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9977', '7.3523', '6.9273', '7.7346', '7.1891']
Training reward models...
Reward model losses: ['0.0532', '1.0999', '0.4888', '0.0328', '0.2899']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7405', '1.7317', '1.6704', '1.9753', '1.8073']
Training action VAE models...
Action VAE losses: ['1.3087', '1.3748', '1.3128', '1.2776', '1.4175']
CM score components:
transition disagreement: 0.4226
reward disagreement: 0.4235
state disagreement: 0.5429
action disagreement: 0.5261
total CM score: 1.9151
friction is complete. CM score: 1.9151
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 30 steps, reward: 1.123
Reset #2: visual intervention applied (success: True)
episode 2: 30 steps, reward: 1.123
Reset #3: visual intervention applied (success: True)
episode 3: 30 steps, reward: 1.123
total data points collected: 150
average episode length: 30.0
average episode reward: 1.123
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([150, 56]), actions: torch.Size([150, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1947', '7.8133', '7.1261', '7.2484', '7.5603']
Training reward models...
Reward model losses: ['0.0722', '0.3148', '0.1032', '0.0194', '0.1066']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6723', '1.6663', '1.8437', '1.6434', '1.6682']
Training action VAE models...
Action VAE losses: ['1.3956', '1.2654', '1.3534', '1.4491', '1.4589']
CM score components:
transition disagreement: 0.4465
reward disagreement: 0.1564
state disagreement: 0.5189
action disagreement: 0.5422
total CM score: 1.6639
visual is complete. CM score: 1.6639
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 24 steps, reward: 0.676
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.255
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.669
total data points collected: 30053
average episode length: 6010.6
average episode reward: 0.089
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30053, 56]), actions: torch.Size([30053, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8160', '7.6916', '7.6893', '6.5771', '7.4382']
Training reward models...
Reward model losses: ['2.6757', '0.8053', '0.1289', '0.0061', '0.0494']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1192', '1.1324', '1.0921', '1.0721', '0.9955']
Training action VAE models...
Action VAE losses: ['1.2307', '1.4100', '1.2647', '1.3126', '1.1502']
CM score components:
transition disagreement: 0.4437
reward disagreement: 0.6462
state disagreement: 0.4830
action disagreement: 0.5555
total CM score: 2.1284
pose is complete. CM score: 2.1284
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 41.490
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.112
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.745
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.661
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4370', '7.2916', '8.2961', '7.4257', '7.3014']
Training reward models...
Reward model losses: ['0.0640', '0.0417', '0.0207', '0.2923', '0.1386']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3141', '1.2721', '1.2302', '1.2523', '1.2850']
Training action VAE models...
Action VAE losses: ['1.4386', '1.3809', '1.4015', '1.4543', '1.4688']
CM score components:
transition disagreement: 0.3736
reward disagreement: 0.1319
state disagreement: 0.4939
action disagreement: 0.5786
total CM score: 1.5780
random is complete. CM score: 1.5780
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.94e+03 |
|    ep_rew_mean     | 2.658982 |
| time/              |          |
|    fps             | 443      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 5582848  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.94e+03    |
|    ep_rew_mean          | 2.658982    |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5586944     |
| train/                  |             |
|    approx_kl            | 0.013975404 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.9       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.567      |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0241     |
|    std                  | 5.39        |
|    value_loss           | 0.0655      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.04e+03    |
|    ep_rew_mean          | 2.6443834   |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5591040     |
| train/                  |             |
|    approx_kl            | 0.010473614 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.9       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.574      |
|    n_updates            | 3135        |
|    policy_gradient_loss | -0.0145     |
|    std                  | 5.41        |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.04e+03    |
|    ep_rew_mean          | 2.6443834   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5595136     |
| train/                  |             |
|    approx_kl            | 0.009444563 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.000116    |
|    loss                 | -0.587      |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 5.43        |
|    value_loss           | 0.00775     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.14e+03    |
|    ep_rew_mean          | 2.6638575   |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 5           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5599232     |
| train/                  |             |
|    approx_kl            | 0.009141153 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.58       |
|    n_updates            | 3165        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 5.44        |
|    value_loss           | 0.00374     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.14e+03    |
|    ep_rew_mean          | 2.6638575   |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 6           |
|    time_elapsed         | 58          |
|    total_timesteps      | 5603328     |
| train/                  |             |
|    approx_kl            | 0.011266097 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.576      |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.0232     |
|    std                  | 5.46        |
|    value_loss           | 0.00766     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.14e+03    |
|    ep_rew_mean          | 2.6638575   |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 7           |
|    time_elapsed         | 67          |
|    total_timesteps      | 5607424     |
| train/                  |             |
|    approx_kl            | 0.011222977 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.587      |
|    n_updates            | 3195        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 5.48        |
|    value_loss           | 0.00235     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.24e+03    |
|    ep_rew_mean          | 2.646359    |
| time/                   |             |
|    fps                  | 421         |
|    iterations           | 8           |
|    time_elapsed         | 77          |
|    total_timesteps      | 5611520     |
| train/                  |             |
|    approx_kl            | 0.011817338 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.000116    |
|    loss                 | -0.576      |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 5.5         |
|    value_loss           | 0.00613     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.24e+03    |
|    ep_rew_mean          | 2.646359    |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 9           |
|    time_elapsed         | 87          |
|    total_timesteps      | 5615616     |
| train/                  |             |
|    approx_kl            | 0.011699238 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.587      |
|    n_updates            | 3225        |
|    policy_gradient_loss | -0.0241     |
|    std                  | 5.51        |
|    value_loss           | 0.0096      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.34e+03    |
|    ep_rew_mean          | 2.6201098   |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 10          |
|    time_elapsed         | 97          |
|    total_timesteps      | 5619712     |
| train/                  |             |
|    approx_kl            | 0.012240227 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.583      |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 5.54        |
|    value_loss           | 0.00643     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.34e+03    |
|    ep_rew_mean          | 2.6201098   |
| time/                   |             |
|    fps                  | 421         |
|    iterations           | 11          |
|    time_elapsed         | 106         |
|    total_timesteps      | 5623808     |
| train/                  |             |
|    approx_kl            | 0.011701954 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.605      |
|    n_updates            | 3255        |
|    policy_gradient_loss | -0.0271     |
|    std                  | 5.56        |
|    value_loss           | 0.00642     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.34e+03    |
|    ep_rew_mean          | 2.6201098   |
| time/                   |             |
|    fps                  | 421         |
|    iterations           | 12          |
|    time_elapsed         | 116         |
|    total_timesteps      | 5627904     |
| train/                  |             |
|    approx_kl            | 0.010845827 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.000116    |
|    loss                 | -0.599      |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 5.56        |
|    value_loss           | 0.00319     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.44e+03   |
|    ep_rew_mean          | 2.6368656  |
| time/                   |            |
|    fps                  | 421        |
|    iterations           | 13         |
|    time_elapsed         | 126        |
|    total_timesteps      | 5632000    |
| train/                  |            |
|    approx_kl            | 0.01032919 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.2      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.000116   |
|    loss                 | -0.602     |
|    n_updates            | 3285       |
|    policy_gradient_loss | -0.0221    |
|    std                  | 5.58       |
|    value_loss           | 0.00351    |
----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_10.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=32.419, success=False
episode 2: length=10001, reward=32.419, success=False
episode 3: length=10001, reward=32.419, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 32.419
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 91.647
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.470
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 65.315
total data points collected: 50005
average episode length: 10001.0
average episode reward: 43.368
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1296', '7.0746', '7.7990', '7.8084', '8.9079']
Training reward models...
Reward model losses: ['0.2222', '0.0101', '0.2691', '0.0041', '0.0152']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1812', '1.2697', '1.2559', '1.1958', '1.0404']
Training action VAE models...
Action VAE losses: ['1.1671', '1.2027', '1.0941', '1.1997', '1.1632']
CM score components:
transition disagreement: 0.4488
reward disagreement: 0.1240
state disagreement: 0.4782
action disagreement: 0.5614
total CM score: 1.6125
goal is complete. CM score: 1.6125
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -1.591
Reset #2: mass intervention applied (success: True)
episode 2: 42 steps, reward: 1.166
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 60.266
total data points collected: 40046
average episode length: 8009.2
average episode reward: 20.228
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40046, 56]), actions: torch.Size([40046, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5258', '7.2708', '7.7212', '8.5762', '7.9648']
Training reward models...
Reward model losses: ['0.0082', '0.0553', '1.6382', '0.1858', '0.1250']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1832', '1.3824', '1.1839', '1.0332', '1.0955']
Training action VAE models...
Action VAE losses: ['1.1034', '1.1779', '1.0656', '1.1477', '1.1203']
CM score components:
transition disagreement: 0.4199
reward disagreement: 0.4145
state disagreement: 0.5027
action disagreement: 0.5274
total CM score: 1.8645
mass is complete. CM score: 1.8645
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 43 steps, reward: 1.593
Reset #2: friction intervention applied (success: True)
episode 2: 41 steps, reward: 1.467
Reset #3: friction intervention applied (success: True)
episode 3: 44 steps, reward: 1.549
total data points collected: 213
average episode length: 42.6
average episode reward: 1.570
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([213, 56]), actions: torch.Size([213, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2392', '7.6448', '7.0204', '7.7484', '6.4736']
Training reward models...
Reward model losses: ['0.0213', '0.0279', '0.1218', '0.0284', '0.2147']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5588', '1.5245', '1.6770', '1.4938', '1.6965']
Training action VAE models...
Action VAE losses: ['1.4561', '1.4601', '1.3138', '1.2790', '1.5320']
CM score components:
transition disagreement: 0.4459
reward disagreement: 0.1405
state disagreement: 0.5205
action disagreement: 0.5493
total CM score: 1.6562
friction is complete. CM score: 1.6562
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 32.419
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 32.419
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 32.419
total data points collected: 50005
average episode length: 10001.0
average episode reward: 32.419
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8953', '7.5462', '7.2584', '6.8077', '7.6014']
Training reward models...
Reward model losses: ['0.0086', '0.0438', '0.2635', '0.2626', '0.1233']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1798', '1.1554', '1.2200', '1.2503', '1.1448']
Training action VAE models...
Action VAE losses: ['1.3736', '1.2752', '1.2428', '1.3514', '1.2721']
CM score components:
transition disagreement: 0.4255
reward disagreement: 0.1286
state disagreement: 0.4961
action disagreement: 0.5807
total CM score: 1.6310
visual is complete. CM score: 1.6310
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 58.431
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 4.301
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -0.925
total data points collected: 50005
average episode length: 10001.0
average episode reward: 12.372
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4841', '7.4312', '7.5871', '7.2840', '7.2985']
Training reward models...
Reward model losses: ['0.2258', '0.0392', '0.2705', '0.0086', '0.0077']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1245', '1.1859', '1.2042', '1.2045', '1.1074']
Training action VAE models...
Action VAE losses: ['1.1758', '1.2657', '1.2260', '1.3718', '1.3309']
CM score components:
transition disagreement: 0.4261
reward disagreement: 0.1366
state disagreement: 0.4724
action disagreement: 0.5171
total CM score: 1.5521
pose is complete. CM score: 1.5521
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 55.634
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 1.657
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.373
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.287
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7768', '7.6564', '8.4258', '7.7636', '7.5416']
Training reward models...
Reward model losses: ['0.0139', '0.0141', '0.2530', '0.0152', '0.2189']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1877', '1.2903', '1.2690', '1.2785', '1.2039']
Training action VAE models...
Action VAE losses: ['1.3704', '1.4687', '1.3497', '1.3325', '1.4269']
CM score components:
transition disagreement: 0.4258
reward disagreement: 0.1286
state disagreement: 0.4903
action disagreement: 0.5664
total CM score: 1.6112
random is complete. CM score: 1.6112
2025-07-16 19:21:11,697 3266759 INFO Meta-Episode 11/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_10.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 91.647
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.470
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 65.315
total data points collected: 50005
average episode length: 10001.0
average episode reward: 43.368
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3573', '7.1775', '6.7730', '7.0934', '7.4989']
Training reward models...
Reward model losses: ['0.1518', '0.2289', '0.2358', '0.0038', '0.0236']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0872', '1.1963', '1.1802', '1.1693', '1.1503']
Training action VAE models...
Action VAE losses: ['1.2414', '1.1901', '1.1265', '1.0537', '1.0888']
CM score components:
transition disagreement: 0.4139
reward disagreement: 0.0599
state disagreement: 0.4517
action disagreement: 0.5483
total CM score: 1.4738
goal is complete. CM score: 1.4738
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -1.591
Reset #2: mass intervention applied (success: True)
episode 2: 42 steps, reward: 1.166
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 60.266
total data points collected: 40046
average episode length: 8009.2
average episode reward: 20.228
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40046, 56]), actions: torch.Size([40046, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1223', '7.6797', '7.0962', '6.8467', '8.3310']
Training reward models...
Reward model losses: ['0.0076', '0.1983', '0.0118', '0.3649', '0.4948']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2100', '1.0365', '1.0819', '1.1939', '1.2951']
Training action VAE models...
Action VAE losses: ['1.2382', '1.1792', '1.0917', '1.2643', '1.1137']
CM score components:
transition disagreement: 0.4237
reward disagreement: 0.1330
state disagreement: 0.5102
action disagreement: 0.5506
total CM score: 1.6176
mass is complete. CM score: 1.6176
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 43 steps, reward: 1.593
Reset #2: friction intervention applied (success: True)
episode 2: 41 steps, reward: 1.467
Reset #3: friction intervention applied (success: True)
episode 3: 44 steps, reward: 1.549
total data points collected: 213
average episode length: 42.6
average episode reward: 1.570
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([213, 56]), actions: torch.Size([213, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9816', '7.3324', '6.8770', '7.7070', '7.1469']
Training reward models...
Reward model losses: ['0.0533', '1.0284', '0.4946', '0.0208', '0.2810']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5803', '1.5236', '1.5339', '1.7572', '1.6425']
Training action VAE models...
Action VAE losses: ['1.3022', '1.3417', '1.3045', '1.2433', '1.4058']
CM score components:
transition disagreement: 0.4195
reward disagreement: 0.4144
state disagreement: 0.5327
action disagreement: 0.5232
total CM score: 1.8899
friction is complete. CM score: 1.8899
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 32.419
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 32.419
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 32.419
total data points collected: 50005
average episode length: 10001.0
average episode reward: 32.419
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2249', '7.8598', '7.1224', '7.1554', '7.5087']
Training reward models...
Reward model losses: ['0.0518', '0.3287', '0.0409', '0.0143', '0.0556']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2032', '1.1267', '1.2895', '1.3262', '1.2507']
Training action VAE models...
Action VAE losses: ['1.2233', '1.0980', '1.2311', '1.2783', '1.3457']
CM score components:
transition disagreement: 0.4399
reward disagreement: 0.1562
state disagreement: 0.4828
action disagreement: 0.5347
total CM score: 1.6136
visual is complete. CM score: 1.6136
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 58.431
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 4.301
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -0.925
total data points collected: 50005
average episode length: 10001.0
average episode reward: 12.372
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7735', '7.7459', '7.7134', '6.6095', '7.4599']
Training reward models...
Reward model losses: ['2.7458', '0.8578', '0.1528', '0.0092', '0.0319']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1840', '1.2201', '1.2203', '1.1710', '1.1201']
Training action VAE models...
Action VAE losses: ['1.2178', '1.3835', '1.2967', '1.3183', '1.2215']
CM score components:
transition disagreement: 0.4445
reward disagreement: 0.6663
state disagreement: 0.4874
action disagreement: 0.5520
total CM score: 2.1501
pose is complete. CM score: 2.1501
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 55.634
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 1.657
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.373
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.287
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4599', '7.2747', '8.3462', '7.3700', '7.2637']
Training reward models...
Reward model losses: ['0.0528', '0.0346', '0.0185', '0.2616', '0.1141']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2219', '1.2535', '1.1588', '1.1717', '1.2360']
Training action VAE models...
Action VAE losses: ['1.3689', '1.3607', '1.3185', '1.3875', '1.4208']
CM score components:
transition disagreement: 0.3771
reward disagreement: 0.1318
state disagreement: 0.4862
action disagreement: 0.5768
total CM score: 1.5719
random is complete. CM score: 1.5719
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.44e+03  |
|    ep_rew_mean     | 2.6303556 |
| time/              |           |
|    fps             | 441       |
|    iterations      | 1         |
|    time_elapsed    | 9         |
|    total_timesteps | 5636096   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.44e+03    |
|    ep_rew_mean          | 2.6303556   |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 2           |
|    time_elapsed         | 19          |
|    total_timesteps      | 5640192     |
| train/                  |             |
|    approx_kl            | 0.011696888 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.3       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.597      |
|    n_updates            | 3315        |
|    policy_gradient_loss | -0.0238     |
|    std                  | 5.62        |
|    value_loss           | 0.00698     |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.52e+03    |
|    ep_rew_mean          | 2.50395     |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5644288     |
| train/                  |             |
|    approx_kl            | 0.009459021 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.3       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.587      |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.0167     |
|    std                  | 5.64        |
|    value_loss           | 0.00171     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.52e+03    |
|    ep_rew_mean          | 2.50395     |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5648384     |
| train/                  |             |
|    approx_kl            | 0.012913071 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.3       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.582      |
|    n_updates            | 3345        |
|    policy_gradient_loss | -0.0253     |
|    std                  | 5.67        |
|    value_loss           | 0.0601      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.52e+03    |
|    ep_rew_mean          | 2.4051948   |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5652480     |
| train/                  |             |
|    approx_kl            | 0.015057335 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.000105    |
|    loss                 | -0.586      |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.0244     |
|    std                  | 5.7         |
|    value_loss           | 0.0198      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 5.52e+03     |
|    ep_rew_mean          | 2.4051948    |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 6            |
|    time_elapsed         | 57           |
|    total_timesteps      | 5656576      |
| train/                  |              |
|    approx_kl            | 0.0132359555 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.2          |
|    entropy_loss         | -28.4        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.000105     |
|    loss                 | -0.606       |
|    n_updates            | 3375         |
|    policy_gradient_loss | -0.0312      |
|    std                  | 5.73         |
|    value_loss           | 0.0107       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.52e+03    |
|    ep_rew_mean          | 2.4051948   |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 7           |
|    time_elapsed         | 66          |
|    total_timesteps      | 5660672     |
| train/                  |             |
|    approx_kl            | 0.010254025 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.5       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.603      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0232     |
|    std                  | 5.75        |
|    value_loss           | 0.00671     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.42e+03    |
|    ep_rew_mean          | 1.6641166   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 8           |
|    time_elapsed         | 76          |
|    total_timesteps      | 5664768     |
| train/                  |             |
|    approx_kl            | 0.009165806 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.594      |
|    n_updates            | 3405        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 5.76        |
|    value_loss           | 0.00278     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.42e+03    |
|    ep_rew_mean          | 1.6641166   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 9           |
|    time_elapsed         | 86          |
|    total_timesteps      | 5668864     |
| train/                  |             |
|    approx_kl            | 0.012002008 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.5       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.000105    |
|    loss                 | -0.602      |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.0282     |
|    std                  | 5.77        |
|    value_loss           | 0.0154      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.51e+03    |
|    ep_rew_mean          | 1.5648042   |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 10          |
|    time_elapsed         | 96          |
|    total_timesteps      | 5672960     |
| train/                  |             |
|    approx_kl            | 0.010354565 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.5       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.000105    |
|    loss                 | -0.601      |
|    n_updates            | 3435        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 5.79        |
|    value_loss           | 0.00255     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.51e+03    |
|    ep_rew_mean          | 1.5648042   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 11          |
|    time_elapsed         | 105         |
|    total_timesteps      | 5677056     |
| train/                  |             |
|    approx_kl            | 0.010875259 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.6        |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.028      |
|    std                  | 5.82        |
|    value_loss           | 0.0224      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 5.51e+03  |
|    ep_rew_mean          | 1.5648042 |
| time/                   |           |
|    fps                  | 427       |
|    iterations           | 12        |
|    time_elapsed         | 114       |
|    total_timesteps      | 5681152   |
| train/                  |           |
|    approx_kl            | 0.0111186 |
|    clip_fraction        | 0.126     |
|    clip_range           | 0.2       |
|    entropy_loss         | -28.6     |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.000105  |
|    loss                 | -0.601    |
|    n_updates            | 3465      |
|    policy_gradient_loss | -0.0228   |
|    std                  | 5.84      |
|    value_loss           | 0.00744   |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.6e+03     |
|    ep_rew_mean          | 1.4878814   |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 13          |
|    time_elapsed         | 124         |
|    total_timesteps      | 5685248     |
| train/                  |             |
|    approx_kl            | 0.010971954 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.000105    |
|    loss                 | -0.604      |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0233     |
|    std                  | 5.84        |
|    value_loss           | 0.00438     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_11.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=1.839, success=False
episode 2: length=10001, reward=1.839, success=False
episode 3: length=10001, reward=1.839, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 1.839
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 89.153
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 1.524
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 40.915
total data points collected: 40026
average episode length: 8005.2
average episode reward: 26.730
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40026, 56]), actions: torch.Size([40026, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1840', '7.0962', '7.8166', '7.8356', '8.8855']
Training reward models...
Reward model losses: ['0.2569', '0.0165', '0.2621', '0.0044', '0.0160']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2920', '1.2687', '1.3588', '1.2964', '1.0851']
Training action VAE models...
Action VAE losses: ['1.4352', '1.4522', '1.2897', '1.3843', '1.3463']
CM score components:
transition disagreement: 0.4477
reward disagreement: 0.1529
state disagreement: 0.4817
action disagreement: 0.5800
total CM score: 1.6623
goal is complete. CM score: 1.6623
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 33 steps, reward: 0.122
Reset #2: mass intervention applied (success: True)
episode 2: 31 steps, reward: -0.193
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.284
total data points collected: 30067
average episode length: 6013.4
average episode reward: 0.733
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30067, 56]), actions: torch.Size([30067, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4132', '7.2564', '7.6549', '8.6154', '7.9230']
Training reward models...
Reward model losses: ['0.0114', '0.1572', '1.8518', '0.2572', '0.1193']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3932', '1.6241', '1.4611', '1.2974', '1.4307']
Training action VAE models...
Action VAE losses: ['1.3804', '1.2968', '1.2901', '1.3470', '1.3415']
CM score components:
transition disagreement: 0.4362
reward disagreement: 0.4759
state disagreement: 0.5463
action disagreement: 0.5286
total CM score: 1.9871
mass is complete. CM score: 1.9871
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 4.602
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 7.050
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.912
total data points collected: 50005
average episode length: 10001.0
average episode reward: 3.548
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2291', '7.6580', '6.9176', '7.6239', '6.4186']
Training reward models...
Reward model losses: ['0.0087', '0.0118', '0.1183', '0.0514', '0.2018']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0795', '1.0721', '1.1454', '1.1795', '1.0742']
Training action VAE models...
Action VAE losses: ['1.4322', '1.4853', '1.2782', '1.2627', '1.4835']
CM score components:
transition disagreement: 0.4464
reward disagreement: 0.1180
state disagreement: 0.4741
action disagreement: 0.5394
total CM score: 1.5779
friction is complete. CM score: 1.5779
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.839
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.839
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.839
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.839
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8802', '7.4431', '7.2498', '6.7877', '7.5837']
Training reward models...
Reward model losses: ['0.0037', '0.0298', '0.2440', '0.2251', '0.1157']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8177', '0.8501', '0.9203', '0.9738', '0.7939']
Training action VAE models...
Action VAE losses: ['1.4345', '1.2291', '1.2662', '1.1856', '1.3858']
CM score components:
transition disagreement: 0.4262
reward disagreement: 0.1163
state disagreement: 0.4748
action disagreement: 0.5731
total CM score: 1.5904
visual is complete. CM score: 1.5904
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 70.291
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.224
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.200
total data points collected: 40030
average episode length: 8006.0
average episode reward: 14.349
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40030, 56]), actions: torch.Size([40030, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4890', '7.4233', '7.5903', '7.2942', '7.2965']
Training reward models...
Reward model losses: ['0.2484', '0.0340', '0.3113', '0.0102', '0.0076']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1050', '1.1914', '1.1944', '1.2174', '1.0924']
Training action VAE models...
Action VAE losses: ['1.3123', '1.3435', '1.3346', '1.5067', '1.4418']
CM score components:
transition disagreement: 0.4264
reward disagreement: 0.1593
state disagreement: 0.4702
action disagreement: 0.5301
total CM score: 1.5860
pose is complete. CM score: 1.5860
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 77.806
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 1.680
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.357
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.596
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7516', '7.6499', '8.3906', '7.7603', '7.5067']
Training reward models...
Reward model losses: ['0.0305', '0.0276', '0.2944', '0.0343', '0.2019']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2330', '1.3593', '1.4192', '1.3464', '1.2209']
Training action VAE models...
Action VAE losses: ['1.4140', '1.5707', '1.3810', '1.3582', '1.4976']
CM score components:
transition disagreement: 0.4221
reward disagreement: 0.1502
state disagreement: 0.4941
action disagreement: 0.5683
total CM score: 1.6347
random is complete. CM score: 1.6347
2025-07-16 19:42:17,553 3266759 INFO Meta-Episode 12/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_11.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 89.153
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 1.524
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 40.915
total data points collected: 40026
average episode length: 8005.2
average episode reward: 26.730
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40026, 56]), actions: torch.Size([40026, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3144', '7.1652', '6.7974', '7.0992', '7.5939']
Training reward models...
Reward model losses: ['0.1756', '0.2114', '0.2361', '0.0068', '0.0206']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1502', '1.2900', '1.2667', '1.2414', '1.2428']
Training action VAE models...
Action VAE losses: ['1.5567', '1.3532', '1.3819', '1.2085', '1.2183']
CM score components:
transition disagreement: 0.4097
reward disagreement: 0.0715
state disagreement: 0.4603
action disagreement: 0.5578
total CM score: 1.4994
goal is complete. CM score: 1.4994
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 33 steps, reward: 0.122
Reset #2: mass intervention applied (success: True)
episode 2: 31 steps, reward: -0.193
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.284
total data points collected: 30067
average episode length: 6013.4
average episode reward: 0.733
termination reasons: ['success', 'success', 'max_length', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30067, 56]), actions: torch.Size([30067, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0629', '7.6994', '6.9793', '6.8529', '8.2839']
Training reward models...
Reward model losses: ['0.0042', '0.1917', '0.0065', '0.3938', '0.5970']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3468', '1.2317', '1.2382', '1.3626', '1.4634']
Training action VAE models...
Action VAE losses: ['1.3945', '1.3247', '1.3210', '1.4104', '1.3363']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.1748
state disagreement: 0.5180
action disagreement: 0.5534
total CM score: 1.6665
mass is complete. CM score: 1.6665
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 4.602
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 7.050
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.912
total data points collected: 50005
average episode length: 10001.0
average episode reward: 3.548
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9708', '7.3477', '6.7590', '7.6108', '7.1166']
Training reward models...
Reward model losses: ['0.0593', '1.1019', '0.4611', '0.0207', '0.3073']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1665', '1.1179', '1.1080', '1.1696', '1.0156']
Training action VAE models...
Action VAE losses: ['1.3586', '1.4048', '1.2387', '1.3659', '1.4942']
CM score components:
transition disagreement: 0.4180
reward disagreement: 0.4207
state disagreement: 0.4897
action disagreement: 0.5263
total CM score: 1.8547
friction is complete. CM score: 1.8547
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.839
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.839
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.839
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.839
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0843', '7.7757', '7.1022', '7.1504', '7.4921']
Training reward models...
Reward model losses: ['0.0523', '0.4198', '0.0692', '0.0131', '0.0539']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9635', '0.8212', '0.9434', '0.9872', '0.9008']
Training action VAE models...
Action VAE losses: ['1.2261', '1.1174', '1.2809', '1.1782', '1.4855']
CM score components:
transition disagreement: 0.4406
reward disagreement: 0.1808
state disagreement: 0.4561
action disagreement: 0.5317
total CM score: 1.6092
visual is complete. CM score: 1.6092
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 70.291
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.224
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.200
total data points collected: 40030
average episode length: 8006.0
average episode reward: 14.349
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40030, 56]), actions: torch.Size([40030, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7894', '7.7242', '7.6896', '6.6207', '7.4364']
Training reward models...
Reward model losses: ['2.8501', '0.8650', '0.1383', '0.0113', '0.0356']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1631', '1.2455', '1.2388', '1.2010', '1.1290']
Training action VAE models...
Action VAE losses: ['1.3160', '1.4645', '1.3523', '1.3374', '1.3364']
CM score components:
transition disagreement: 0.4448
reward disagreement: 0.6857
state disagreement: 0.4906
action disagreement: 0.5529
total CM score: 2.1741
pose is complete. CM score: 2.1741
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 77.806
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 1.680
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.357
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.596
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4438', '7.3007', '8.3299', '7.4188', '7.2929']
Training reward models...
Reward model losses: ['0.0873', '0.0464', '0.0398', '0.2874', '0.1393']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2816', '1.3374', '1.2764', '1.2404', '1.3194']
Training action VAE models...
Action VAE losses: ['1.4836', '1.3853', '1.3800', '1.4809', '1.5132']
CM score components:
transition disagreement: 0.3756
reward disagreement: 0.1277
state disagreement: 0.5003
action disagreement: 0.5863
total CM score: 1.5900
random is complete. CM score: 1.5900
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.6e+03   |
|    ep_rew_mean     | 1.4878814 |
| time/              |           |
|    fps             | 473       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5689344   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.6e+03    |
|    ep_rew_mean          | 1.4878814  |
| time/                   |            |
|    fps                  | 455        |
|    iterations           | 2          |
|    time_elapsed         | 17         |
|    total_timesteps      | 5693440    |
| train/                  |            |
|    approx_kl            | 0.01210285 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.6      |
|    explained_variance   | 0.497      |
|    learning_rate        | 9.41e-05   |
|    loss                 | -0.577     |
|    n_updates            | 3510       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 5.86       |
|    value_loss           | 0.0451     |
----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.7e+03     |
|    ep_rew_mean          | 1.4585152   |
| time/                   |             |
|    fps                  | 450         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5697536     |
| train/                  |             |
|    approx_kl            | 0.012470563 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.7       |
|    explained_variance   | 0.914       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.597      |
|    n_updates            | 3525        |
|    policy_gradient_loss | -0.0228     |
|    std                  | 5.88        |
|    value_loss           | 0.00849     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.7e+03     |
|    ep_rew_mean          | 1.4585152   |
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 4           |
|    time_elapsed         | 36          |
|    total_timesteps      | 5701632     |
| train/                  |             |
|    approx_kl            | 0.013378504 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.7       |
|    explained_variance   | 0.673       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.592      |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.0288     |
|    std                  | 5.9         |
|    value_loss           | 0.0312      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.7e+03     |
|    ep_rew_mean          | 0.98825616  |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 5           |
|    time_elapsed         | 46          |
|    total_timesteps      | 5705728     |
| train/                  |             |
|    approx_kl            | 0.010510949 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.7       |
|    explained_variance   | 0.91        |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.599      |
|    n_updates            | 3555        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 5.92        |
|    value_loss           | 0.00464     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.7e+03     |
|    ep_rew_mean          | 0.98825616  |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 6           |
|    time_elapsed         | 55          |
|    total_timesteps      | 5709824     |
| train/                  |             |
|    approx_kl            | 0.012722561 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.7       |
|    explained_variance   | 0.647       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.603      |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.0239     |
|    std                  | 5.94        |
|    value_loss           | 0.0176      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.7e+03     |
|    ep_rew_mean          | 0.98825616  |
| time/                   |             |
|    fps                  | 443         |
|    iterations           | 7           |
|    time_elapsed         | 64          |
|    total_timesteps      | 5713920     |
| train/                  |             |
|    approx_kl            | 0.011514549 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.8       |
|    explained_variance   | 0.876       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.604      |
|    n_updates            | 3585        |
|    policy_gradient_loss | -0.022      |
|    std                  | 5.96        |
|    value_loss           | 0.00759     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.78e+03   |
|    ep_rew_mean          | 0.79480195 |
| time/                   |            |
|    fps                  | 439        |
|    iterations           | 8          |
|    time_elapsed         | 74         |
|    total_timesteps      | 5718016    |
| train/                  |            |
|    approx_kl            | 0.01061228 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.8      |
|    explained_variance   | 0.557      |
|    learning_rate        | 9.41e-05   |
|    loss                 | -0.593     |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 5.96       |
|    value_loss           | 0.0444     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 5.78e+03     |
|    ep_rew_mean          | 0.79480195   |
| time/                   |              |
|    fps                  | 438          |
|    iterations           | 9            |
|    time_elapsed         | 84           |
|    total_timesteps      | 5722112      |
| train/                  |              |
|    approx_kl            | 0.0106977895 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.2          |
|    entropy_loss         | -28.8        |
|    explained_variance   | 0.695        |
|    learning_rate        | 9.41e-05     |
|    loss                 | -0.606       |
|    n_updates            | 3615         |
|    policy_gradient_loss | -0.0252      |
|    std                  | 5.98         |
|    value_loss           | 0.0296       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.88e+03    |
|    ep_rew_mean          | 0.75532013  |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 10          |
|    time_elapsed         | 94          |
|    total_timesteps      | 5726208     |
| train/                  |             |
|    approx_kl            | 0.010276135 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.8       |
|    explained_variance   | 0.858       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.599      |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 6           |
|    value_loss           | 0.008       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.88e+03    |
|    ep_rew_mean          | 0.75532013  |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 11          |
|    time_elapsed         | 103         |
|    total_timesteps      | 5730304     |
| train/                  |             |
|    approx_kl            | 0.009598162 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.824       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.599      |
|    n_updates            | 3645        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 6.01        |
|    value_loss           | 0.0234      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.88e+03    |
|    ep_rew_mean          | 0.75532013  |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 12          |
|    time_elapsed         | 113         |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 0.010605614 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.908       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.593      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0158     |
|    std                  | 6.02        |
|    value_loss           | 0.00458     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.98e+03    |
|    ep_rew_mean          | 0.7561969   |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 13          |
|    time_elapsed         | 123         |
|    total_timesteps      | 5738496     |
| train/                  |             |
|    approx_kl            | 0.011580465 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.888       |
|    learning_rate        | 9.41e-05    |
|    loss                 | -0.587      |
|    n_updates            | 3675        |
|    policy_gradient_loss | -0.0172     |
|    std                  | 6.03        |
|    value_loss           | 0.00486     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_12.zip

evaluating student performance (5 episodes)...
episode 1: length=32, reward=-0.096, success=True
episode 2: length=32, reward=-0.096, success=True
episode 3: length=32, reward=-0.096, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.096
average episode length: 32.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 69.701
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 19.907
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 6.779
total data points collected: 40028
average episode length: 8005.6
average episode reward: 20.799
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2140', '7.0866', '7.7845', '7.7913', '8.8335']
Training reward models...
Reward model losses: ['0.2804', '0.0103', '0.2608', '0.0027', '0.0142']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1159', '1.1863', '1.0810', '1.1059', '0.9217']
Training action VAE models...
Action VAE losses: ['1.1882', '1.1781', '1.1603', '1.2743', '1.2402']
CM score components:
transition disagreement: 0.4492
reward disagreement: 0.1562
state disagreement: 0.4750
action disagreement: 0.5771
total CM score: 1.6575
goal is complete. CM score: 1.6575
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 37.094
Reset #2: mass intervention applied (success: True)
episode 2: 40 steps, reward: -0.562
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 1.499
total data points collected: 40044
average episode length: 8008.8
average episode reward: 31.704
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40044, 56]), actions: torch.Size([40044, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4147', '7.2544', '7.6588', '8.5626', '7.9393']
Training reward models...
Reward model losses: ['0.0111', '0.1346', '1.8557', '0.2890', '0.1021']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1535', '1.3444', '1.2189', '1.0691', '1.1853']
Training action VAE models...
Action VAE losses: ['1.3914', '1.3073', '1.4311', '1.3058', '1.3683']
CM score components:
transition disagreement: 0.4332
reward disagreement: 0.4880
state disagreement: 0.5097
action disagreement: 0.5524
total CM score: 1.9832
mass is complete. CM score: 1.9832
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 32 steps, reward: -0.051
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: -0.096
Reset #3: friction intervention applied (success: True)
episode 3: 32 steps, reward: -0.051
total data points collected: 160
average episode length: 32.0
average episode reward: -0.060
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([160, 56]), actions: torch.Size([160, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2744', '7.6679', '7.0832', '7.7741', '6.4842']
Training reward models...
Reward model losses: ['0.0268', '0.0153', '0.1172', '0.0402', '0.2662']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7172', '1.7136', '1.8688', '1.6221', '2.0963']
Training action VAE models...
Action VAE losses: ['1.4183', '1.4540', '1.3142', '1.2932', '1.5096']
CM score components:
transition disagreement: 0.4555
reward disagreement: 0.1477
state disagreement: 0.5651
action disagreement: 0.5517
total CM score: 1.7200
friction is complete. CM score: 1.7200
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 32 steps, reward: -0.096
Reset #2: visual intervention applied (success: True)
episode 2: 32 steps, reward: -0.096
Reset #3: visual intervention applied (success: True)
episode 3: 32 steps, reward: -0.096
total data points collected: 160
average episode length: 32.0
average episode reward: -0.096
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([160, 56]), actions: torch.Size([160, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9945', '7.6230', '7.3167', '6.8773', '7.6910']
Training reward models...
Reward model losses: ['0.0078', '0.0589', '0.2783', '0.2312', '0.1670']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8107', '1.7456', '1.6216', '2.1959', '1.7735']
Training action VAE models...
Action VAE losses: ['1.5649', '1.5025', '1.5277', '1.4461', '1.4630']
CM score components:
transition disagreement: 0.4359
reward disagreement: 0.1466
state disagreement: 0.5636
action disagreement: 0.5893
total CM score: 1.7353
visual is complete. CM score: 1.7353
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 21 steps, reward: 0.515
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.424
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.283
total data points collected: 40025
average episode length: 8005.0
average episode reward: -0.056
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4580', '7.4423', '7.5772', '7.3010', '7.2640']
Training reward models...
Reward model losses: ['0.2129', '0.0417', '0.2615', '0.0107', '0.0091']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1847', '1.2180', '1.1798', '1.1926', '1.1339']
Training action VAE models...
Action VAE losses: ['1.3133', '1.3474', '1.3590', '1.4973', '1.4823']
CM score components:
transition disagreement: 0.4261
reward disagreement: 0.1315
state disagreement: 0.4722
action disagreement: 0.5284
total CM score: 1.5582
pose is complete. CM score: 1.5582
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 0.795
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.390
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -1.233
total data points collected: 50005
average episode length: 10001.0
average episode reward: 2.391
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7400', '7.6092', '8.3666', '7.7640', '7.4808']
Training reward models...
Reward model losses: ['0.0185', '0.0153', '0.3211', '0.0230', '0.1413']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2148', '1.2713', '1.2983', '1.2129', '1.1667']
Training action VAE models...
Action VAE losses: ['1.4137', '1.4733', '1.4779', '1.3429', '1.4288']
CM score components:
transition disagreement: 0.4144
reward disagreement: 0.1600
state disagreement: 0.4800
action disagreement: 0.5647
total CM score: 1.6191
random is complete. CM score: 1.6191
2025-07-16 19:58:59,971 3266759 INFO Meta-Episode 13/30: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_12.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 69.701
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 19.907
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 6.779
total data points collected: 40028
average episode length: 8005.6
average episode reward: 20.799
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2700', '7.1540', '6.7725', '7.1017', '7.6578']
Training reward models...
Reward model losses: ['0.1947', '0.2467', '0.2329', '0.0028', '0.0170']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9713', '1.1133', '1.0657', '1.0425', '1.0618']
Training action VAE models...
Action VAE losses: ['1.3626', '1.2066', '1.1339', '1.1445', '1.1077']
CM score components:
transition disagreement: 0.4052
reward disagreement: 0.0668
state disagreement: 0.4459
action disagreement: 0.5608
total CM score: 1.4787
goal is complete. CM score: 1.4787
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 37.094
Reset #2: mass intervention applied (success: True)
episode 2: 40 steps, reward: -0.562
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 1.499
total data points collected: 40044
average episode length: 8008.8
average episode reward: 31.704
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40044, 56]), actions: torch.Size([40044, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0792', '7.7225', '7.0180', '6.8458', '8.2613']
Training reward models...
Reward model losses: ['0.0074', '0.2315', '0.0089', '0.3968', '0.6152']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2337', '1.0321', '1.0693', '1.1708', '1.2539']
Training action VAE models...
Action VAE losses: ['1.4487', '1.4025', '1.3166', '1.3904', '1.2528']
CM score components:
transition disagreement: 0.4231
reward disagreement: 0.1976
state disagreement: 0.5052
action disagreement: 0.5458
total CM score: 1.6716
mass is complete. CM score: 1.6716
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 32 steps, reward: -0.051
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: -0.096
Reset #3: friction intervention applied (success: True)
episode 3: 32 steps, reward: -0.051
total data points collected: 160
average episode length: 32.0
average episode reward: -0.060
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([160, 56]), actions: torch.Size([160, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9904', '7.3391', '6.9327', '7.7483', '7.2156']
Training reward models...
Reward model losses: ['0.0605', '1.2218', '0.3856', '0.0263', '0.2344']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7449', '1.6535', '1.6773', '1.9333', '1.8082']
Training action VAE models...
Action VAE losses: ['1.3238', '1.3748', '1.2982', '1.3118', '1.4330']
CM score components:
transition disagreement: 0.4239
reward disagreement: 0.4225
state disagreement: 0.5396
action disagreement: 0.5282
total CM score: 1.9142
friction is complete. CM score: 1.9142
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 32 steps, reward: -0.096
Reset #2: visual intervention applied (success: True)
episode 2: 32 steps, reward: -0.096
Reset #3: visual intervention applied (success: True)
episode 3: 32 steps, reward: -0.096
total data points collected: 160
average episode length: 32.0
average episode reward: -0.096
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([160, 56]), actions: torch.Size([160, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2478', '7.8562', '7.1161', '7.2760', '7.5529']
Training reward models...
Reward model losses: ['0.0856', '0.3608', '0.0777', '0.0116', '0.0772']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7477', '1.6999', '1.8281', '1.6343', '1.6424']
Training action VAE models...
Action VAE losses: ['1.3947', '1.2931', '1.3945', '1.4543', '1.4567']
CM score components:
transition disagreement: 0.4485
reward disagreement: 0.1708
state disagreement: 0.5104
action disagreement: 0.5379
total CM score: 1.6675
visual is complete. CM score: 1.6675
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 21 steps, reward: 0.515
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.424
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.283
total data points collected: 40025
average episode length: 8005.0
average episode reward: -0.056
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40025, 56]), actions: torch.Size([40025, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7820', '7.7206', '7.7046', '6.5879', '7.4466']
Training reward models...
Reward model losses: ['2.7775', '0.8903', '0.1479', '0.0108', '0.0371']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2104', '1.2639', '1.2697', '1.2558', '1.1253']
Training action VAE models...
Action VAE losses: ['1.3106', '1.5480', '1.3911', '1.3625', '1.2926']
CM score components:
transition disagreement: 0.4470
reward disagreement: 0.6756
state disagreement: 0.4957
action disagreement: 0.5567
total CM score: 2.1750
pose is complete. CM score: 2.1750
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 0.795
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.390
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: -1.233
total data points collected: 50005
average episode length: 10001.0
average episode reward: 2.391
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4237', '7.2905', '8.2955', '7.4215', '7.3312']
Training reward models...
Reward model losses: ['0.0460', '0.0400', '0.0229', '0.3059', '0.1452']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2531', '1.2240', '1.2001', '1.2312', '1.3096']
Training action VAE models...
Action VAE losses: ['1.5287', '1.4739', '1.3958', '1.5209', '1.4668']
CM score components:
transition disagreement: 0.3778
reward disagreement: 0.1408
state disagreement: 0.5022
action disagreement: 0.5908
total CM score: 1.6116
random is complete. CM score: 1.6116
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.98e+03  |
|    ep_rew_mean     | 0.7561969 |
| time/              |           |
|    fps             | 464       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5742592   |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.98e+03   |
|    ep_rew_mean          | 0.7561969  |
| time/                   |            |
|    fps                  | 446        |
|    iterations           | 2          |
|    time_elapsed         | 18         |
|    total_timesteps      | 5746688    |
| train/                  |            |
|    approx_kl            | 0.01097903 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29        |
|    explained_variance   | 0.538      |
|    learning_rate        | 8.47e-05   |
|    loss                 | -0.597     |
|    n_updates            | 3705       |
|    policy_gradient_loss | -0.025     |
|    std                  | 6.1        |
|    value_loss           | 0.0345     |
----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.07e+03    |
|    ep_rew_mean          | 0.6884122   |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5750784     |
| train/                  |             |
|    approx_kl            | 0.009928899 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29         |
|    explained_variance   | 0.893       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.598      |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.0176     |
|    std                  | 6.12        |
|    value_loss           | 0.00473     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.07e+03    |
|    ep_rew_mean          | 0.6884122   |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5754880     |
| train/                  |             |
|    approx_kl            | 0.010315474 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29         |
|    explained_variance   | 0.597       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.583      |
|    n_updates            | 3735        |
|    policy_gradient_loss | -0.0241     |
|    std                  | 6.13        |
|    value_loss           | 0.066       |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.46019763  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5758976     |
| train/                  |             |
|    approx_kl            | 0.009763556 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.939       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.604      |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.0159     |
|    std                  | 6.13        |
|    value_loss           | 0.00327     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.46019763  |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 5763072     |
| train/                  |             |
|    approx_kl            | 0.009460719 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.798       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.611      |
|    n_updates            | 3765        |
|    policy_gradient_loss | -0.0222     |
|    std                  | 6.14        |
|    value_loss           | 0.0132      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.15e+03   |
|    ep_rew_mean          | 0.46019763 |
| time/                   |            |
|    fps                  | 425        |
|    iterations           | 7          |
|    time_elapsed         | 67         |
|    total_timesteps      | 5767168    |
| train/                  |            |
|    approx_kl            | 0.0092236  |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.1      |
|    explained_variance   | 0.95       |
|    learning_rate        | 8.47e-05   |
|    loss                 | -0.608     |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.0164    |
|    std                  | 6.15       |
|    value_loss           | 0.00398    |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.15e+03     |
|    ep_rew_mean          | 0.46698716   |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 8            |
|    time_elapsed         | 76           |
|    total_timesteps      | 5771264      |
| train/                  |              |
|    approx_kl            | 0.0076534655 |
|    clip_fraction        | 0.0757       |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.1        |
|    explained_variance   | 0.96         |
|    learning_rate        | 8.47e-05     |
|    loss                 | -0.601       |
|    n_updates            | 3795         |
|    policy_gradient_loss | -0.0124      |
|    std                  | 6.15         |
|    value_loss           | 0.003        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.46698716  |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 9           |
|    time_elapsed         | 86          |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 0.011573889 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.685       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.602      |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.0262     |
|    std                  | 6.18        |
|    value_loss           | 0.0381      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.3880242   |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 10          |
|    time_elapsed         | 96          |
|    total_timesteps      | 5779456     |
| train/                  |             |
|    approx_kl            | 0.009437097 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.782       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.618      |
|    n_updates            | 3825        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 6.19        |
|    value_loss           | 0.00721     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.3880242   |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 11          |
|    time_elapsed         | 106         |
|    total_timesteps      | 5783552     |
| train/                  |             |
|    approx_kl            | 0.010378214 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.651       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.607      |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.025      |
|    std                  | 6.21        |
|    value_loss           | 0.0111      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.15e+03    |
|    ep_rew_mean          | 0.3880242   |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 12          |
|    time_elapsed         | 116         |
|    total_timesteps      | 5787648     |
| train/                  |             |
|    approx_kl            | 0.009181417 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.2       |
|    explained_variance   | 0.653       |
|    learning_rate        | 8.47e-05    |
|    loss                 | -0.603      |
|    n_updates            | 3855        |
|    policy_gradient_loss | -0.0163     |
|    std                  | 6.23        |
|    value_loss           | 0.0224      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.25e+03     |
|    ep_rew_mean          | 0.36383548   |
| time/                   |              |
|    fps                  | 423          |
|    iterations           | 13           |
|    time_elapsed         | 125          |
|    total_timesteps      | 5791744      |
| train/                  |              |
|    approx_kl            | 0.0085858125 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.2        |
|    explained_variance   | 0.942        |
|    learning_rate        | 8.47e-05     |
|    loss                 | -0.611       |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.0181      |
|    std                  | 6.26         |
|    value_loss           | 0.00348      |
------------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_13.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=3.925, success=False
episode 2: length=10001, reward=3.925, success=False
episode 3: length=10001, reward=3.925, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 3.925
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 1.580
Reset #2: goal intervention applied (success: True)
episode 2: 24 steps, reward: 0.797
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 9.972
total data points collected: 40028
average episode length: 8005.6
average episode reward: 2.618
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2332', '7.1237', '7.8318', '7.8352', '8.8459']
Training reward models...
Reward model losses: ['0.2536', '0.0108', '0.2649', '0.0042', '0.0177']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2443', '1.2904', '1.2648', '1.2971', '1.0391']
Training action VAE models...
Action VAE losses: ['1.4068', '1.4468', '1.3640', '1.4199', '1.4196']
CM score components:
transition disagreement: 0.4469
reward disagreement: 0.1443
state disagreement: 0.4808
action disagreement: 0.5807
total CM score: 1.6527
goal is complete. CM score: 1.6527
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 50.696
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 10.155
Reset #3: mass intervention applied (success: True)
episode 3: 90 steps, reward: 0.821
total data points collected: 40094
average episode length: 8018.8
average episode reward: 33.490
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40094, 56]), actions: torch.Size([40094, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4255', '7.2525', '7.6560', '8.5863', '7.9369']
Training reward models...
Reward model losses: ['0.0096', '0.1542', '1.8419', '0.2453', '0.1150']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1968', '1.3916', '1.2539', '1.1041', '1.1802']
Training action VAE models...
Action VAE losses: ['1.3296', '1.4169', '1.3375', '1.3427', '1.3493']
CM score components:
transition disagreement: 0.4339
reward disagreement: 0.4785
state disagreement: 0.5155
action disagreement: 0.5363
total CM score: 1.9642
mass is complete. CM score: 1.9642
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.604
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 31.264
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.416
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.186
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2244', '7.6451', '6.9177', '7.6246', '6.4349']
Training reward models...
Reward model losses: ['0.0103', '0.0123', '0.1229', '0.0496', '0.2060']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1334', '1.1289', '1.2234', '1.2298', '1.2137']
Training action VAE models...
Action VAE losses: ['1.3796', '1.4507', '1.2539', '1.2882', '1.4710']
CM score components:
transition disagreement: 0.4439
reward disagreement: 0.1119
state disagreement: 0.4845
action disagreement: 0.5377
total CM score: 1.5779
friction is complete. CM score: 1.5779
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 3.925
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 3.925
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 3.925
total data points collected: 50005
average episode length: 10001.0
average episode reward: 3.925
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9578', '7.4465', '7.2185', '6.7715', '7.6389']
Training reward models...
Reward model losses: ['0.0021', '0.0257', '0.2282', '0.1819', '0.1650']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7552', '0.7601', '0.8337', '0.8709', '0.7341']
Training action VAE models...
Action VAE losses: ['1.2493', '1.1859', '1.1651', '1.1764', '1.2176']
CM score components:
transition disagreement: 0.4189
reward disagreement: 0.1153
state disagreement: 0.4694
action disagreement: 0.5746
total CM score: 1.5783
visual is complete. CM score: 1.5783
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 105.929
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 20.087
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 13.226
total data points collected: 40029
average episode length: 8005.8
average episode reward: 42.217
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4361', '7.3803', '7.5432', '7.3461', '7.2464']
Training reward models...
Reward model losses: ['0.2094', '0.0507', '0.2813', '0.0141', '0.0054']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1126', '1.1866', '1.1925', '1.2365', '1.1450']
Training action VAE models...
Action VAE losses: ['1.2805', '1.3642', '1.3755', '1.4618', '1.4490']
CM score components:
transition disagreement: 0.4243
reward disagreement: 0.1383
state disagreement: 0.4746
action disagreement: 0.5217
total CM score: 1.5589
pose is complete. CM score: 1.5589
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.232
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.180
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.509
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.679
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7620', '7.6319', '8.4013', '7.7768', '7.4870']
Training reward models...
Reward model losses: ['0.0625', '0.0558', '0.3554', '0.0681', '0.2013']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2146', '1.3150', '1.3914', '1.2497', '1.1813']
Training action VAE models...
Action VAE losses: ['1.4580', '1.5535', '1.3556', '1.3331', '1.4561']
CM score components:
transition disagreement: 0.4199
reward disagreement: 0.1598
state disagreement: 0.4881
action disagreement: 0.5646
total CM score: 1.6325
random is complete. CM score: 1.6325
2025-07-16 20:17:27,794 3266759 INFO Meta-Episode 14/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_13.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 1.580
Reset #2: goal intervention applied (success: True)
episode 2: 24 steps, reward: 0.797
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 9.972
total data points collected: 40028
average episode length: 8005.6
average episode reward: 2.618
termination reasons: ['max_length', 'success', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40028, 56]), actions: torch.Size([40028, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2363', '7.0911', '6.8311', '7.1017', '7.6391']
Training reward models...
Reward model losses: ['0.1819', '0.2438', '0.2613', '0.0089', '0.0127']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1221', '1.2637', '1.2777', '1.1794', '1.2223']
Training action VAE models...
Action VAE losses: ['1.5455', '1.3776', '1.5025', '1.2781', '1.3108']
CM score components:
transition disagreement: 0.4077
reward disagreement: 0.0881
state disagreement: 0.4620
action disagreement: 0.5560
total CM score: 1.5138
goal is complete. CM score: 1.5138
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 50.696
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 10.155
Reset #3: mass intervention applied (success: True)
episode 3: 90 steps, reward: 0.821
total data points collected: 40094
average episode length: 8018.8
average episode reward: 33.490
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40094, 56]), actions: torch.Size([40094, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0673', '7.7080', '7.0095', '6.8554', '8.2679']
Training reward models...
Reward model losses: ['0.0041', '0.2315', '0.0071', '0.3664', '0.6215']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2404', '1.0982', '1.0792', '1.2604', '1.2763']
Training action VAE models...
Action VAE losses: ['1.4188', '1.3370', '1.3324', '1.4308', '1.3618']
CM score components:
transition disagreement: 0.4211
reward disagreement: 0.1996
state disagreement: 0.5160
action disagreement: 0.5466
total CM score: 1.6833
mass is complete. CM score: 1.6833
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.604
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 31.264
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.416
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.186
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9482', '7.3456', '6.7812', '7.5966', '7.1247']
Training reward models...
Reward model losses: ['0.0583', '1.0795', '0.4411', '0.0194', '0.2977']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1975', '1.1833', '1.1567', '1.2511', '1.0900']
Training action VAE models...
Action VAE losses: ['1.2898', '1.3880', '1.2361', '1.2031', '1.4603']
CM score components:
transition disagreement: 0.4166
reward disagreement: 0.4126
state disagreement: 0.4934
action disagreement: 0.5217
total CM score: 1.8442
friction is complete. CM score: 1.8442
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 3.925
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 3.925
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 3.925
total data points collected: 50005
average episode length: 10001.0
average episode reward: 3.925
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1125', '7.7632', '7.0972', '7.1273', '7.4398']
Training reward models...
Reward model losses: ['0.0934', '0.4479', '0.0882', '0.0151', '0.0723']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8531', '0.7488', '0.8078', '0.9455', '0.8235']
Training action VAE models...
Action VAE losses: ['1.0803', '1.0364', '1.0140', '1.1867', '1.2138']
CM score components:
transition disagreement: 0.4423
reward disagreement: 0.1791
state disagreement: 0.4534
action disagreement: 0.5287
total CM score: 1.6035
visual is complete. CM score: 1.6035
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 10001 steps, reward: 105.929
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 20.087
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 13.226
total data points collected: 40029
average episode length: 8005.8
average episode reward: 42.217
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40029, 56]), actions: torch.Size([40029, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7720', '7.7452', '7.6923', '6.6165', '7.4047']
Training reward models...
Reward model losses: ['2.7867', '0.9709', '0.1653', '0.0123', '0.0283']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2572', '1.2143', '1.1968', '1.2710', '1.1322']
Training action VAE models...
Action VAE losses: ['1.3510', '1.4987', '1.4286', '1.4779', '1.3402']
CM score components:
transition disagreement: 0.4417
reward disagreement: 0.6882
state disagreement: 0.4971
action disagreement: 0.5645
total CM score: 2.1915
pose is complete. CM score: 2.1915
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: -1.232
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.180
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.509
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.679
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4602', '7.3084', '8.3600', '7.4273', '7.3429']
Training reward models...
Reward model losses: ['0.0982', '0.0912', '0.0734', '0.2821', '0.1691']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3012', '1.2458', '1.2746', '1.2856', '1.2924']
Training action VAE models...
Action VAE losses: ['1.4035', '1.3630', '1.3813', '1.4753', '1.4920']
CM score components:
transition disagreement: 0.3770
reward disagreement: 0.1234
state disagreement: 0.4919
action disagreement: 0.5889
total CM score: 1.5811
random is complete. CM score: 1.5811
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 6.25e+03   |
|    ep_rew_mean     | 0.36383548 |
| time/              |            |
|    fps             | 459        |
|    iterations      | 1          |
|    time_elapsed    | 8          |
|    total_timesteps | 5795840    |
-----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.25e+03    |
|    ep_rew_mean          | 0.36383548  |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.009787306 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.81        |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.619      |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 6.29        |
|    value_loss           | 0.0103      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.25e+03    |
|    ep_rew_mean          | 0.33803985  |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5804032     |
| train/                  |             |
|    approx_kl            | 0.008835793 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.922       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.608      |
|    n_updates            | 3915        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 6.31        |
|    value_loss           | 0.00351     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.25e+03    |
|    ep_rew_mean          | 0.33803985  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5808128     |
| train/                  |             |
|    approx_kl            | 0.009872603 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.851       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.619      |
|    n_updates            | 3930        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 6.32        |
|    value_loss           | 0.00694     |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.35e+03    |
|    ep_rew_mean          | 0.33149967  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5812224     |
| train/                  |             |
|    approx_kl            | 0.007508604 |
|    clip_fraction        | 0.0787      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.961       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.605      |
|    n_updates            | 3945        |
|    policy_gradient_loss | -0.0139     |
|    std                  | 6.33        |
|    value_loss           | 0.00232     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.35e+03    |
|    ep_rew_mean          | 0.33149967  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 5816320     |
| train/                  |             |
|    approx_kl            | 0.009991421 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.737       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.569      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0181     |
|    std                  | 6.34        |
|    value_loss           | 0.0556      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.35e+03    |
|    ep_rew_mean          | 0.33149967  |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 7           |
|    time_elapsed         | 67          |
|    total_timesteps      | 5820416     |
| train/                  |             |
|    approx_kl            | 0.008917194 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.4       |
|    explained_variance   | 0.869       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.595      |
|    n_updates            | 3975        |
|    policy_gradient_loss | -0.0123     |
|    std                  | 6.35        |
|    value_loss           | 0.0165      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.34e+03    |
|    ep_rew_mean          | 0.32815936  |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 8           |
|    time_elapsed         | 76          |
|    total_timesteps      | 5824512     |
| train/                  |             |
|    approx_kl            | 0.008077315 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.4       |
|    explained_variance   | 0.966       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.617      |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0138     |
|    std                  | 6.37        |
|    value_loss           | 0.00177     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.34e+03     |
|    ep_rew_mean          | 0.32815936   |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 9            |
|    time_elapsed         | 86           |
|    total_timesteps      | 5828608      |
| train/                  |              |
|    approx_kl            | 0.0098291375 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.4        |
|    explained_variance   | 0.881        |
|    learning_rate        | 7.63e-05     |
|    loss                 | -0.614       |
|    n_updates            | 4005         |
|    policy_gradient_loss | -0.0189      |
|    std                  | 6.39         |
|    value_loss           | 0.0163       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.44e+03     |
|    ep_rew_mean          | 0.29228595   |
| time/                   |              |
|    fps                  | 422          |
|    iterations           | 10           |
|    time_elapsed         | 96           |
|    total_timesteps      | 5832704      |
| train/                  |              |
|    approx_kl            | 0.0068646427 |
|    clip_fraction        | 0.0838       |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.4        |
|    explained_variance   | 0.636        |
|    learning_rate        | 7.63e-05     |
|    loss                 | -0.598       |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00934     |
|    std                  | 6.41         |
|    value_loss           | 0.0123       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.29228595  |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 11          |
|    time_elapsed         | 106         |
|    total_timesteps      | 5836800     |
| train/                  |             |
|    approx_kl            | 0.011151842 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.93        |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.623      |
|    n_updates            | 4035        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 6.43        |
|    value_loss           | 0.016       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.29228595  |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 12          |
|    time_elapsed         | 115         |
|    total_timesteps      | 5840896     |
| train/                  |             |
|    approx_kl            | 0.010840818 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.784       |
|    learning_rate        | 7.63e-05    |
|    loss                 | -0.605      |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.0167     |
|    std                  | 6.44        |
|    value_loss           | 0.0103      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.44e+03   |
|    ep_rew_mean          | 0.34120345 |
| time/                   |            |
|    fps                  | 425        |
|    iterations           | 13         |
|    time_elapsed         | 125        |
|    total_timesteps      | 5844992    |
| train/                  |            |
|    approx_kl            | 0.00898548 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.5      |
|    explained_variance   | 0.941      |
|    learning_rate        | 7.63e-05   |
|    loss                 | -0.616     |
|    n_updates            | 4065       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 6.46       |
|    value_loss           | 0.00255    |
----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_14.zip

evaluating student performance (5 episodes)...
episode 1: length=34, reward=1.858, success=True
episode 2: length=34, reward=1.858, success=True
episode 3: length=34, reward=1.858, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 1.858
average episode length: 34.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 5.475
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 3.997
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 13.032
total data points collected: 40024
average episode length: 8004.8
average episode reward: 4.691
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40024, 56]), actions: torch.Size([40024, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2160', '7.0850', '7.8077', '7.8155', '8.8592']
Training reward models...
Reward model losses: ['0.2709', '0.0132', '0.2627', '0.0041', '0.0173']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3406', '1.3382', '1.3578', '1.3208', '1.1276']
Training action VAE models...
Action VAE losses: ['1.4288', '1.4854', '1.3300', '1.4735', '1.4693']
CM score components:
transition disagreement: 0.4511
reward disagreement: 0.1594
state disagreement: 0.4917
action disagreement: 0.5821
total CM score: 1.6842
goal is complete. CM score: 1.6842
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 26 steps, reward: 0.113
Reset #2: mass intervention applied (success: True)
episode 2: 28 steps, reward: 0.841
Reset #3: mass intervention applied (success: True)
episode 3: 27 steps, reward: 0.492
total data points collected: 133
average episode length: 26.6
average episode reward: 0.417
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([133, 56]), actions: torch.Size([133, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5347', '7.3838', '7.7265', '8.6720', '7.9726']
Training reward models...
Reward model losses: ['0.0139', '0.1296', '1.6589', '0.2388', '0.1579']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6322', '1.4915', '1.7961', '1.6893', '1.7405']
Training action VAE models...
Action VAE losses: ['1.3884', '1.4693', '1.3667', '1.3803', '1.3534']
CM score components:
transition disagreement: 0.4329
reward disagreement: 0.4394
state disagreement: 0.5638
action disagreement: 0.5602
total CM score: 1.9962
mass is complete. CM score: 1.9962
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 34 steps, reward: 1.858
Reset #2: friction intervention applied (success: True)
episode 2: 34 steps, reward: 1.858
Reset #3: friction intervention applied (success: True)
episode 3: 34 steps, reward: 1.858
total data points collected: 170
average episode length: 34.0
average episode reward: 1.858
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([170, 56]), actions: torch.Size([170, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2803', '7.7156', '7.0382', '7.8055', '6.5316']
Training reward models...
Reward model losses: ['0.0371', '0.0605', '0.1339', '0.0344', '0.2216']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7479', '1.6998', '1.8451', '1.6388', '2.1113']
Training action VAE models...
Action VAE losses: ['1.4569', '1.4456', '1.3310', '1.3061', '1.5267']
CM score components:
transition disagreement: 0.4490
reward disagreement: 0.1474
state disagreement: 0.5681
action disagreement: 0.5524
total CM score: 1.7168
friction is complete. CM score: 1.7168
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 34 steps, reward: 1.858
Reset #2: visual intervention applied (success: True)
episode 2: 34 steps, reward: 1.858
Reset #3: visual intervention applied (success: True)
episode 3: 34 steps, reward: 1.858
total data points collected: 170
average episode length: 34.0
average episode reward: 1.858
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([170, 56]), actions: torch.Size([170, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0069', '7.6227', '7.3337', '6.8888', '7.6926']
Training reward models...
Reward model losses: ['0.0113', '0.0910', '0.2975', '0.2017', '0.1420']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7917', '1.7629', '1.6194', '2.1721', '1.7719']
Training action VAE models...
Action VAE losses: ['1.5331', '1.4544', '1.4551', '1.5046', '1.4644']
CM score components:
transition disagreement: 0.4318
reward disagreement: 0.1426
state disagreement: 0.5630
action disagreement: 0.5915
total CM score: 1.7289
visual is complete. CM score: 1.7289
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 20 steps, reward: 1.374
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 8.606
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 9.542
total data points collected: 30042
average episode length: 6008.4
average episode reward: 3.916
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30042, 56]), actions: torch.Size([30042, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4606', '7.4239', '7.5864', '7.3620', '7.2733']
Training reward models...
Reward model losses: ['0.2165', '0.0467', '0.2503', '0.0128', '0.0082']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1403', '1.2880', '1.1859', '1.2512', '1.1501']
Training action VAE models...
Action VAE losses: ['1.2208', '1.3273', '1.2828', '1.4307', '1.3290']
CM score components:
transition disagreement: 0.4295
reward disagreement: 0.1346
state disagreement: 0.4745
action disagreement: 0.5191
total CM score: 1.5578
pose is complete. CM score: 1.5578
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 155 steps, reward: 2.500
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -3.440
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.625
total data points collected: 40159
average episode length: 8031.8
average episode reward: 1.271
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40159, 56]), actions: torch.Size([40159, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7463', '7.6340', '8.3712', '7.7978', '7.4655']
Training reward models...
Reward model losses: ['0.0276', '0.0216', '0.3602', '0.0212', '0.1721']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1730', '1.3161', '1.3265', '1.2490', '1.1771']
Training action VAE models...
Action VAE losses: ['1.4588', '1.4635', '1.4084', '1.2935', '1.4859']
CM score components:
transition disagreement: 0.4224
reward disagreement: 0.1742
state disagreement: 0.4852
action disagreement: 0.5621
total CM score: 1.6439
random is complete. CM score: 1.6439
2025-07-16 20:32:10,160 3266759 INFO Meta-Episode 15/30: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_14.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 5.475
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 3.997
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 13.032
total data points collected: 40024
average episode length: 8004.8
average episode reward: 4.691
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40024, 56]), actions: torch.Size([40024, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2617', '7.1302', '6.7950', '7.1060', '7.6652']
Training reward models...
Reward model losses: ['0.2034', '0.2170', '0.2568', '0.0130', '0.0185']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2322', '1.3028', '1.3044', '1.2315', '1.2575']
Training action VAE models...
Action VAE losses: ['1.5842', '1.4198', '1.5186', '1.2785', '1.3050']
CM score components:
transition disagreement: 0.4083
reward disagreement: 0.0835
state disagreement: 0.4636
action disagreement: 0.5642
total CM score: 1.5196
goal is complete. CM score: 1.5196
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 26 steps, reward: 0.113
Reset #2: mass intervention applied (success: True)
episode 2: 28 steps, reward: 0.841
Reset #3: mass intervention applied (success: True)
episode 3: 27 steps, reward: 0.492
total data points collected: 133
average episode length: 26.6
average episode reward: 0.417
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([133, 56]), actions: torch.Size([133, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1709', '7.8193', '7.1099', '7.0056', '8.3807']
Training reward models...
Reward model losses: ['0.0183', '0.2657', '0.0125', '0.3186', '0.5365']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7297', '1.5473', '1.5123', '1.6310', '1.9778']
Training action VAE models...
Action VAE losses: ['1.4903', '1.3932', '1.3506', '1.4301', '1.3838']
CM score components:
transition disagreement: 0.4262
reward disagreement: 0.1751
state disagreement: 0.5680
action disagreement: 0.5446
total CM score: 1.7138
mass is complete. CM score: 1.7138
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 34 steps, reward: 1.858
Reset #2: friction intervention applied (success: True)
episode 2: 34 steps, reward: 1.858
Reset #3: friction intervention applied (success: True)
episode 3: 34 steps, reward: 1.858
total data points collected: 170
average episode length: 34.0
average episode reward: 1.858
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([170, 56]), actions: torch.Size([170, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0249', '7.3828', '6.9599', '7.7708', '7.1569']
Training reward models...
Reward model losses: ['0.0518', '0.9836', '0.5167', '0.0277', '0.3056']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7462', '1.7001', '1.7112', '1.9110', '1.8686']
Training action VAE models...
Action VAE losses: ['1.2770', '1.3447', '1.2817', '1.2542', '1.4106']
CM score components:
transition disagreement: 0.4224
reward disagreement: 0.4034
state disagreement: 0.5437
action disagreement: 0.5254
total CM score: 1.8949
friction is complete. CM score: 1.8949
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 34 steps, reward: 1.858
Reset #2: visual intervention applied (success: True)
episode 2: 34 steps, reward: 1.858
Reset #3: visual intervention applied (success: True)
episode 3: 34 steps, reward: 1.858
total data points collected: 170
average episode length: 34.0
average episode reward: 1.858
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([170, 56]), actions: torch.Size([170, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1391', '7.8129', '7.1622', '7.2520', '7.5856']
Training reward models...
Reward model losses: ['0.0719', '0.2902', '0.1408', '0.0256', '0.1290']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6932', '1.6921', '1.8173', '1.6254', '1.6825']
Training action VAE models...
Action VAE losses: ['1.3964', '1.2722', '1.3745', '1.4659', '1.4610']
CM score components:
transition disagreement: 0.4466
reward disagreement: 0.1699
state disagreement: 0.5192
action disagreement: 0.5398
total CM score: 1.6755
visual is complete. CM score: 1.6755
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 20 steps, reward: 1.374
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 8.606
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 9.542
total data points collected: 30042
average episode length: 6008.4
average episode reward: 3.916
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30042, 56]), actions: torch.Size([30042, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7820', '7.7214', '7.6895', '6.6203', '7.4010']
Training reward models...
Reward model losses: ['2.8735', '0.9368', '0.1443', '0.0059', '0.0330']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1951', '1.2530', '1.2241', '1.1942', '1.1718']
Training action VAE models...
Action VAE losses: ['1.2745', '1.4252', '1.3232', '1.3944', '1.2577']
CM score components:
transition disagreement: 0.4441
reward disagreement: 0.7005
state disagreement: 0.4942
action disagreement: 0.5611
total CM score: 2.1999
pose is complete. CM score: 2.1999
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 155 steps, reward: 2.500
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -3.440
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.625
total data points collected: 40159
average episode length: 8031.8
average episode reward: 1.271
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40159, 56]), actions: torch.Size([40159, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4302', '7.3171', '8.3151', '7.4241', '7.3364']
Training reward models...
Reward model losses: ['0.0628', '0.0850', '0.0264', '0.2898', '0.1516']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2633', '1.2235', '1.2904', '1.2386', '1.2502']
Training action VAE models...
Action VAE losses: ['1.4639', '1.5116', '1.4162', '1.4298', '1.4705']
CM score components:
transition disagreement: 0.3777
reward disagreement: 0.1375
state disagreement: 0.4934
action disagreement: 0.5972
total CM score: 1.6058
random is complete. CM score: 1.6058
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 6.34e+03   |
|    ep_rew_mean     | 0.39335206 |
| time/              |            |
|    fps             | 443        |
|    iterations      | 1          |
|    time_elapsed    | 9          |
|    total_timesteps | 5849088    |
-----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.34e+03    |
|    ep_rew_mean          | 0.39335206  |
| time/                   |             |
|    fps                  | 412         |
|    iterations           | 2           |
|    time_elapsed         | 19          |
|    total_timesteps      | 5853184     |
| train/                  |             |
|    approx_kl            | 0.009329989 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.641       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.613      |
|    n_updates            | 4095        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 6.48        |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.34e+03    |
|    ep_rew_mean          | 0.24102737  |
| time/                   |             |
|    fps                  | 407         |
|    iterations           | 3           |
|    time_elapsed         | 30          |
|    total_timesteps      | 5857280     |
| train/                  |             |
|    approx_kl            | 0.008201345 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.96        |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.605      |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.0144     |
|    std                  | 6.49        |
|    value_loss           | 0.00218     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.34e+03    |
|    ep_rew_mean          | 0.24102737  |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 4           |
|    time_elapsed         | 40          |
|    total_timesteps      | 5861376     |
| train/                  |             |
|    approx_kl            | 0.008313769 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.781       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.618      |
|    n_updates            | 4125        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 6.5         |
|    value_loss           | 0.0129      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.17821793  |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 5           |
|    time_elapsed         | 49          |
|    total_timesteps      | 5865472     |
| train/                  |             |
|    approx_kl            | 0.009905144 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.89        |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.591      |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.0137     |
|    std                  | 6.52        |
|    value_loss           | 0.0051      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.17821793  |
| time/                   |             |
|    fps                  | 411         |
|    iterations           | 6           |
|    time_elapsed         | 59          |
|    total_timesteps      | 5869568     |
| train/                  |             |
|    approx_kl            | 0.008965151 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.658       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.605      |
|    n_updates            | 4155        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 6.53        |
|    value_loss           | 0.0264      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.44e+03     |
|    ep_rew_mean          | 0.17821793   |
| time/                   |              |
|    fps                  | 412          |
|    iterations           | 7            |
|    time_elapsed         | 69           |
|    total_timesteps      | 5873664      |
| train/                  |              |
|    approx_kl            | 0.0077847773 |
|    clip_fraction        | 0.0953       |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.6        |
|    explained_variance   | 0.939        |
|    learning_rate        | 6.86e-05     |
|    loss                 | -0.617       |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.0172      |
|    std                  | 6.54         |
|    value_loss           | 0.0033       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.14216813  |
| time/                   |             |
|    fps                  | 412         |
|    iterations           | 8           |
|    time_elapsed         | 79          |
|    total_timesteps      | 5877760     |
| train/                  |             |
|    approx_kl            | 0.009497926 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.957       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.613      |
|    n_updates            | 4185        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 6.54        |
|    value_loss           | 0.00276     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.44e+03     |
|    ep_rew_mean          | 0.14216813   |
| time/                   |              |
|    fps                  | 414          |
|    iterations           | 9            |
|    time_elapsed         | 88           |
|    total_timesteps      | 5881856      |
| train/                  |              |
|    approx_kl            | 0.0071218917 |
|    clip_fraction        | 0.0819       |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.6        |
|    explained_variance   | 0.889        |
|    learning_rate        | 6.86e-05     |
|    loss                 | -0.619       |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.0179      |
|    std                  | 6.55         |
|    value_loss           | 0.0118       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.10008229  |
| time/                   |             |
|    fps                  | 414         |
|    iterations           | 10          |
|    time_elapsed         | 98          |
|    total_timesteps      | 5885952     |
| train/                  |             |
|    approx_kl            | 0.007743335 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.964       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.618      |
|    n_updates            | 4215        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 6.56        |
|    value_loss           | 0.00192     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.10008229  |
| time/                   |             |
|    fps                  | 415         |
|    iterations           | 11          |
|    time_elapsed         | 108         |
|    total_timesteps      | 5890048     |
| train/                  |             |
|    approx_kl            | 0.008544126 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.602       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.623      |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.02       |
|    std                  | 6.57        |
|    value_loss           | 0.0129      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.44e+03    |
|    ep_rew_mean          | 0.10008229  |
| time/                   |             |
|    fps                  | 415         |
|    iterations           | 12          |
|    time_elapsed         | 118         |
|    total_timesteps      | 5894144     |
| train/                  |             |
|    approx_kl            | 0.007844197 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.882       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.621      |
|    n_updates            | 4245        |
|    policy_gradient_loss | -0.016      |
|    std                  | 6.58        |
|    value_loss           | 0.00637     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.097352855 |
| time/                   |             |
|    fps                  | 417         |
|    iterations           | 13          |
|    time_elapsed         | 127         |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.009413487 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.956       |
|    learning_rate        | 6.86e-05    |
|    loss                 | -0.621      |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0167     |
|    std                  | 6.6         |
|    value_loss           | 0.00255     |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_15.zip

evaluating student performance (5 episodes)...
episode 1: length=26, reward=0.844, success=True
episode 2: length=26, reward=0.844, success=True
episode 3: length=26, reward=0.844, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 0.844
average episode length: 26.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 10.638
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.401
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 1.269
total data points collected: 50005
average episode length: 10001.0
average episode reward: 8.687
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2030', '7.0861', '7.8110', '7.7940', '8.8704']
Training reward models...
Reward model losses: ['0.2740', '0.0096', '0.2751', '0.0050', '0.0172']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2702', '1.2774', '1.3173', '1.2460', '1.1239']
Training action VAE models...
Action VAE losses: ['1.3897', '1.4782', '1.2976', '1.4581', '1.4437']
CM score components:
transition disagreement: 0.4521
reward disagreement: 0.1583
state disagreement: 0.4869
action disagreement: 0.5801
total CM score: 1.6775
goal is complete. CM score: 1.6775
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 141 steps, reward: 2.563
Reset #2: mass intervention applied (success: True)
episode 2: 666 steps, reward: 5.978
Reset #3: mass intervention applied (success: True)
episode 3: 113 steps, reward: 2.559
total data points collected: 11178
average episode length: 2235.6
average episode reward: 9.480
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([11178, 56]), actions: torch.Size([11178, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4406', '7.2554', '7.6955', '8.5122', '7.9666']
Training reward models...
Reward model losses: ['0.0129', '0.1237', '1.7518', '0.2550', '0.0938']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9161', '0.8787', '0.9214', '0.8378', '0.8498']
Training action VAE models...
Action VAE losses: ['1.2785', '1.3807', '1.2216', '1.2447', '1.2566']
CM score components:
transition disagreement: 0.4300
reward disagreement: 0.4673
state disagreement: 0.4740
action disagreement: 0.5296
total CM score: 1.9009
mass is complete. CM score: 1.9009
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 26 steps, reward: 0.844
Reset #2: friction intervention applied (success: True)
episode 2: 26 steps, reward: 0.844
Reset #3: friction intervention applied (success: True)
episode 3: 26 steps, reward: 0.844
total data points collected: 130
average episode length: 26.0
average episode reward: 0.844
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([130, 56]), actions: torch.Size([130, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2583', '7.6723', '7.0624', '7.7819', '6.4925']
Training reward models...
Reward model losses: ['0.0306', '0.0358', '0.1293', '0.0376', '0.2300']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6051', '1.5542', '1.7392', '1.5224', '1.8437']
Training action VAE models...
Action VAE losses: ['1.4551', '1.4635', '1.3152', '1.2884', '1.5249']
CM score components:
transition disagreement: 0.4510
reward disagreement: 0.1431
state disagreement: 0.5380
action disagreement: 0.5389
total CM score: 1.6709
friction is complete. CM score: 1.6709
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 26 steps, reward: 0.844
Reset #2: visual intervention applied (success: True)
episode 2: 26 steps, reward: 0.844
Reset #3: visual intervention applied (success: True)
episode 3: 26 steps, reward: 0.844
total data points collected: 130
average episode length: 26.0
average episode reward: 0.844
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([130, 56]), actions: torch.Size([130, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9933', '7.6338', '7.2981', '6.8720', '7.6681']
Training reward models...
Reward model losses: ['0.0101', '0.1011', '0.2893', '0.2187', '0.1457']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7633', '1.5837', '1.4885', '1.9150', '1.6469']
Training action VAE models...
Action VAE losses: ['1.5657', '1.4593', '1.4702', '1.5222', '1.4527']
CM score components:
transition disagreement: 0.4327
reward disagreement: 0.1484
state disagreement: 0.5463
action disagreement: 0.6060
total CM score: 1.7334
visual is complete. CM score: 1.7334
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 20 steps, reward: 0.943
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 20.079
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 13.783
total data points collected: 30047
average episode length: 6009.4
average episode reward: 10.636
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30047, 56]), actions: torch.Size([30047, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4233', '7.3721', '7.5466', '7.3641', '7.2201']
Training reward models...
Reward model losses: ['0.2133', '0.0562', '0.2828', '0.0160', '0.0030']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0246', '0.9913', '0.9983', '0.9450', '1.1449']
Training action VAE models...
Action VAE losses: ['1.0440', '1.1701', '1.0878', '1.2032', '1.1554']
CM score components:
transition disagreement: 0.4244
reward disagreement: 0.1355
state disagreement: 0.4868
action disagreement: 0.5095
total CM score: 1.5562
pose is complete. CM score: 1.5562
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 4.753
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 2.298
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.552
total data points collected: 40641
average episode length: 8128.2
average episode reward: 3.723
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40641, 56]), actions: torch.Size([40641, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7567', '7.6520', '8.3848', '7.8018', '7.4682']
Training reward models...
Reward model losses: ['0.0275', '0.0199', '0.3744', '0.0265', '0.1707']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1633', '1.3587', '1.3317', '1.2136', '1.2209']
Training action VAE models...
Action VAE losses: ['1.5242', '1.4967', '1.3770', '1.3534', '1.4960']
CM score components:
transition disagreement: 0.4222
reward disagreement: 0.1796
state disagreement: 0.4830
action disagreement: 0.5700
total CM score: 1.6548
random is complete. CM score: 1.6548
2025-07-16 20:42:03,196 3266759 INFO Meta-Episode 16/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_15.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 10.638
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.401
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 1.269
total data points collected: 50005
average episode length: 10001.0
average episode reward: 8.687
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2714', '7.1231', '6.8079', '7.1177', '7.6808']
Training reward models...
Reward model losses: ['0.2177', '0.2264', '0.2641', '0.0118', '0.0205']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1793', '1.2438', '1.2729', '1.1890', '1.2442']
Training action VAE models...
Action VAE losses: ['1.5326', '1.4646', '1.6083', '1.2908', '1.3226']
CM score components:
transition disagreement: 0.4094
reward disagreement: 0.0816
state disagreement: 0.4675
action disagreement: 0.5689
total CM score: 1.5274
goal is complete. CM score: 1.5274
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 141 steps, reward: 2.563
Reset #2: mass intervention applied (success: True)
episode 2: 666 steps, reward: 5.978
Reset #3: mass intervention applied (success: True)
episode 3: 113 steps, reward: 2.559
total data points collected: 11178
average episode length: 2235.6
average episode reward: 9.480
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([11178, 56]), actions: torch.Size([11178, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0828', '7.7110', '7.0880', '6.8279', '8.2340']
Training reward models...
Reward model losses: ['0.0049', '0.2779', '0.0077', '0.3524', '0.6579']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9190', '0.7972', '0.8451', '0.9686', '0.9854']
Training action VAE models...
Action VAE losses: ['1.3552', '1.2687', '1.2789', '1.2976', '1.1927']
CM score components:
transition disagreement: 0.4224
reward disagreement: 0.2094
state disagreement: 0.4880
action disagreement: 0.5439
total CM score: 1.6638
mass is complete. CM score: 1.6638
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 26 steps, reward: 0.844
Reset #2: friction intervention applied (success: True)
episode 2: 26 steps, reward: 0.844
Reset #3: friction intervention applied (success: True)
episode 3: 26 steps, reward: 0.844
total data points collected: 130
average episode length: 26.0
average episode reward: 0.844
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([130, 56]), actions: torch.Size([130, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9952', '7.3442', '6.9339', '7.7457', '7.1529']
Training reward models...
Reward model losses: ['0.0638', '1.0408', '0.4580', '0.0199', '0.2622']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5669', '1.5209', '1.5029', '1.7477', '1.7256']
Training action VAE models...
Action VAE losses: ['1.2994', '1.3577', '1.2722', '1.2450', '1.4316']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.4074
state disagreement: 0.5311
action disagreement: 0.5334
total CM score: 1.8958
friction is complete. CM score: 1.8958
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 26 steps, reward: 0.844
Reset #2: visual intervention applied (success: True)
episode 2: 26 steps, reward: 0.844
Reset #3: visual intervention applied (success: True)
episode 3: 26 steps, reward: 0.844
total data points collected: 130
average episode length: 26.0
average episode reward: 0.844
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([130, 56]), actions: torch.Size([130, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1712', '7.8172', '7.1109', '7.2511', '7.5566']
Training reward models...
Reward model losses: ['0.0766', '0.3083', '0.1064', '0.0266', '0.1092']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5397', '1.5403', '1.6444', '1.4814', '1.4625']
Training action VAE models...
Action VAE losses: ['1.3993', '1.2526', '1.3863', '1.4035', '1.4366']
CM score components:
transition disagreement: 0.4478
reward disagreement: 0.1617
state disagreement: 0.5012
action disagreement: 0.5378
total CM score: 1.6485
visual is complete. CM score: 1.6485
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 20 steps, reward: 0.943
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 20.079
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 13.783
total data points collected: 30047
average episode length: 6009.4
average episode reward: 10.636
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30047, 56]), actions: torch.Size([30047, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7682', '7.7431', '7.6676', '6.6157', '7.4178']
Training reward models...
Reward model losses: ['2.7042', '0.9621', '0.1671', '0.0129', '0.0312']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0597', '0.9713', '1.0232', '0.9100', '1.0701']
Training action VAE models...
Action VAE losses: ['1.0322', '1.2281', '1.2338', '1.2130', '1.1167']
CM score components:
transition disagreement: 0.4384
reward disagreement: 0.6703
state disagreement: 0.5061
action disagreement: 0.5523
total CM score: 2.1670
pose is complete. CM score: 2.1670
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 4.753
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 2.298
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.552
total data points collected: 40641
average episode length: 8128.2
average episode reward: 3.723
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'success']
success rate: 1/5
tensor shapes - states: torch.Size([40641, 56]), actions: torch.Size([40641, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4515', '7.3239', '8.3375', '7.4152', '7.3369']
Training reward models...
Reward model losses: ['0.0748', '0.0578', '0.0260', '0.2783', '0.1562']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2983', '1.2532', '1.2422', '1.2515', '1.2612']
Training action VAE models...
Action VAE losses: ['1.5025', '1.4351', '1.4492', '1.4784', '1.4535']
CM score components:
transition disagreement: 0.3749
reward disagreement: 0.1278
state disagreement: 0.4933
action disagreement: 0.5853
total CM score: 1.5813
random is complete. CM score: 1.5813
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
------------------------------------
| rollout/           |             |
|    ep_len_mean     | 6.54e+03    |
|    ep_rew_mean     | 0.097352855 |
| time/              |             |
|    fps             | 475         |
|    iterations      | 1           |
|    time_elapsed    | 8           |
|    total_timesteps | 5902336     |
------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.097352855 |
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 2           |
|    time_elapsed         | 17          |
|    total_timesteps      | 5906432     |
| train/                  |             |
|    approx_kl            | 0.010555582 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.837       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.621      |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 6.62        |
|    value_loss           | 0.00867     |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.12456631  |
| time/                   |             |
|    fps                  | 449         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5910528     |
| train/                  |             |
|    approx_kl            | 0.008628244 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.908       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.612      |
|    n_updates            | 4305        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 6.63        |
|    value_loss           | 0.00572     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.12456631  |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 4           |
|    time_elapsed         | 37          |
|    total_timesteps      | 5914624     |
| train/                  |             |
|    approx_kl            | 0.008476488 |
|    clip_fraction        | 0.0935      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.897       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.63       |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.021      |
|    std                  | 6.65        |
|    value_loss           | 0.00814     |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.13619323  |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 5918720     |
| train/                  |             |
|    approx_kl            | 0.008257734 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.95        |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.612      |
|    n_updates            | 4335        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 6.66        |
|    value_loss           | 0.00303     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.54e+03   |
|    ep_rew_mean          | 0.13619323 |
| time/                   |            |
|    fps                  | 436        |
|    iterations           | 6          |
|    time_elapsed         | 56         |
|    total_timesteps      | 5922816    |
| train/                  |            |
|    approx_kl            | 0.00763873 |
|    clip_fraction        | 0.084      |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.8      |
|    explained_variance   | 0.914      |
|    learning_rate        | 6.18e-05   |
|    loss                 | -0.625     |
|    n_updates            | 4350       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 6.67       |
|    value_loss           | 0.00511    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.54e+03    |
|    ep_rew_mean          | 0.13619323  |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 5926912     |
| train/                  |             |
|    approx_kl            | 0.008051134 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.919       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.616      |
|    n_updates            | 4365        |
|    policy_gradient_loss | -0.0147     |
|    std                  | 6.69        |
|    value_loss           | 0.00491     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.64e+03    |
|    ep_rew_mean          | 0.13554063  |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 8           |
|    time_elapsed         | 74          |
|    total_timesteps      | 5931008     |
| train/                  |             |
|    approx_kl            | 0.009508481 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.97        |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.611      |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.0181     |
|    std                  | 6.69        |
|    value_loss           | 0.00199     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.64e+03    |
|    ep_rew_mean          | 0.13554063  |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 9           |
|    time_elapsed         | 84          |
|    total_timesteps      | 5935104     |
| train/                  |             |
|    approx_kl            | 0.008505663 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.706       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.618      |
|    n_updates            | 4395        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 6.72        |
|    value_loss           | 0.0213      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.64e+03    |
|    ep_rew_mean          | 0.11757155  |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 5939200     |
| train/                  |             |
|    approx_kl            | 0.008285115 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.93        |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.619      |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0159     |
|    std                  | 6.72        |
|    value_loss           | 0.00393     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.64e+03    |
|    ep_rew_mean          | 0.11757155  |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 11          |
|    time_elapsed         | 103         |
|    total_timesteps      | 5943296     |
| train/                  |             |
|    approx_kl            | 0.009690739 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.87        |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.629      |
|    n_updates            | 4425        |
|    policy_gradient_loss | -0.0238     |
|    std                  | 6.73        |
|    value_loss           | 0.00929     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.64e+03    |
|    ep_rew_mean          | 0.11757155  |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 12          |
|    time_elapsed         | 113         |
|    total_timesteps      | 5947392     |
| train/                  |             |
|    approx_kl            | 0.008038869 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.927       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.621      |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0147     |
|    std                  | 6.73        |
|    value_loss           | 0.00326     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.74e+03    |
|    ep_rew_mean          | 0.11770788  |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 13          |
|    time_elapsed         | 123         |
|    total_timesteps      | 5951488     |
| train/                  |             |
|    approx_kl            | 0.007095304 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.941       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.619      |
|    n_updates            | 4455        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 6.75        |
|    value_loss           | 0.003       |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_16.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=-0.833, success=False
episode 2: length=10001, reward=-0.833, success=False
episode 3: length=10001, reward=-0.833, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.833
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: -1.188
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.517
Reset #3: goal intervention applied (success: True)
episode 3: 30 steps, reward: 1.533
total data points collected: 40034
average episode length: 8006.8
average episode reward: 0.376
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40034, 56]), actions: torch.Size([40034, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2209', '7.1219', '7.8261', '7.8308', '8.8529']
Training reward models...
Reward model losses: ['0.2914', '0.0188', '0.2470', '0.0046', '0.0185']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2816', '1.2947', '1.3150', '1.2382', '1.1126']
Training action VAE models...
Action VAE losses: ['1.4565', '1.4644', '1.3407', '1.4960', '1.4361']
CM score components:
transition disagreement: 0.4507
reward disagreement: 0.1639
state disagreement: 0.4883
action disagreement: 0.5855
total CM score: 1.6884
goal is complete. CM score: 1.6884
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -0.471
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.909
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -0.369
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.241
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3787', '7.2613', '7.6350', '8.5949', '7.9497']
Training reward models...
Reward model losses: ['0.0142', '0.1827', '1.9037', '0.2768', '0.1211']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2550', '1.4480', '1.2690', '1.1688', '1.2746']
Training action VAE models...
Action VAE losses: ['1.3337', '1.3149', '1.3869', '1.3214', '1.3725']
CM score components:
transition disagreement: 0.4374
reward disagreement: 0.4901
state disagreement: 0.5250
action disagreement: 0.5439
total CM score: 1.9963
mass is complete. CM score: 1.9963
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -0.489
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.280
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.454
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.068
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2371', '7.6612', '6.9209', '7.6246', '6.4263']
Training reward models...
Reward model losses: ['0.0201', '0.0164', '0.1225', '0.0547', '0.2150']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0738', '1.0596', '1.1429', '1.1257', '1.0824']
Training action VAE models...
Action VAE losses: ['1.4051', '1.4275', '1.3641', '1.2447', '1.5335']
CM score components:
transition disagreement: 0.4491
reward disagreement: 0.1150
state disagreement: 0.4728
action disagreement: 0.5370
total CM score: 1.5740
friction is complete. CM score: 1.5740
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.833
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.833
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.833
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.833
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8811', '7.4612', '7.2480', '6.8019', '7.5704']
Training reward models...
Reward model losses: ['0.0069', '0.0399', '0.2441', '0.2330', '0.1300']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9356', '0.9450', '1.0157', '1.0627', '0.9226']
Training action VAE models...
Action VAE losses: ['1.5294', '1.4899', '1.4758', '1.4101', '1.5361']
CM score components:
transition disagreement: 0.4269
reward disagreement: 0.1267
state disagreement: 0.4862
action disagreement: 0.5897
total CM score: 1.6296
visual is complete. CM score: 1.6296
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 16 steps, reward: 0.677
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: -3.327
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 0.599
total data points collected: 20486
average episode length: 4097.2
average episode reward: 0.363
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([20486, 56]), actions: torch.Size([20486, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4829', '7.4523', '7.5921', '7.2882', '7.2608']
Training reward models...
Reward model losses: ['0.2273', '0.0364', '0.2898', '0.0142', '0.0090']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8341', '0.8278', '0.8194', '0.8277', '0.8145']
Training action VAE models...
Action VAE losses: ['1.3160', '1.3643', '1.3462', '1.4818', '1.4523']
CM score components:
transition disagreement: 0.4276
reward disagreement: 0.1484
state disagreement: 0.4545
action disagreement: 0.5162
total CM score: 1.5467
pose is complete. CM score: 1.5467
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 39.903
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.770
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.236
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.655
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7506', '7.6478', '8.3848', '7.8011', '7.4544']
Training reward models...
Reward model losses: ['0.0228', '0.0147', '0.3444', '0.0225', '0.1623']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2275', '1.3237', '1.3666', '1.2672', '1.2434']
Training action VAE models...
Action VAE losses: ['1.5309', '1.4894', '1.3647', '1.3330', '1.5208']
CM score components:
transition disagreement: 0.4235
reward disagreement: 0.1708
state disagreement: 0.4883
action disagreement: 0.5694
total CM score: 1.6519
random is complete. CM score: 1.6519
2025-07-16 20:58:23,070 3266759 INFO Meta-Episode 17/30: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_16.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: -1.188
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.517
Reset #3: goal intervention applied (success: True)
episode 3: 30 steps, reward: 1.533
total data points collected: 40034
average episode length: 8006.8
average episode reward: 0.376
termination reasons: ['max_length', 'max_length', 'success', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40034, 56]), actions: torch.Size([40034, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2736', '7.1252', '6.8397', '7.1008', '7.7018']
Training reward models...
Reward model losses: ['0.1921', '0.1996', '0.2316', '0.0055', '0.0101']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1435', '1.2987', '1.2002', '1.1700', '1.1944']
Training action VAE models...
Action VAE losses: ['1.6050', '1.4214', '1.4318', '1.2712', '1.3263']
CM score components:
transition disagreement: 0.4122
reward disagreement: 0.0912
state disagreement: 0.4561
action disagreement: 0.5655
total CM score: 1.5250
goal is complete. CM score: 1.5250
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -0.471
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.909
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: -0.369
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.241
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0696', '7.7429', '6.9852', '6.8520', '8.2723']
Training reward models...
Reward model losses: ['0.0038', '0.1821', '0.0067', '0.3967', '0.5730']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2857', '1.1381', '1.1154', '1.2953', '1.3768']
Training action VAE models...
Action VAE losses: ['1.4414', '1.3881', '1.2751', '1.3965', '1.2998']
CM score components:
transition disagreement: 0.4217
reward disagreement: 0.1658
state disagreement: 0.5213
action disagreement: 0.5454
total CM score: 1.6542
mass is complete. CM score: 1.6542
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -0.489
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -3.280
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.454
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.068
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9785', '7.3473', '6.7617', '7.6051', '7.1106']
Training reward models...
Reward model losses: ['0.0669', '1.0999', '0.4496', '0.0186', '0.3044']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1136', '1.1273', '1.1359', '1.1847', '1.0521']
Training action VAE models...
Action VAE losses: ['1.3077', '1.3110', '1.2548', '1.2867', '1.3924']
CM score components:
transition disagreement: 0.4199
reward disagreement: 0.4195
state disagreement: 0.4923
action disagreement: 0.5109
total CM score: 1.8427
friction is complete. CM score: 1.8427
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.833
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.833
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.833
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.833
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0866', '7.7598', '7.0829', '7.1505', '7.4760']
Training reward models...
Reward model losses: ['0.0599', '0.4203', '0.0815', '0.0167', '0.0507']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0320', '0.9291', '1.0268', '1.1233', '0.9911']
Training action VAE models...
Action VAE losses: ['1.3104', '1.3153', '1.3451', '1.4894', '1.6300']
CM score components:
transition disagreement: 0.4363
reward disagreement: 0.1787
state disagreement: 0.4704
action disagreement: 0.5378
total CM score: 1.6232
visual is complete. CM score: 1.6232
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 16 steps, reward: 0.677
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: -3.327
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 0.599
total data points collected: 20486
average episode length: 4097.2
average episode reward: 0.363
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([20486, 56]), actions: torch.Size([20486, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7904', '7.7031', '7.6794', '6.6169', '7.4640']
Training reward models...
Reward model losses: ['2.7839', '0.8832', '0.1503', '0.0091', '0.0438']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8569', '0.7988', '0.8240', '0.8032', '0.8208']
Training action VAE models...
Action VAE losses: ['1.3499', '1.4506', '1.3824', '1.4525', '1.3073']
CM score components:
transition disagreement: 0.4489
reward disagreement: 0.6753
state disagreement: 0.4702
action disagreement: 0.5602
total CM score: 2.1547
pose is complete. CM score: 2.1547
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 39.903
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.770
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.236
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.655
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4528', '7.3258', '8.3228', '7.4140', '7.3290']
Training reward models...
Reward model losses: ['0.0699', '0.0485', '0.0256', '0.2864', '0.1595']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3596', '1.2004', '1.2996', '1.2745', '1.2992']
Training action VAE models...
Action VAE losses: ['1.4947', '1.4050', '1.4704', '1.5052', '1.4412']
CM score components:
transition disagreement: 0.3769
reward disagreement: 0.1321
state disagreement: 0.4966
action disagreement: 0.5835
total CM score: 1.5891
random is complete. CM score: 1.5891
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 6.74e+03   |
|    ep_rew_mean     | 0.11770788 |
| time/              |            |
|    fps             | 448        |
|    iterations      | 1          |
|    time_elapsed    | 9          |
|    total_timesteps | 5955584    |
-----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.74e+03     |
|    ep_rew_mean          | 0.11770788   |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 2            |
|    time_elapsed         | 19           |
|    total_timesteps      | 5959680      |
| train/                  |              |
|    approx_kl            | 0.0079217125 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.2          |
|    entropy_loss         | -29.9        |
|    explained_variance   | 0.67         |
|    learning_rate        | 5.56e-05     |
|    loss                 | -0.609       |
|    n_updates            | 4485         |
|    policy_gradient_loss | -0.016       |
|    std                  | 6.78         |
|    value_loss           | 0.0304       |
------------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.84e+03    |
|    ep_rew_mean          | 0.11316063  |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5963776     |
| train/                  |             |
|    approx_kl            | 0.007502678 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.902       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.612      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0129     |
|    std                  | 6.78        |
|    value_loss           | 0.00426     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.84e+03    |
|    ep_rew_mean          | 0.11316063  |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 5967872     |
| train/                  |             |
|    approx_kl            | 0.009811047 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.542       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.596      |
|    n_updates            | 4515        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 6.79        |
|    value_loss           | 0.0544      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.026950885 |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 5           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5971968     |
| train/                  |             |
|    approx_kl            | 0.010628954 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.884       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.628      |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0161     |
|    std                  | 6.79        |
|    value_loss           | 0.00527     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.026950885 |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 5976064     |
| train/                  |             |
|    approx_kl            | 0.011466208 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.812       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.615      |
|    n_updates            | 4545        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 6.81        |
|    value_loss           | 0.0281      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.026950885 |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 7           |
|    time_elapsed         | 66          |
|    total_timesteps      | 5980160     |
| train/                  |             |
|    approx_kl            | 0.009238636 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.809       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.623      |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.0178     |
|    std                  | 6.82        |
|    value_loss           | 0.00638     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 6.83e+03     |
|    ep_rew_mean          | 0.07041721   |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 8            |
|    time_elapsed         | 76           |
|    total_timesteps      | 5984256      |
| train/                  |              |
|    approx_kl            | 0.0065281335 |
|    clip_fraction        | 0.0864       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30          |
|    explained_variance   | 0.925        |
|    learning_rate        | 5.56e-05     |
|    loss                 | -0.616       |
|    n_updates            | 4575         |
|    policy_gradient_loss | -0.0146      |
|    std                  | 6.83         |
|    value_loss           | 0.004        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.83e+03    |
|    ep_rew_mean          | 0.07041721  |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 9           |
|    time_elapsed         | 86          |
|    total_timesteps      | 5988352     |
| train/                  |             |
|    approx_kl            | 0.008416297 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.433       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.617      |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0214     |
|    std                  | 6.83        |
|    value_loss           | 0.0421      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.04917908  |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 10          |
|    time_elapsed         | 95          |
|    total_timesteps      | 5992448     |
| train/                  |             |
|    approx_kl            | 0.006777584 |
|    clip_fraction        | 0.0566      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.689       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.613      |
|    n_updates            | 4605        |
|    policy_gradient_loss | -0.012      |
|    std                  | 6.84        |
|    value_loss           | 0.022       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.04917908  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 11          |
|    time_elapsed         | 105         |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.008949853 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.84        |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.624      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 6.85        |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.93e+03    |
|    ep_rew_mean          | 0.04917908  |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 12          |
|    time_elapsed         | 114         |
|    total_timesteps      | 6000640     |
| train/                  |             |
|    approx_kl            | 0.009088417 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.895       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.62       |
|    n_updates            | 4635        |
|    policy_gradient_loss | -0.016      |
|    std                  | 6.86        |
|    value_loss           | 0.00441     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.03e+03    |
|    ep_rew_mean          | 0.010890591 |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 13          |
|    time_elapsed         | 124         |
|    total_timesteps      | 6004736     |
| train/                  |             |
|    approx_kl            | 0.008152725 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.1       |
|    explained_variance   | 0.925       |
|    learning_rate        | 5.56e-05    |
|    loss                 | -0.622      |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0166     |
|    std                  | 6.87        |
|    value_loss           | 0.0027      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_17.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=47.079, success=False
episode 2: length=10001, reward=47.079, success=False
episode 3: length=10001, reward=47.079, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 47.079
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 74.280
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.217
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 18.664
total data points collected: 50005
average episode length: 10001.0
average episode reward: 18.203
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2004', '7.1126', '7.8146', '7.7974', '8.8711']
Training reward models...
Reward model losses: ['0.2533', '0.0126', '0.2499', '0.0049', '0.0182']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2424', '1.3458', '1.3046', '1.3388', '1.1008']
Training action VAE models...
Action VAE losses: ['1.4261', '1.4806', '1.2791', '1.4724', '1.4796']
CM score components:
transition disagreement: 0.4491
reward disagreement: 0.1525
state disagreement: 0.4893
action disagreement: 0.5863
total CM score: 1.6772
goal is complete. CM score: 1.6772
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 2459 steps, reward: 18.957
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 30.678
Reset #3: mass intervention applied (success: True)
episode 3: 99 steps, reward: 2.595
total data points collected: 24225
average episode length: 4845.0
average episode reward: 22.953
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length']
success rate: 3/5
tensor shapes - states: torch.Size([24225, 56]), actions: torch.Size([24225, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3952', '7.2728', '7.6955', '8.4962', '8.0118']
Training reward models...
Reward model losses: ['0.0170', '0.1622', '1.6745', '0.2537', '0.1178']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9541', '1.0276', '1.0139', '0.9406', '0.9526']
Training action VAE models...
Action VAE losses: ['1.2907', '1.3192', '1.2268', '1.2251', '1.2722']
CM score components:
transition disagreement: 0.4304
reward disagreement: 0.4565
state disagreement: 0.4778
action disagreement: 0.5378
total CM score: 1.9026
mass is complete. CM score: 1.9026
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 47.079
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 47.079
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 47.079
total data points collected: 50005
average episode length: 10001.0
average episode reward: 47.079
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2336', '7.5709', '6.8975', '7.6160', '6.4852']
Training reward models...
Reward model losses: ['0.0044', '0.0204', '0.2219', '0.0263', '0.1653']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8249', '0.7524', '0.8626', '0.8433', '0.7842']
Training action VAE models...
Action VAE losses: ['1.0141', '1.0761', '1.0373', '0.9289', '1.2461']
CM score components:
transition disagreement: 0.4346
reward disagreement: 0.1446
state disagreement: 0.4460
action disagreement: 0.5315
total CM score: 1.5567
friction is complete. CM score: 1.5567
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 47.079
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 47.079
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 47.079
total data points collected: 50005
average episode length: 10001.0
average episode reward: 47.079
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9108', '7.4457', '7.2797', '6.7905', '7.6431']
Training reward models...
Reward model losses: ['0.0018', '0.0266', '0.2175', '0.2181', '0.2138']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7727', '0.7731', '0.8482', '0.8736', '0.7416']
Training action VAE models...
Action VAE losses: ['1.1787', '1.0866', '1.1175', '1.1595', '1.0874']
CM score components:
transition disagreement: 0.4233
reward disagreement: 0.1376
state disagreement: 0.4677
action disagreement: 0.5643
total CM score: 1.5929
visual is complete. CM score: 1.5929
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.964
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 39.072
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 14.367
total data points collected: 40018
average episode length: 8003.6
average episode reward: 18.039
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4725', '7.3965', '7.5800', '7.3546', '7.2591']
Training reward models...
Reward model losses: ['0.2274', '0.0418', '0.2566', '0.0137', '0.0052']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2630', '1.3766', '1.2914', '1.3878', '1.2713']
Training action VAE models...
Action VAE losses: ['1.2853', '1.3532', '1.3331', '1.4498', '1.4008']
CM score components:
transition disagreement: 0.4287
reward disagreement: 0.1424
state disagreement: 0.4824
action disagreement: 0.5192
total CM score: 1.5728
pose is complete. CM score: 1.5728
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 15.039
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.868
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.029
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.831
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7490', '7.6430', '8.3774', '7.8038', '7.4716']
Training reward models...
Reward model losses: ['0.0576', '0.0517', '0.4038', '0.0559', '0.2086']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2201', '1.3787', '1.3409', '1.2692', '1.2064']
Training action VAE models...
Action VAE losses: ['1.4924', '1.5209', '1.3751', '1.3786', '1.5104']
CM score components:
transition disagreement: 0.4239
reward disagreement: 0.1800
state disagreement: 0.4920
action disagreement: 0.5680
total CM score: 1.6640
random is complete. CM score: 1.6640
2025-07-16 21:18:30,676 3266759 INFO Meta-Episode 18/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_17.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 74.280
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.217
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 18.664
total data points collected: 50005
average episode length: 10001.0
average episode reward: 18.203
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2732', '7.0991', '6.8047', '7.1039', '7.6750']
Training reward models...
Reward model losses: ['0.2073', '0.2246', '0.2784', '0.0107', '0.0180']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1573', '1.2899', '1.2446', '1.2129', '1.2515']
Training action VAE models...
Action VAE losses: ['1.5843', '1.4577', '1.5514', '1.2570', '1.3542']
CM score components:
transition disagreement: 0.4079
reward disagreement: 0.0856
state disagreement: 0.4630
action disagreement: 0.5682
total CM score: 1.5246
goal is complete. CM score: 1.5246
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 2459 steps, reward: 18.957
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 30.678
Reset #3: mass intervention applied (success: True)
episode 3: 99 steps, reward: 2.595
total data points collected: 24225
average episode length: 4845.0
average episode reward: 22.953
termination reasons: ['success', 'max_length', 'success', 'success', 'max_length']
success rate: 3/5
tensor shapes - states: torch.Size([24225, 56]), actions: torch.Size([24225, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0629', '7.6878', '7.0660', '6.8325', '8.2161']
Training reward models...
Reward model losses: ['0.0044', '0.2541', '0.0056', '0.3575', '0.6489']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9963', '0.8668', '0.9146', '0.9792', '1.0579']
Training action VAE models...
Action VAE losses: ['1.3695', '1.2907', '1.2709', '1.3143', '1.2032']
CM score components:
transition disagreement: 0.4174
reward disagreement: 0.2019
state disagreement: 0.4863
action disagreement: 0.5505
total CM score: 1.6562
mass is complete. CM score: 1.6562
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 47.079
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 47.079
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 47.079
total data points collected: 50005
average episode length: 10001.0
average episode reward: 47.079
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9109', '7.3069', '6.8242', '7.6982', '7.1363']
Training reward models...
Reward model losses: ['0.0771', '0.9638', '0.4156', '0.0227', '0.2848']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8267', '0.8006', '0.8532', '0.8821', '0.7343']
Training action VAE models...
Action VAE losses: ['0.8871', '0.9388', '0.9144', '0.9061', '1.0598']
CM score components:
transition disagreement: 0.4226
reward disagreement: 0.3857
state disagreement: 0.4628
action disagreement: 0.5111
total CM score: 1.7822
friction is complete. CM score: 1.7822
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 47.079
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 47.079
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 47.079
total data points collected: 50005
average episode length: 10001.0
average episode reward: 47.079
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0598', '7.7660', '7.0510', '7.1696', '7.4896']
Training reward models...
Reward model losses: ['0.0480', '0.3820', '0.0513', '0.0190', '0.0522']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8566', '0.7874', '0.8184', '0.9791', '0.8334']
Training action VAE models...
Action VAE losses: ['1.0871', '0.9099', '0.9824', '1.0554', '1.0923']
CM score components:
transition disagreement: 0.4400
reward disagreement: 0.1645
state disagreement: 0.4528
action disagreement: 0.5220
total CM score: 1.5793
visual is complete. CM score: 1.5793
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.964
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 39.072
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 14.367
total data points collected: 40018
average episode length: 8003.6
average episode reward: 18.039
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7704', '7.7178', '7.6857', '6.6341', '7.3928']
Training reward models...
Reward model losses: ['2.9601', '0.9325', '0.1451', '0.0085', '0.0350']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3171', '1.4151', '1.3397', '1.3501', '1.2353']
Training action VAE models...
Action VAE losses: ['1.3534', '1.5160', '1.3615', '1.4151', '1.2969']
CM score components:
transition disagreement: 0.4434
reward disagreement: 0.7115
state disagreement: 0.5047
action disagreement: 0.5532
total CM score: 2.2128
pose is complete. CM score: 2.2128
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 15.039
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 0.868
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.029
total data points collected: 50005
average episode length: 10001.0
average episode reward: 7.831
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4264', '7.3161', '8.3379', '7.4115', '7.3280']
Training reward models...
Reward model losses: ['0.0955', '0.0971', '0.0536', '0.3077', '0.1814']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3080', '1.2153', '1.2298', '1.2327', '1.3256']
Training action VAE models...
Action VAE losses: ['1.5264', '1.4683', '1.4641', '1.4634', '1.4448']
CM score components:
transition disagreement: 0.3765
reward disagreement: 0.1367
state disagreement: 0.4941
action disagreement: 0.5887
total CM score: 1.5960
random is complete. CM score: 1.5960
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
------------------------------------
| rollout/           |             |
|    ep_len_mean     | 7.03e+03    |
|    ep_rew_mean     | 0.021644723 |
| time/              |             |
|    fps             | 449         |
|    iterations      | 1           |
|    time_elapsed    | 9           |
|    total_timesteps | 6008832     |
------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.03e+03    |
|    ep_rew_mean          | 0.021644723 |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 6012928     |
| train/                  |             |
|    approx_kl            | 0.008285196 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.1       |
|    explained_variance   | 0.565       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.612      |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.89        |
|    value_loss           | 0.0375      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.03e+03    |
|    ep_rew_mean          | 0.021882117 |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 6017024     |
| train/                  |             |
|    approx_kl            | 0.006465244 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.1       |
|    explained_variance   | 0.603       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.568      |
|    n_updates            | 4695        |
|    policy_gradient_loss | -0.0118     |
|    std                  | 6.91        |
|    value_loss           | 0.107       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.03e+03     |
|    ep_rew_mean          | 0.021882117  |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 4            |
|    time_elapsed         | 38           |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0067072283 |
|    clip_fraction        | 0.0522       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.1        |
|    explained_variance   | 0.523        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.584       |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.0132      |
|    std                  | 6.91         |
|    value_loss           | 0.0868       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.13e+03     |
|    ep_rew_mean          | 0.0026506304 |
| time/                   |              |
|    fps                  | 429          |
|    iterations           | 5            |
|    time_elapsed         | 47           |
|    total_timesteps      | 6025216      |
| train/                  |              |
|    approx_kl            | 0.006954331  |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.1        |
|    explained_variance   | 0.558        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.564       |
|    n_updates            | 4725         |
|    policy_gradient_loss | -0.0128      |
|    std                  | 6.91         |
|    value_loss           | 0.115        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.13e+03     |
|    ep_rew_mean          | 0.0026506304 |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 6            |
|    time_elapsed         | 56           |
|    total_timesteps      | 6029312      |
| train/                  |              |
|    approx_kl            | 0.009011399  |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.1        |
|    explained_variance   | 0.65         |
|    learning_rate        | 5e-05        |
|    loss                 | -0.619       |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.0221      |
|    std                  | 6.92         |
|    value_loss           | 0.0175       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.13e+03     |
|    ep_rew_mean          | 0.0026506304 |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 7            |
|    time_elapsed         | 66           |
|    total_timesteps      | 6033408      |
| train/                  |              |
|    approx_kl            | 0.0077248677 |
|    clip_fraction        | 0.0636       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.1        |
|    explained_variance   | 0.638        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.611       |
|    n_updates            | 4755         |
|    policy_gradient_loss | -0.0151      |
|    std                  | 6.93         |
|    value_loss           | 0.0519       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.13e+03     |
|    ep_rew_mean          | -0.015547328 |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 8            |
|    time_elapsed         | 76           |
|    total_timesteps      | 6037504      |
| train/                  |              |
|    approx_kl            | 0.0076424726 |
|    clip_fraction        | 0.0733       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.2        |
|    explained_variance   | 0.779        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.614       |
|    n_updates            | 4770         |
|    policy_gradient_loss | -0.0147      |
|    std                  | 6.94         |
|    value_loss           | 0.0137       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.13e+03     |
|    ep_rew_mean          | -0.015547328 |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 9            |
|    time_elapsed         | 85           |
|    total_timesteps      | 6041600      |
| train/                  |              |
|    approx_kl            | 0.007298558  |
|    clip_fraction        | 0.0828       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.2        |
|    explained_variance   | 0.7          |
|    learning_rate        | 5e-05        |
|    loss                 | -0.621       |
|    n_updates            | 4785         |
|    policy_gradient_loss | -0.0183      |
|    std                  | 6.95         |
|    value_loss           | 0.0223       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.23e+03    |
|    ep_rew_mean          | -0.05560109 |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 10          |
|    time_elapsed         | 95          |
|    total_timesteps      | 6045696     |
| train/                  |             |
|    approx_kl            | 0.007961102 |
|    clip_fraction        | 0.0797      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.2       |
|    explained_variance   | 0.831       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.618      |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.0163     |
|    std                  | 6.96        |
|    value_loss           | 0.0139      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.23e+03    |
|    ep_rew_mean          | -0.05560109 |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 11          |
|    time_elapsed         | 104         |
|    total_timesteps      | 6049792     |
| train/                  |             |
|    approx_kl            | 0.00843659  |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.2       |
|    explained_variance   | 0.729       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.63       |
|    n_updates            | 4815        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 6.98        |
|    value_loss           | 0.0168      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.23e+03    |
|    ep_rew_mean          | -0.05560109 |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 12          |
|    time_elapsed         | 114         |
|    total_timesteps      | 6053888     |
| train/                  |             |
|    approx_kl            | 0.008189921 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.2       |
|    explained_variance   | 0.941       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.628      |
|    n_updates            | 4830        |
|    policy_gradient_loss | -0.0132     |
|    std                  | 6.98        |
|    value_loss           | 0.00264     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.049905367 |
| time/                   |              |
|    fps                  | 429          |
|    iterations           | 13           |
|    time_elapsed         | 124          |
|    total_timesteps      | 6057984      |
| train/                  |              |
|    approx_kl            | 0.008913601  |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.2        |
|    explained_variance   | 0.948        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.629       |
|    n_updates            | 4845         |
|    policy_gradient_loss | -0.0168      |
|    std                  | 6.98         |
|    value_loss           | 0.00248      |
------------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_18.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=1.138, success=False
episode 2: length=10001, reward=1.138, success=False
episode 3: length=10001, reward=1.138, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 1.138
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 75.589
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.342
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 30.833
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.473
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1696', '7.0984', '7.8178', '7.7860', '8.8933']
Training reward models...
Reward model losses: ['0.2601', '0.0158', '0.2548', '0.0055', '0.0166']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2836', '1.2835', '1.2912', '1.2864', '1.1212']
Training action VAE models...
Action VAE losses: ['1.3952', '1.4987', '1.2998', '1.4397', '1.4693']
CM score components:
transition disagreement: 0.4537
reward disagreement: 0.1567
state disagreement: 0.4851
action disagreement: 0.5814
total CM score: 1.6769
goal is complete. CM score: 1.6769
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.232
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 1.578
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.027
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.047
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4211', '7.2704', '7.6495', '8.6021', '7.9275']
Training reward models...
Reward model losses: ['0.0181', '0.1801', '1.8415', '0.2326', '0.1127']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2751', '1.4583', '1.3609', '1.2200', '1.2492']
Training action VAE models...
Action VAE losses: ['1.3705', '1.3926', '1.3729', '1.3544', '1.3640']
CM score components:
transition disagreement: 0.4386
reward disagreement: 0.4742
state disagreement: 0.5203
action disagreement: 0.5477
total CM score: 1.9807
mass is complete. CM score: 1.9807
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.138
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.138
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.138
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.138
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2114', '7.6522', '6.9242', '7.6323', '6.4326']
Training reward models...
Reward model losses: ['0.0060', '0.0046', '0.1068', '0.0635', '0.1668']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.6832', '0.6795', '0.7357', '0.7638', '0.7099']
Training action VAE models...
Action VAE losses: ['1.0898', '1.1516', '1.0626', '1.0039', '1.2341']
CM score components:
transition disagreement: 0.4453
reward disagreement: 0.0914
state disagreement: 0.4338
action disagreement: 0.5321
total CM score: 1.5026
friction is complete. CM score: 1.5026
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.138
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.138
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.138
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.138
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9110', '7.4716', '7.2171', '6.7937', '7.6203']
Training reward models...
Reward model losses: ['0.0049', '0.0205', '0.2341', '0.1752', '0.1146']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.6642', '0.6770', '0.7443', '0.7833', '0.6564']
Training action VAE models...
Action VAE losses: ['1.2545', '1.1489', '1.1694', '1.1306', '1.1374']
CM score components:
transition disagreement: 0.4262
reward disagreement: 0.1039
state disagreement: 0.4601
action disagreement: 0.5678
total CM score: 1.5579
visual is complete. CM score: 1.5579
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.860
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 30.917
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 14.365
total data points collected: 40018
average episode length: 8003.6
average episode reward: 9.289
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4668', '7.3556', '7.5843', '7.3436', '7.2779']
Training reward models...
Reward model losses: ['0.2446', '0.0385', '0.2669', '0.0160', '0.0058']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2988', '1.3548', '1.3288', '1.4028', '1.2638']
Training action VAE models...
Action VAE losses: ['1.3150', '1.4015', '1.3496', '1.4758', '1.4528']
CM score components:
transition disagreement: 0.4290
reward disagreement: 0.1545
state disagreement: 0.4816
action disagreement: 0.5195
total CM score: 1.5847
pose is complete. CM score: 1.5847
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 3.161
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 2.072
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.103
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.410
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7472', '7.6404', '8.3678', '7.7918', '7.4702']
Training reward models...
Reward model losses: ['0.0209', '0.0156', '0.3501', '0.0188', '0.1646']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2189', '1.3195', '1.3907', '1.2662', '1.1981']
Training action VAE models...
Action VAE losses: ['1.5159', '1.5356', '1.3440', '1.3875', '1.4995']
CM score components:
transition disagreement: 0.4223
reward disagreement: 0.1747
state disagreement: 0.4909
action disagreement: 0.5671
total CM score: 1.6550
random is complete. CM score: 1.6550
2025-07-16 21:39:38,905 3266759 INFO Meta-Episode 19/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_18.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 75.589
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -0.342
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 30.833
total data points collected: 50005
average episode length: 10001.0
average episode reward: 21.473
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2906', '7.0927', '6.8183', '7.1121', '7.7010']
Training reward models...
Reward model losses: ['0.2137', '0.2188', '0.2670', '0.0100', '0.0158']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1464', '1.2215', '1.2814', '1.2018', '1.2370']
Training action VAE models...
Action VAE losses: ['1.5652', '1.4493', '1.5511', '1.3255', '1.3127']
CM score components:
transition disagreement: 0.4109
reward disagreement: 0.0921
state disagreement: 0.4563
action disagreement: 0.5664
total CM score: 1.5257
goal is complete. CM score: 1.5257
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.232
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 1.578
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.027
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.047
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0638', '7.7278', '7.0184', '6.8418', '8.2882']
Training reward models...
Reward model losses: ['0.0055', '0.2400', '0.0063', '0.3865', '0.5852']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3239', '1.1589', '1.1520', '1.2753', '1.3646']
Training action VAE models...
Action VAE losses: ['1.4464', '1.4047', '1.2989', '1.4400', '1.3347']
CM score components:
transition disagreement: 0.4176
reward disagreement: 0.1863
state disagreement: 0.5159
action disagreement: 0.5466
total CM score: 1.6665
mass is complete. CM score: 1.6665
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.138
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.138
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.138
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.138
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9588', '7.3570', '6.7743', '7.5670', '7.1274']
Training reward models...
Reward model losses: ['0.0546', '1.1206', '0.4726', '0.0196', '0.3129']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7212', '0.7127', '0.7387', '0.7607', '0.6619']
Training action VAE models...
Action VAE losses: ['1.0425', '1.0570', '0.9429', '1.0172', '1.1160']
CM score components:
transition disagreement: 0.4150
reward disagreement: 0.4219
state disagreement: 0.4485
action disagreement: 0.5061
total CM score: 1.7914
friction is complete. CM score: 1.7914
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.138
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.138
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.138
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.138
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1244', '7.7518', '7.1040', '7.1294', '7.4363']
Training reward models...
Reward model losses: ['0.1115', '0.4685', '0.0756', '0.0098', '0.0655']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7738', '0.6677', '0.7393', '0.8500', '0.7274']
Training action VAE models...
Action VAE losses: ['1.0452', '1.0281', '1.0992', '1.2003', '1.2666']
CM score components:
transition disagreement: 0.4374
reward disagreement: 0.1885
state disagreement: 0.4453
action disagreement: 0.5298
total CM score: 1.6010
visual is complete. CM score: 1.6010
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.860
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 30.917
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 14.365
total data points collected: 40018
average episode length: 8003.6
average episode reward: 9.289
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7638', '7.7039', '7.6781', '6.6181', '7.3897']
Training reward models...
Reward model losses: ['3.0280', '0.9553', '0.1502', '0.0162', '0.0376']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3378', '1.4390', '1.3730', '1.3450', '1.2086']
Training action VAE models...
Action VAE losses: ['1.3826', '1.5458', '1.4490', '1.4597', '1.3542']
CM score components:
transition disagreement: 0.4423
reward disagreement: 0.7197
state disagreement: 0.5037
action disagreement: 0.5538
total CM score: 2.2195
pose is complete. CM score: 2.2195
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 3.161
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: 2.072
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 0.103
total data points collected: 50005
average episode length: 10001.0
average episode reward: 9.410
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4397', '7.3081', '8.3281', '7.3955', '7.3171']
Training reward models...
Reward model losses: ['0.0594', '0.0615', '0.0176', '0.2758', '0.1549']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3308', '1.1851', '1.2469', '1.2853', '1.2778']
Training action VAE models...
Action VAE losses: ['1.5568', '1.4413', '1.5206', '1.4858', '1.4760']
CM score components:
transition disagreement: 0.3741
reward disagreement: 0.1349
state disagreement: 0.4905
action disagreement: 0.5876
total CM score: 1.5870
random is complete. CM score: 1.5870
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
-------------------------------------
| rollout/           |              |
|    ep_len_mean     | 7.23e+03     |
|    ep_rew_mean     | -0.049905367 |
| time/              |              |
|    fps             | 444          |
|    iterations      | 1            |
|    time_elapsed    | 9            |
|    total_timesteps | 6062080      |
-------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.049905367 |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 2            |
|    time_elapsed         | 19           |
|    total_timesteps      | 6066176      |
| train/                  |              |
|    approx_kl            | 0.008677135  |
|    clip_fraction        | 0.0798       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.2        |
|    explained_variance   | 0.799        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.609       |
|    n_updates            | 4875         |
|    policy_gradient_loss | -0.0186      |
|    std                  | 7            |
|    value_loss           | 0.0547       |
------------------------------------------
Reset #2: visual intervention applied (success: True)
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.072103865 |
| time/                   |              |
|    fps                  | 429          |
|    iterations           | 3            |
|    time_elapsed         | 28           |
|    total_timesteps      | 6070272      |
| train/                  |              |
|    approx_kl            | 0.007471366  |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.2        |
|    explained_variance   | 0.826        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.598       |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.0132      |
|    std                  | 7.01         |
|    value_loss           | 0.0594       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.072103865 |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 4            |
|    time_elapsed         | 38           |
|    total_timesteps      | 6074368      |
| train/                  |              |
|    approx_kl            | 0.007812321  |
|    clip_fraction        | 0.0897       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.3        |
|    explained_variance   | 0.683        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.629       |
|    n_updates            | 4905         |
|    policy_gradient_loss | -0.0192      |
|    std                  | 7.02         |
|    value_loss           | 0.018        |
------------------------------------------
Reset #3: visual intervention applied (success: True)
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.17902029  |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 5            |
|    time_elapsed         | 47           |
|    total_timesteps      | 6078464      |
| train/                  |              |
|    approx_kl            | 0.0071010105 |
|    clip_fraction        | 0.0683       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.3        |
|    explained_variance   | 0.802        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.611       |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.014       |
|    std                  | 7.04         |
|    value_loss           | 0.0386       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.23e+03    |
|    ep_rew_mean          | -0.17902029 |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 6082560     |
| train/                  |             |
|    approx_kl            | 0.006893684 |
|    clip_fraction        | 0.0623      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.611       |
|    learning_rate        | 4.5e-05     |
|    loss                 | -0.612      |
|    n_updates            | 4935        |
|    policy_gradient_loss | -0.0159     |
|    std                  | 7.05        |
|    value_loss           | 0.0562      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.23e+03     |
|    ep_rew_mean          | -0.17902029  |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 7            |
|    time_elapsed         | 67           |
|    total_timesteps      | 6086656      |
| train/                  |              |
|    approx_kl            | 0.0075350376 |
|    clip_fraction        | 0.0687       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.3        |
|    explained_variance   | 0.819        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.608       |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.0153      |
|    std                  | 7.05         |
|    value_loss           | 0.0382       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.33e+03     |
|    ep_rew_mean          | -0.18188685  |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 8            |
|    time_elapsed         | 77           |
|    total_timesteps      | 6090752      |
| train/                  |              |
|    approx_kl            | 0.0067421664 |
|    clip_fraction        | 0.0584       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.3        |
|    explained_variance   | 0.746        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.619       |
|    n_updates            | 4965         |
|    policy_gradient_loss | -0.0165      |
|    std                  | 7.06         |
|    value_loss           | 0.0166       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.33e+03     |
|    ep_rew_mean          | -0.18188685  |
| time/                   |              |
|    fps                  | 424          |
|    iterations           | 9            |
|    time_elapsed         | 86           |
|    total_timesteps      | 6094848      |
| train/                  |              |
|    approx_kl            | 0.0073374445 |
|    clip_fraction        | 0.0817       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.3        |
|    explained_variance   | 0.625        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.617       |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.0183      |
|    std                  | 7.08         |
|    value_loss           | 0.0279       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.16990744 |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 10          |
|    time_elapsed         | 96          |
|    total_timesteps      | 6098944     |
| train/                  |             |
|    approx_kl            | 0.007479802 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.709       |
|    learning_rate        | 4.5e-05     |
|    loss                 | -0.612      |
|    n_updates            | 4995        |
|    policy_gradient_loss | -0.0128     |
|    std                  | 7.09        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.16990744 |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 11          |
|    time_elapsed         | 106         |
|    total_timesteps      | 6103040     |
| train/                  |             |
|    approx_kl            | 0.009862102 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.758       |
|    learning_rate        | 4.5e-05     |
|    loss                 | -0.597      |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.0162     |
|    std                  | 7.1         |
|    value_loss           | 0.0739      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.16990744 |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 12          |
|    time_elapsed         | 115         |
|    total_timesteps      | 6107136     |
| train/                  |             |
|    approx_kl            | 0.008118817 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.751       |
|    learning_rate        | 4.5e-05     |
|    loss                 | -0.63       |
|    n_updates            | 5025        |
|    policy_gradient_loss | -0.0162     |
|    std                  | 7.11        |
|    value_loss           | 0.00752     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.33e+03     |
|    ep_rew_mean          | -0.1764016   |
| time/                   |              |
|    fps                  | 424          |
|    iterations           | 13           |
|    time_elapsed         | 125          |
|    total_timesteps      | 6111232      |
| train/                  |              |
|    approx_kl            | 0.0077250306 |
|    clip_fraction        | 0.0644       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.4        |
|    explained_variance   | 0.733        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.625       |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.0139      |
|    std                  | 7.11         |
|    value_loss           | 0.0143       |
------------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_19.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=1.198, success=False
episode 2: length=10001, reward=1.198, success=False
episode 3: length=10001, reward=1.198, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 1.198
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 2.456
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 0.597
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -1.229
total data points collected: 30235
average episode length: 6047.0
average episode reward: 1.263
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30235, 56]), actions: torch.Size([30235, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2192', '7.1392', '7.8117', '7.8290', '8.8656']
Training reward models...
Reward model losses: ['0.2794', '0.0184', '0.2515', '0.0039', '0.0127']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2759', '1.3192', '1.3291', '1.3001', '1.0967']
Training action VAE models...
Action VAE losses: ['1.4882', '1.5430', '1.3401', '1.4921', '1.4491']
CM score components:
transition disagreement: 0.4494
reward disagreement: 0.1661
state disagreement: 0.4873
action disagreement: 0.5902
total CM score: 1.6930
goal is complete. CM score: 1.6930
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 35 steps, reward: 1.304
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 1.756
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 7.805
total data points collected: 30071
average episode length: 6014.2
average episode reward: 2.815
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30071, 56]), actions: torch.Size([30071, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3906', '7.2732', '7.6353', '8.5936', '7.9395']
Training reward models...
Reward model losses: ['0.0185', '0.1832', '1.9005', '0.2537', '0.1025']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3380', '1.5646', '1.4326', '1.3800', '1.4362']
Training action VAE models...
Action VAE losses: ['1.3688', '1.3250', '1.3659', '1.3169', '1.3449']
CM score components:
transition disagreement: 0.4383
reward disagreement: 0.4867
state disagreement: 0.5425
action disagreement: 0.5483
total CM score: 2.0159
mass is complete. CM score: 2.0159
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.245
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: 1.027
Reset #3: friction intervention applied (success: True)
episode 3: 31 steps, reward: 1.091
total data points collected: 30066
average episode length: 6013.2
average episode reward: 1.494
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30066, 56]), actions: torch.Size([30066, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2517', '7.6490', '6.9228', '7.6215', '6.4307']
Training reward models...
Reward model losses: ['0.0122', '0.0180', '0.1309', '0.0457', '0.2158']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6482', '1.4654', '1.4875', '1.5776', '1.4110']
Training action VAE models...
Action VAE losses: ['1.4054', '1.4449', '1.3922', '1.2703', '1.5938']
CM score components:
transition disagreement: 0.4458
reward disagreement: 0.1268
state disagreement: 0.5176
action disagreement: 0.5410
total CM score: 1.6312
friction is complete. CM score: 1.6312
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.198
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.198
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.198
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.198
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8916', '7.4404', '7.2320', '6.8111', '7.5745']
Training reward models...
Reward model losses: ['0.0052', '0.0340', '0.2339', '0.2184', '0.1241']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1300', '1.1248', '1.1869', '1.1882', '1.0928']
Training action VAE models...
Action VAE losses: ['1.5104', '1.4288', '1.4171', '1.4107', '1.4750']
CM score components:
transition disagreement: 0.4271
reward disagreement: 0.1191
state disagreement: 0.5062
action disagreement: 0.5861
total CM score: 1.6385
visual is complete. CM score: 1.6385
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.762
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.168
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -2.008
total data points collected: 40018
average episode length: 8003.6
average episode reward: -0.038
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4648', '7.4307', '7.5972', '7.2929', '7.2455']
Training reward models...
Reward model losses: ['0.2235', '0.0355', '0.3117', '0.0127', '0.0076']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2304', '1.2727', '1.2645', '1.3705', '1.2156']
Training action VAE models...
Action VAE losses: ['1.3086', '1.3619', '1.3374', '1.4789', '1.4320']
CM score components:
transition disagreement: 0.4261
reward disagreement: 0.1550
state disagreement: 0.4761
action disagreement: 0.5180
total CM score: 1.5752
pose is complete. CM score: 1.5752
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 2.169
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.352
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.749
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.662
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7682', '7.6540', '8.3816', '7.7915', '7.4680']
Training reward models...
Reward model losses: ['0.0181', '0.0124', '0.3825', '0.0179', '0.1453']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1898', '1.3636', '1.3841', '1.2609', '1.2455']
Training action VAE models...
Action VAE losses: ['1.4599', '1.5312', '1.3277', '1.4313', '1.6133']
CM score components:
transition disagreement: 0.4209
reward disagreement: 0.1805
state disagreement: 0.4908
action disagreement: 0.5743
total CM score: 1.6666
random is complete. CM score: 1.6666
2025-07-16 22:00:35,383 3266759 INFO Meta-Episode 20/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_19.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 2.456
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 0.597
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -1.229
total data points collected: 30235
average episode length: 6047.0
average episode reward: 1.263
termination reasons: ['max_length', 'max_length', 'max_length', 'success', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30235, 56]), actions: torch.Size([30235, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2862', '7.1527', '6.8219', '7.0981', '7.6877']
Training reward models...
Reward model losses: ['0.2009', '0.1944', '0.2342', '0.0045', '0.0128']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1414', '1.3254', '1.2785', '1.2287', '1.2733']
Training action VAE models...
Action VAE losses: ['1.5965', '1.4369', '1.4759', '1.3281', '1.3958']
CM score components:
transition disagreement: 0.4100
reward disagreement: 0.0953
state disagreement: 0.4651
action disagreement: 0.5666
total CM score: 1.5370
goal is complete. CM score: 1.5370
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 35 steps, reward: 1.304
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 1.756
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 7.805
total data points collected: 30071
average episode length: 6014.2
average episode reward: 2.815
termination reasons: ['success', 'max_length', 'max_length', 'success', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30071, 56]), actions: torch.Size([30071, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0640', '7.7164', '6.9985', '6.8613', '8.2657']
Training reward models...
Reward model losses: ['0.0050', '0.2046', '0.0060', '0.3846', '0.6088']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3679', '1.2637', '1.2632', '1.2886', '1.3620']
Training action VAE models...
Action VAE losses: ['1.4396', '1.3876', '1.2689', '1.4263', '1.2765']
CM score components:
transition disagreement: 0.4185
reward disagreement: 0.1787
state disagreement: 0.5061
action disagreement: 0.5456
total CM score: 1.6490
mass is complete. CM score: 1.6490
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.245
Reset #2: friction intervention applied (success: True)
episode 2: 32 steps, reward: 1.027
Reset #3: friction intervention applied (success: True)
episode 3: 31 steps, reward: 1.091
total data points collected: 30066
average episode length: 6013.2
average episode reward: 1.494
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([30066, 56]), actions: torch.Size([30066, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9659', '7.3271', '6.7871', '7.6427', '7.1000']
Training reward models...
Reward model losses: ['0.0676', '1.1156', '0.4551', '0.0150', '0.2801']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5279', '1.6375', '1.6085', '1.5809', '1.4690']
Training action VAE models...
Action VAE losses: ['1.2449', '1.3169', '1.2705', '1.2477', '1.3853']
CM score components:
transition disagreement: 0.4168
reward disagreement: 0.4286
state disagreement: 0.5355
action disagreement: 0.5194
total CM score: 1.9002
friction is complete. CM score: 1.9002
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.198
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.198
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.198
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.198
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0895', '7.7585', '7.0650', '7.1532', '7.4696']
Training reward models...
Reward model losses: ['0.0616', '0.4250', '0.0790', '0.0174', '0.0454']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2483', '1.0690', '1.2313', '1.3324', '1.1804']
Training action VAE models...
Action VAE losses: ['1.2464', '1.2907', '1.3076', '1.4415', '1.5533']
CM score components:
transition disagreement: 0.4378
reward disagreement: 0.1819
state disagreement: 0.4826
action disagreement: 0.5376
total CM score: 1.6399
visual is complete. CM score: 1.6399
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 0.762
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.168
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -2.008
total data points collected: 40018
average episode length: 8003.6
average episode reward: -0.038
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7811', '7.6879', '7.6722', '6.6205', '7.4527']
Training reward models...
Reward model losses: ['2.7505', '0.8601', '0.1545', '0.0099', '0.0447']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3455', '1.3569', '1.3507', '1.3844', '1.1606']
Training action VAE models...
Action VAE losses: ['1.3188', '1.4393', '1.3755', '1.4425', '1.3127']
CM score components:
transition disagreement: 0.4501
reward disagreement: 0.6651
state disagreement: 0.4949
action disagreement: 0.5587
total CM score: 2.1688
pose is complete. CM score: 2.1688
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 2.169
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.352
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.749
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.662
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4518', '7.3225', '8.3414', '7.4142', '7.3346']
Training reward models...
Reward model losses: ['0.0672', '0.0496', '0.0211', '0.2837', '0.1474']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3238', '1.2931', '1.2778', '1.3115', '1.2924']
Training action VAE models...
Action VAE losses: ['1.5798', '1.4450', '1.4869', '1.5118', '1.4671']
CM score components:
transition disagreement: 0.3749
reward disagreement: 0.1341
state disagreement: 0.4997
action disagreement: 0.5809
total CM score: 1.5895
random is complete. CM score: 1.5895
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 7.33e+03   |
|    ep_rew_mean     | -0.1764016 |
| time/              |            |
|    fps             | 445        |
|    iterations      | 1          |
|    time_elapsed    | 9          |
|    total_timesteps | 6115328    |
-----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.1764016  |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 6119424     |
| train/                  |             |
|    approx_kl            | 0.006523486 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.805       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.591      |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 7.12        |
|    value_loss           | 0.0789      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.15413815 |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 6123520     |
| train/                  |             |
|    approx_kl            | 0.006835665 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.746       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.612      |
|    n_updates            | 5085        |
|    policy_gradient_loss | -0.0135     |
|    std                  | 7.13        |
|    value_loss           | 0.0335      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.15413815 |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 6127616     |
| train/                  |             |
|    approx_kl            | 0.006052376 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.902       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.618      |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.0132     |
|    std                  | 7.14        |
|    value_loss           | 0.0258      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.17026459  |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 5            |
|    time_elapsed         | 48           |
|    total_timesteps      | 6131712      |
| train/                  |              |
|    approx_kl            | 0.0066460078 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.4        |
|    explained_variance   | 0.752        |
|    learning_rate        | 4.05e-05     |
|    loss                 | -0.623       |
|    n_updates            | 5115         |
|    policy_gradient_loss | -0.0135      |
|    std                  | 7.15         |
|    value_loss           | 0.0171       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.17026459 |
| time/                   |             |
|    fps                  | 424         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 6135808     |
| train/                  |             |
|    approx_kl            | 0.007368221 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.754       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.618      |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 7.17        |
|    value_loss           | 0.0285      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.17026459 |
| time/                   |             |
|    fps                  | 426         |
|    iterations           | 7           |
|    time_elapsed         | 67          |
|    total_timesteps      | 6139904     |
| train/                  |             |
|    approx_kl            | 0.008059157 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.702       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.624      |
|    n_updates            | 5145        |
|    policy_gradient_loss | -0.016      |
|    std                  | 7.18        |
|    value_loss           | 0.0238      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.16442898 |
| time/                   |             |
|    fps                  | 428         |
|    iterations           | 8           |
|    time_elapsed         | 76          |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.007987331 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.899       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.624      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 7.18        |
|    value_loss           | 0.00854     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.16442898 |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 9           |
|    time_elapsed         | 85          |
|    total_timesteps      | 6148096     |
| train/                  |             |
|    approx_kl            | 0.008028673 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.86        |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.633      |
|    n_updates            | 5175        |
|    policy_gradient_loss | -0.018      |
|    std                  | 7.19        |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.170471   |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 10          |
|    time_elapsed         | 95          |
|    total_timesteps      | 6152192     |
| train/                  |             |
|    approx_kl            | 0.008860038 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.702       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.626      |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.017      |
|    std                  | 7.2         |
|    value_loss           | 0.0159      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.170471    |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 11           |
|    time_elapsed         | 104          |
|    total_timesteps      | 6156288      |
| train/                  |              |
|    approx_kl            | 0.0075227367 |
|    clip_fraction        | 0.0789       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.5        |
|    explained_variance   | 0.911        |
|    learning_rate        | 4.05e-05     |
|    loss                 | -0.629       |
|    n_updates            | 5205         |
|    policy_gradient_loss | -0.0151      |
|    std                  | 7.2          |
|    value_loss           | 0.024        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.170471   |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 12          |
|    time_elapsed         | 113         |
|    total_timesteps      | 6160384     |
| train/                  |             |
|    approx_kl            | 0.006819682 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.651       |
|    learning_rate        | 4.05e-05    |
|    loss                 | -0.623      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.015      |
|    std                  | 7.22        |
|    value_loss           | 0.0301      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.28378388  |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 13           |
|    time_elapsed         | 123          |
|    total_timesteps      | 6164480      |
| train/                  |              |
|    approx_kl            | 0.0091443565 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.5        |
|    explained_variance   | 0.617        |
|    learning_rate        | 4.05e-05     |
|    loss                 | -0.621       |
|    n_updates            | 5235         |
|    policy_gradient_loss | -0.0183      |
|    std                  | 7.23         |
|    value_loss           | 0.0251       |
------------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_20.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=-0.303, success=False
episode 2: length=10001, reward=-0.303, success=False
episode 3: length=10001, reward=-0.303, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.303
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 0.806
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -1.402
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -1.999
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.100
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1936', '7.0828', '7.7920', '7.8154', '8.8788']
Training reward models...
Reward model losses: ['0.2235', '0.0119', '0.2551', '0.0065', '0.0206']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2044', '1.2557', '1.2607', '1.2033', '1.0299']
Training action VAE models...
Action VAE losses: ['1.4672', '1.4243', '1.3412', '1.3751', '1.4814']
CM score components:
transition disagreement: 0.4508
reward disagreement: 0.1374
state disagreement: 0.4835
action disagreement: 0.5765
total CM score: 1.6482
goal is complete. CM score: 1.6482
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.601
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.588
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.238
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.021
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.2285', '7.6683', '8.5928', '7.9709']
Training reward models...
Reward model losses: ['0.0079', '0.1646', '1.7717', '0.2228', '0.1285']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3538', '1.4873', '1.3376', '1.2107', '1.3094']
Training action VAE models...
Action VAE losses: ['1.3413', '1.4172', '1.3453', '1.3053', '1.3566']
CM score components:
transition disagreement: 0.4299
reward disagreement: 0.4725
state disagreement: 0.5272
action disagreement: 0.5419
total CM score: 1.9715
mass is complete. CM score: 1.9715
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -2.677
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -0.138
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.015
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2582', '7.6394', '6.9382', '7.6494', '6.4344']
Training reward models...
Reward model losses: ['0.0163', '0.0282', '0.1646', '0.0336', '0.2394']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9966', '0.9516', '1.0436', '0.9908', '0.9752']
Training action VAE models...
Action VAE losses: ['1.3391', '1.4058', '1.4378', '1.3886', '1.6864']
CM score components:
transition disagreement: 0.4452
reward disagreement: 0.1338
state disagreement: 0.4614
action disagreement: 0.5498
total CM score: 1.5901
friction is complete. CM score: 1.5901
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.303
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.303
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.303
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.303
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9253', '7.5276', '7.2680', '6.7837', '7.5762']
Training reward models...
Reward model losses: ['0.0049', '0.0458', '0.2606', '0.2231', '0.2041']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.7846', '0.7869', '0.8734', '0.9279', '0.8191']
Training action VAE models...
Action VAE losses: ['1.5554', '1.4431', '1.5159', '1.3773', '1.3477']
CM score components:
transition disagreement: 0.4257
reward disagreement: 0.1506
state disagreement: 0.4696
action disagreement: 0.5763
total CM score: 1.6222
visual is complete. CM score: 1.6222
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 15 steps, reward: 0.799
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 2.121
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 22.406
total data points collected: 40019
average episode length: 8003.8
average episode reward: 5.752
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40019, 56]), actions: torch.Size([40019, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4987', '7.4195', '7.5802', '7.3455', '7.2555']
Training reward models...
Reward model losses: ['0.2400', '0.0389', '0.2930', '0.0138', '0.0064']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1788', '1.2082', '1.1874', '1.2231', '1.1399']
Training action VAE models...
Action VAE losses: ['1.3544', '1.3748', '1.3884', '1.5000', '1.4750']
CM score components:
transition disagreement: 0.4279
reward disagreement: 0.1595
state disagreement: 0.4765
action disagreement: 0.5199
total CM score: 1.5839
pose is complete. CM score: 1.5839
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 3.961
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -3.827
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.599
total data points collected: 50005
average episode length: 10001.0
average episode reward: 2.383
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7579', '7.6499', '8.3799', '7.8038', '7.4753']
Training reward models...
Reward model losses: ['0.0260', '0.0210', '0.3972', '0.0208', '0.1754']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2265', '1.4231', '1.3400', '1.3043', '1.2253']
Training action VAE models...
Action VAE losses: ['1.4797', '1.5502', '1.3546', '1.4105', '1.5334']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.1868
state disagreement: 0.4874
action disagreement: 0.5697
total CM score: 1.6678
random is complete. CM score: 1.6678
2025-07-16 22:20:55,634 3266759 INFO Meta-Episode 21/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_20.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 0.806
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -1.402
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -1.999
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.100
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2692', '7.0975', '6.8006', '7.1048', '7.6671']
Training reward models...
Reward model losses: ['0.2461', '0.2280', '0.2582', '0.0113', '0.0198']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0542', '1.1791', '1.2020', '1.1374', '1.1590']
Training action VAE models...
Action VAE losses: ['1.4562', '1.4063', '1.4950', '1.3665', '1.3448']
CM score components:
transition disagreement: 0.4097
reward disagreement: 0.0810
state disagreement: 0.4536
action disagreement: 0.5541
total CM score: 1.4984
goal is complete. CM score: 1.4984
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: -2.601
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.588
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.238
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.021
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0672', '7.7237', '7.0472', '6.8572', '8.2422']
Training reward models...
Reward model losses: ['0.0081', '0.2564', '0.0096', '0.3125', '0.6651']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3505', '1.1748', '1.1915', '1.3007', '1.4533']
Training action VAE models...
Action VAE losses: ['1.4602', '1.3995', '1.3184', '1.4341', '1.2952']
CM score components:
transition disagreement: 0.4200
reward disagreement: 0.2180
state disagreement: 0.5216
action disagreement: 0.5463
total CM score: 1.7059
mass is complete. CM score: 1.7059
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -2.677
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -0.138
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.015
total data points collected: 50005
average episode length: 10001.0
average episode reward: -1.137
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9107', '7.3033', '6.8262', '7.6941', '7.0622']
Training reward models...
Reward model losses: ['0.0864', '1.1756', '0.4750', '0.0254', '0.1940']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0124', '0.9929', '1.0240', '1.0350', '0.9107']
Training action VAE models...
Action VAE losses: ['1.1851', '1.2274', '1.2122', '1.1560', '1.4050']
CM score components:
transition disagreement: 0.4174
reward disagreement: 0.4348
state disagreement: 0.4772
action disagreement: 0.5412
total CM score: 1.8706
friction is complete. CM score: 1.8706
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.303
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.303
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.303
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.303
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1251', '7.7671', '7.0734', '7.1869', '7.4742']
Training reward models...
Reward model losses: ['0.0484', '0.3363', '0.0846', '0.0161', '0.0393']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8703', '0.8296', '0.8348', '1.0165', '0.8507']
Training action VAE models...
Action VAE losses: ['1.4429', '1.2119', '1.2091', '1.3427', '1.4076']
CM score components:
transition disagreement: 0.4426
reward disagreement: 0.1529
state disagreement: 0.4539
action disagreement: 0.5370
total CM score: 1.5864
visual is complete. CM score: 1.5864
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 15 steps, reward: 0.799
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 2.121
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 22.406
total data points collected: 40019
average episode length: 8003.8
average episode reward: 5.752
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40019, 56]), actions: torch.Size([40019, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7706', '7.7242', '7.6896', '6.6466', '7.4119']
Training reward models...
Reward model losses: ['2.9147', '0.9546', '0.1575', '0.0121', '0.0400']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2438', '1.2557', '1.2783', '1.2886', '1.1229']
Training action VAE models...
Action VAE losses: ['1.4343', '1.5702', '1.4078', '1.4700', '1.3287']
CM score components:
transition disagreement: 0.4470
reward disagreement: 0.7084
state disagreement: 0.5010
action disagreement: 0.5564
total CM score: 2.2127
pose is complete. CM score: 2.2127
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 3.961
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -3.827
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 4.599
total data points collected: 50005
average episode length: 10001.0
average episode reward: 2.383
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4257', '7.3116', '8.3314', '7.3939', '7.3265']
Training reward models...
Reward model losses: ['0.0662', '0.0605', '0.0222', '0.2871', '0.1561']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3048', '1.2933', '1.2879', '1.2695', '1.4054']
Training action VAE models...
Action VAE losses: ['1.5723', '1.4629', '1.4723', '1.4893', '1.4864']
CM score components:
transition disagreement: 0.3749
reward disagreement: 0.1372
state disagreement: 0.5022
action disagreement: 0.5832
total CM score: 1.5975
random is complete. CM score: 1.5975
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
------------------------------------
| rollout/           |             |
|    ep_len_mean     | 7.43e+03    |
|    ep_rew_mean     | -0.28378388 |
| time/              |             |
|    fps             | 464         |
|    iterations      | 1           |
|    time_elapsed    | 8           |
|    total_timesteps | 6168576     |
------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.28378388 |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 6172672     |
| train/                  |             |
|    approx_kl            | 0.006077397 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.619       |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.624      |
|    n_updates            | 5265        |
|    policy_gradient_loss | -0.0139     |
|    std                  | 7.25        |
|    value_loss           | 0.0221      |
-----------------------------------------
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.33e+03    |
|    ep_rew_mean          | -0.33584622 |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 6176768     |
| train/                  |             |
|    approx_kl            | 0.008492075 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.794       |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.63       |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.0165     |
|    std                  | 7.26        |
|    value_loss           | 0.0137      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.33e+03     |
|    ep_rew_mean          | -0.33584622  |
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 4            |
|    time_elapsed         | 37           |
|    total_timesteps      | 6180864      |
| train/                  |              |
|    approx_kl            | 0.0075602005 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.6        |
|    explained_variance   | 0.89         |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.627       |
|    n_updates            | 5295         |
|    policy_gradient_loss | -0.0186      |
|    std                  | 7.27         |
|    value_loss           | 0.0285       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.39424706 |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 5           |
|    time_elapsed         | 46          |
|    total_timesteps      | 6184960     |
| train/                  |             |
|    approx_kl            | 0.009420998 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.881       |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.623      |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 7.28        |
|    value_loss           | 0.00763     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.39424706  |
| time/                   |              |
|    fps                  | 438          |
|    iterations           | 6            |
|    time_elapsed         | 56           |
|    total_timesteps      | 6189056      |
| train/                  |              |
|    approx_kl            | 0.0074100355 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.6        |
|    explained_variance   | 0.777        |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.62        |
|    n_updates            | 5325         |
|    policy_gradient_loss | -0.0163      |
|    std                  | 7.29         |
|    value_loss           | 0.038        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.39424706 |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 7           |
|    time_elapsed         | 65          |
|    total_timesteps      | 6193152     |
| train/                  |             |
|    approx_kl            | 0.008451799 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.548       |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.635      |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0163     |
|    std                  | 7.29        |
|    value_loss           | 0.019       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.4322323   |
| time/                   |              |
|    fps                  | 433          |
|    iterations           | 8            |
|    time_elapsed         | 75           |
|    total_timesteps      | 6197248      |
| train/                  |              |
|    approx_kl            | 0.0071978453 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.6        |
|    explained_variance   | 0.768        |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.624       |
|    n_updates            | 5355         |
|    policy_gradient_loss | -0.014       |
|    std                  | 7.29         |
|    value_loss           | 0.0346       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.4322323   |
| time/                   |              |
|    fps                  | 432          |
|    iterations           | 9            |
|    time_elapsed         | 85           |
|    total_timesteps      | 6201344      |
| train/                  |              |
|    approx_kl            | 0.0069082202 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.6        |
|    explained_variance   | 0.82         |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.616       |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.0165      |
|    std                  | 7.31         |
|    value_loss           | 0.0433       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.4235931  |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 10          |
|    time_elapsed         | 94          |
|    total_timesteps      | 6205440     |
| train/                  |             |
|    approx_kl            | 0.006023079 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.626      |
|    n_updates            | 5385        |
|    policy_gradient_loss | -0.0131     |
|    std                  | 7.32        |
|    value_loss           | 0.0174      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.43e+03     |
|    ep_rew_mean          | -0.4235931   |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 11           |
|    time_elapsed         | 104          |
|    total_timesteps      | 6209536      |
| train/                  |              |
|    approx_kl            | 0.0067839995 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.6        |
|    explained_variance   | 0.676        |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.625       |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.0153      |
|    std                  | 7.32         |
|    value_loss           | 0.0252       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.43e+03    |
|    ep_rew_mean          | -0.4235931  |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 12          |
|    time_elapsed         | 113         |
|    total_timesteps      | 6213632     |
| train/                  |             |
|    approx_kl            | 0.007652445 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.646       |
|    learning_rate        | 3.65e-05    |
|    loss                 | -0.626      |
|    n_updates            | 5415        |
|    policy_gradient_loss | -0.014      |
|    std                  | 7.33        |
|    value_loss           | 0.019       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.53e+03   |
|    ep_rew_mean          | -0.4203796 |
| time/                   |            |
|    fps                  | 431        |
|    iterations           | 13         |
|    time_elapsed         | 123        |
|    total_timesteps      | 6217728    |
| train/                  |            |
|    approx_kl            | 0.00707571 |
|    clip_fraction        | 0.0676     |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.6      |
|    explained_variance   | 0.772      |
|    learning_rate        | 3.65e-05   |
|    loss                 | -0.624     |
|    n_updates            | 5430       |
|    policy_gradient_loss | -0.0145    |
|    std                  | 7.34       |
|    value_loss           | 0.0218     |
----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_21.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=1.501, success=False
episode 2: length=10001, reward=1.501, success=False
episode 3: length=10001, reward=1.501, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 1.501
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 2.098
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -2.404
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -0.164
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.437
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2215', '7.1222', '7.8217', '7.8380', '8.8412']
Training reward models...
Reward model losses: ['0.2908', '0.0187', '0.2388', '0.0033', '0.0158']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2522', '1.3076', '1.3188', '1.2849', '1.1589']
Training action VAE models...
Action VAE losses: ['1.4907', '1.4128', '1.3402', '1.4820', '1.4748']
CM score components:
transition disagreement: 0.4505
reward disagreement: 0.1621
state disagreement: 0.4945
action disagreement: 0.5848
total CM score: 1.6918
goal is complete. CM score: 1.6918
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.251
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.447
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.567
total data points collected: 50005
average episode length: 10001.0
average episode reward: 5.183
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4031', '7.2743', '7.6503', '8.5826', '7.9475']
Training reward models...
Reward model losses: ['0.0165', '0.1972', '1.9028', '0.2393', '0.1226']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2439', '1.4468', '1.3075', '1.1460', '1.2509']
Training action VAE models...
Action VAE losses: ['1.3777', '1.3492', '1.4253', '1.3362', '1.3525']
CM score components:
transition disagreement: 0.4381
reward disagreement: 0.4911
state disagreement: 0.5246
action disagreement: 0.5478
total CM score: 2.0017
mass is complete. CM score: 2.0017
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.525
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.220
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.447
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.426
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2482', '7.6528', '6.9158', '7.6048', '6.4244']
Training reward models...
Reward model losses: ['0.0190', '0.0151', '0.1138', '0.0525', '0.2141']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9433', '0.9316', '0.9989', '1.0023', '0.9611']
Training action VAE models...
Action VAE losses: ['1.3987', '1.4169', '1.3528', '1.2561', '1.5250']
CM score components:
transition disagreement: 0.4495
reward disagreement: 0.1145
state disagreement: 0.4592
action disagreement: 0.5384
total CM score: 1.5616
friction is complete. CM score: 1.5616
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.501
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.501
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.501
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.501
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8885', '7.4402', '7.2413', '6.7982', '7.5703']
Training reward models...
Reward model losses: ['0.0053', '0.0395', '0.2408', '0.2387', '0.1213']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9061', '0.9163', '0.9868', '1.0228', '0.8888']
Training action VAE models...
Action VAE losses: ['1.5138', '1.4990', '1.4714', '1.3756', '1.5174']
CM score components:
transition disagreement: 0.4282
reward disagreement: 0.1268
state disagreement: 0.4826
action disagreement: 0.5897
total CM score: 1.6273
visual is complete. CM score: 1.6273
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 1.192
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.267
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.440
total data points collected: 40018
average episode length: 8003.6
average episode reward: -0.751
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4730', '7.4161', '7.5848', '7.3139', '7.2544']
Training reward models...
Reward model losses: ['0.2284', '0.0346', '0.3319', '0.0137', '0.0073']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2035', '1.3162', '1.2835', '1.3935', '1.2636']
Training action VAE models...
Action VAE losses: ['1.3136', '1.3734', '1.3574', '1.5128', '1.4778']
CM score components:
transition disagreement: 0.4262
reward disagreement: 0.1676
state disagreement: 0.4795
action disagreement: 0.5185
total CM score: 1.5918
pose is complete. CM score: 1.5918
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 1.417
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.471
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.556
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.023
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7727', '7.6361', '8.3790', '7.8008', '7.4706']
Training reward models...
Reward model losses: ['0.0203', '0.0122', '0.3649', '0.0217', '0.1393']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2091', '1.2932', '1.3523', '1.2614', '1.1789']
Training action VAE models...
Action VAE losses: ['1.5306', '1.4897', '1.3744', '1.3562', '1.5329']
CM score components:
transition disagreement: 0.4185
reward disagreement: 0.1730
state disagreement: 0.4860
action disagreement: 0.5731
total CM score: 1.6506
random is complete. CM score: 1.6506
2025-07-16 22:43:29,876 3266759 INFO Meta-Episode 22/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_21.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 2.098
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: -2.404
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: -0.164
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.437
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2768', '7.1215', '6.8387', '7.0942', '7.7026']
Training reward models...
Reward model losses: ['0.1898', '0.1907', '0.2238', '0.0056', '0.0079']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1328', '1.3244', '1.2101', '1.1930', '1.2231']
Training action VAE models...
Action VAE losses: ['1.6187', '1.4417', '1.3795', '1.2738', '1.2929']
CM score components:
transition disagreement: 0.4123
reward disagreement: 0.0934
state disagreement: 0.4592
action disagreement: 0.5642
total CM score: 1.5290
goal is complete. CM score: 1.5290
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.251
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: 16.447
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 0.567
total data points collected: 50005
average episode length: 10001.0
average episode reward: 5.183
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0649', '7.7309', '7.0171', '6.8448', '8.2680']
Training reward models...
Reward model losses: ['0.0042', '0.2305', '0.0060', '0.3784', '0.5963']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2751', '1.1002', '1.1196', '1.2610', '1.3256']
Training action VAE models...
Action VAE losses: ['1.4386', '1.3569', '1.2674', '1.3644', '1.2970']
CM score components:
transition disagreement: 0.4191
reward disagreement: 0.1872
state disagreement: 0.5167
action disagreement: 0.5440
total CM score: 1.6670
mass is complete. CM score: 1.6670
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: 1.525
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: 1.220
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: 1.447
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.426
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9855', '7.3351', '6.7639', '7.6003', '7.1038']
Training reward models...
Reward model losses: ['0.0640', '1.0819', '0.4507', '0.0150', '0.3127']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9855', '0.9679', '0.9771', '1.0358', '0.9151']
Training action VAE models...
Action VAE losses: ['1.3022', '1.3286', '1.2667', '1.2928', '1.3870']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.4174
state disagreement: 0.4755
action disagreement: 0.5106
total CM score: 1.8239
friction is complete. CM score: 1.8239
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: 1.501
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: 1.501
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: 1.501
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.501
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0837', '7.7633', '7.0808', '7.1543', '7.4671']
Training reward models...
Reward model losses: ['0.0538', '0.4116', '0.0871', '0.0157', '0.0494']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0077', '0.9077', '0.9964', '1.0845', '0.9640']
Training action VAE models...
Action VAE losses: ['1.2866', '1.3139', '1.3364', '1.4684', '1.6312']
CM score components:
transition disagreement: 0.4375
reward disagreement: 0.1779
state disagreement: 0.4677
action disagreement: 0.5353
total CM score: 1.6184
visual is complete. CM score: 1.6184
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 14 steps, reward: 1.192
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 1.267
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: -1.440
total data points collected: 40018
average episode length: 8003.6
average episode reward: -0.751
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 1/5
tensor shapes - states: torch.Size([40018, 56]), actions: torch.Size([40018, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7785', '7.6963', '7.6902', '6.6229', '7.4299']
Training reward models...
Reward model losses: ['2.8369', '0.9201', '0.1562', '0.0144', '0.0449']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3452', '1.3881', '1.3470', '1.4508', '1.1813']
Training action VAE models...
Action VAE losses: ['1.3629', '1.4680', '1.4072', '1.4996', '1.3423']
CM score components:
transition disagreement: 0.4476
reward disagreement: 0.6903
state disagreement: 0.5003
action disagreement: 0.5605
total CM score: 2.1987
pose is complete. CM score: 2.1987
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 1.417
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -2.471
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 1.556
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.023
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4568', '7.3186', '8.3567', '7.4311', '7.3357']
Training reward models...
Reward model losses: ['0.0715', '0.0401', '0.0216', '0.2724', '0.1486']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2957', '1.2186', '1.3054', '1.2737', '1.2710']
Training action VAE models...
Action VAE losses: ['1.5288', '1.4467', '1.4410', '1.4577', '1.4520']
CM score components:
transition disagreement: 0.3762
reward disagreement: 0.1253
state disagreement: 0.4926
action disagreement: 0.5812
total CM score: 1.5753
random is complete. CM score: 1.5753
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 7.53e+03   |
|    ep_rew_mean     | -0.4203796 |
| time/              |            |
|    fps             | 473        |
|    iterations      | 1          |
|    time_elapsed    | 8          |
|    total_timesteps | 6221824    |
-----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.53e+03     |
|    ep_rew_mean          | -0.4203796   |
| time/                   |              |
|    fps                  | 453          |
|    iterations           | 2            |
|    time_elapsed         | 18           |
|    total_timesteps      | 6225920      |
| train/                  |              |
|    approx_kl            | 0.0072130826 |
|    clip_fraction        | 0.0598       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.794        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.611       |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.0166      |
|    std                  | 7.35         |
|    value_loss           | 0.0386       |
------------------------------------------
Reset #2: visual intervention applied (success: True)
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.61e+03     |
|    ep_rew_mean          | -0.502076    |
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 3            |
|    time_elapsed         | 27           |
|    total_timesteps      | 6230016      |
| train/                  |              |
|    approx_kl            | 0.0072516073 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.731        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.62        |
|    n_updates            | 5475         |
|    policy_gradient_loss | -0.0164      |
|    std                  | 7.36         |
|    value_loss           | 0.0301       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.61e+03    |
|    ep_rew_mean          | -0.502076   |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 4           |
|    time_elapsed         | 37          |
|    total_timesteps      | 6234112     |
| train/                  |             |
|    approx_kl            | 0.007271883 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.762       |
|    learning_rate        | 3.28e-05    |
|    loss                 | -0.638      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 7.37        |
|    value_loss           | 0.0116      |
-----------------------------------------
Reset #3: visual intervention applied (success: True)
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.61e+03    |
|    ep_rew_mean          | -0.50583786 |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 6238208     |
| train/                  |             |
|    approx_kl            | 0.006256131 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.897       |
|    learning_rate        | 3.28e-05    |
|    loss                 | -0.626      |
|    n_updates            | 5505        |
|    policy_gradient_loss | -0.0123     |
|    std                  | 7.37        |
|    value_loss           | 0.00616     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.61e+03    |
|    ep_rew_mean          | -0.50583786 |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 6           |
|    time_elapsed         | 57          |
|    total_timesteps      | 6242304     |
| train/                  |             |
|    approx_kl            | 0.007543044 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.605       |
|    learning_rate        | 3.28e-05    |
|    loss                 | -0.584      |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.0125     |
|    std                  | 7.38        |
|    value_loss           | 0.111       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.61e+03     |
|    ep_rew_mean          | -0.50583786  |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 7            |
|    time_elapsed         | 67           |
|    total_timesteps      | 6246400      |
| train/                  |              |
|    approx_kl            | 0.0073861605 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.708        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.623       |
|    n_updates            | 5535         |
|    policy_gradient_loss | -0.016       |
|    std                  | 7.39         |
|    value_loss           | 0.0477       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.61e+03    |
|    ep_rew_mean          | -0.5288717  |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 8           |
|    time_elapsed         | 77          |
|    total_timesteps      | 6250496     |
| train/                  |             |
|    approx_kl            | 0.006809296 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.838       |
|    learning_rate        | 3.28e-05    |
|    loss                 | -0.629      |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.0134     |
|    std                  | 7.39        |
|    value_loss           | 0.0102      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.61e+03     |
|    ep_rew_mean          | -0.5288717   |
| time/                   |              |
|    fps                  | 422          |
|    iterations           | 9            |
|    time_elapsed         | 87           |
|    total_timesteps      | 6254592      |
| train/                  |              |
|    approx_kl            | 0.0069865556 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.832        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.626       |
|    n_updates            | 5565         |
|    policy_gradient_loss | -0.0159      |
|    std                  | 7.4          |
|    value_loss           | 0.0228       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.71e+03     |
|    ep_rew_mean          | -0.5386609   |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 10           |
|    time_elapsed         | 97           |
|    total_timesteps      | 6258688      |
| train/                  |              |
|    approx_kl            | 0.0057716714 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.873        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.623       |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.00938     |
|    std                  | 7.4          |
|    value_loss           | 0.00529      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.71e+03     |
|    ep_rew_mean          | -0.5386609   |
| time/                   |              |
|    fps                  | 420          |
|    iterations           | 11           |
|    time_elapsed         | 107          |
|    total_timesteps      | 6262784      |
| train/                  |              |
|    approx_kl            | 0.0060137985 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.862        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.636       |
|    n_updates            | 5595         |
|    policy_gradient_loss | -0.0136      |
|    std                  | 7.41         |
|    value_loss           | 0.0216       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 7.71e+03     |
|    ep_rew_mean          | -0.5386609   |
| time/                   |              |
|    fps                  | 419          |
|    iterations           | 12           |
|    time_elapsed         | 117          |
|    total_timesteps      | 6266880      |
| train/                  |              |
|    approx_kl            | 0.0060163904 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -30.7        |
|    explained_variance   | 0.838        |
|    learning_rate        | 3.28e-05     |
|    loss                 | -0.62        |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.0122      |
|    std                  | 7.41         |
|    value_loss           | 0.0293       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.81e+03    |
|    ep_rew_mean          | -0.53605855 |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 13          |
|    time_elapsed         | 127         |
|    total_timesteps      | 6270976     |
| train/                  |             |
|    approx_kl            | 0.005808552 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.805       |
|    learning_rate        | 3.28e-05    |
|    loss                 | -0.639      |
|    n_updates            | 5625        |
|    policy_gradient_loss | -0.0127     |
|    std                  | 7.42        |
|    value_loss           | 0.0074      |
-----------------------------------------
Training verification for visual:
  Episodes completed: 0
  Mean reward: 0.000
  Mean length: 0.0
  Success rate: 0.000
  WARNING: No episodes completed during training!
training episode rewards: []
saved student model to meta_teacher_student_logs/temp_student_model_episode_22.zip

evaluating student performance (5 episodes)...
episode 1: length=10001, reward=-0.063, success=False
episode 2: length=10001, reward=-0.063, success=False
episode 3: length=10001, reward=-0.063, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: -0.063
average episode length: 10001.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 0.924
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 1.288
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 2.307
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.216
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1828', '7.0776', '7.8146', '7.8275', '8.8840']
Training reward models...
Reward model losses: ['0.2036', '0.0092', '0.2631', '0.0038', '0.0199']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1632', '1.1601', '1.1710', '1.0925', '0.9890']
Training action VAE models...
Action VAE losses: ['1.4923', '1.4081', '1.2709', '1.3855', '1.5092']
CM score components:
transition disagreement: 0.4502
reward disagreement: 0.1305
state disagreement: 0.4718
action disagreement: 0.5752
total CM score: 1.6276
goal is complete. CM score: 1.6276
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.429
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.546
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 1.037
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.715
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4486', '7.2257', '7.6649', '8.5983', '7.9606']
Training reward models...
Reward model losses: ['0.0111', '0.1640', '1.8093', '0.2315', '0.1117']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3144', '1.4892', '1.2493', '1.2038', '1.2786']
Training action VAE models...
Action VAE losses: ['1.3481', '1.4541', '1.3500', '1.3098', '1.3238']
CM score components:
transition disagreement: 0.4309
reward disagreement: 0.4756
state disagreement: 0.5192
action disagreement: 0.5387
total CM score: 1.9644
mass is complete. CM score: 1.9644
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -0.063
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -0.063
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.063
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.063
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.2534', '7.6493', '6.9251', '7.6301', '6.4348']
Training reward models...
Reward model losses: ['0.0062', '0.0167', '0.1417', '0.0261', '0.2244']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9004', '0.8633', '0.9412', '0.9152', '0.8942']
Training action VAE models...
Action VAE losses: ['1.3872', '1.4429', '1.4073', '1.4187', '1.7196']
CM score components:
transition disagreement: 0.4467
reward disagreement: 0.1319
state disagreement: 0.4509
action disagreement: 0.5559
total CM score: 1.5854
friction is complete. CM score: 1.5854
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.063
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.063
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.063
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.063
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9281', '7.5208', '7.2608', '6.7746', '7.5722']
Training reward models...
Reward model losses: ['0.0020', '0.0487', '0.2668', '0.2230', '0.1970']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.8445', '0.8594', '0.9198', '0.9877', '0.8410']
Training action VAE models...
Action VAE losses: ['1.5577', '1.4562', '1.4516', '1.4574', '1.4200']
CM score components:
transition disagreement: 0.4281
reward disagreement: 0.1554
state disagreement: 0.4723
action disagreement: 0.5856
total CM score: 1.6415
visual is complete. CM score: 1.6415
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 13 steps, reward: 0.733
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 0.414
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 0.943
total data points collected: 30047
average episode length: 6009.4
average episode reward: 0.761
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30047, 56]), actions: torch.Size([30047, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4693', '7.4717', '7.5882', '7.3176', '7.2515']
Training reward models...
Reward model losses: ['0.2160', '0.0404', '0.2713', '0.0124', '0.0075']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2752', '1.0828', '1.1311', '1.0604', '1.2430']
Training action VAE models...
Action VAE losses: ['1.1996', '1.2996', '1.2282', '1.3912', '1.3947']
CM score components:
transition disagreement: 0.4244
reward disagreement: 0.1361
state disagreement: 0.4974
action disagreement: 0.5243
total CM score: 1.5823
pose is complete. CM score: 1.5823
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 2.524
Reset #2: random intervention applied (success: True)
episode 2: 10001 steps, reward: -0.490
Reset #3: random intervention applied (success: True)
episode 3: 10001 steps, reward: 2.414
total data points collected: 50005
average episode length: 10001.0
average episode reward: 2.606
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7521', '7.6411', '8.3742', '7.7865', '7.4877']
Training reward models...
Reward model losses: ['0.0225', '0.0145', '0.3537', '0.0235', '0.1564']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.1892', '1.2944', '1.2876', '1.2627', '1.1491']
Training action VAE models...
Action VAE losses: ['1.5481', '1.5068', '1.3932', '1.3647', '1.5141']
CM score components:
transition disagreement: 0.4171
reward disagreement: 0.1716
state disagreement: 0.4836
action disagreement: 0.5733
total CM score: 1.6456
random is complete. CM score: 1.6456
2025-07-16 23:06:13,842 3266759 INFO Meta-Episode 23/30: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
loading student model from meta_teacher_student_logs/temp_student_model_episode_22.zip
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 10001 steps, reward: 0.924
Reset #2: goal intervention applied (success: True)
episode 2: 10001 steps, reward: 1.288
Reset #3: goal intervention applied (success: True)
episode 3: 10001 steps, reward: 2.307
total data points collected: 50005
average episode length: 10001.0
average episode reward: 1.216
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2671', '7.0929', '6.7893', '7.1160', '7.6719']
Training reward models...
Reward model losses: ['0.2396', '0.2150', '0.2606', '0.0114', '0.0157']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.0063', '1.1282', '1.1160', '1.0854', '1.1201']
Training action VAE models...
Action VAE losses: ['1.4393', '1.3882', '1.5181', '1.3226', '1.3429']
CM score components:
transition disagreement: 0.4092
reward disagreement: 0.0833
state disagreement: 0.4516
action disagreement: 0.5529
total CM score: 1.4969
goal is complete. CM score: 1.4969
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 10001 steps, reward: 1.429
Reset #2: mass intervention applied (success: True)
episode 2: 10001 steps, reward: -0.546
Reset #3: mass intervention applied (success: True)
episode 3: 10001 steps, reward: 1.037
total data points collected: 50005
average episode length: 10001.0
average episode reward: 0.715
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0762', '7.7262', '7.0334', '6.8535', '8.2588']
Training reward models...
Reward model losses: ['0.0051', '0.2308', '0.0083', '0.3316', '0.6711']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3657', '1.1823', '1.1603', '1.2412', '1.4731']
Training action VAE models...
Action VAE losses: ['1.4042', '1.3537', '1.3243', '1.3948', '1.2720']
CM score components:
transition disagreement: 0.4192
reward disagreement: 0.2096
state disagreement: 0.5235
action disagreement: 0.5413
total CM score: 1.6937
mass is complete. CM score: 1.6937
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 10001 steps, reward: -0.063
Reset #2: friction intervention applied (success: True)
episode 2: 10001 steps, reward: -0.063
Reset #3: friction intervention applied (success: True)
episode 3: 10001 steps, reward: -0.063
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.063
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9109', '7.3003', '6.8221', '7.6795', '7.0559']
Training reward models...
Reward model losses: ['0.0672', '1.2028', '0.4613', '0.0140', '0.2035']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9512', '0.9509', '0.9266', '0.9464', '0.8277']
Training action VAE models...
Action VAE losses: ['1.0949', '1.3281', '1.2349', '1.0836', '1.2974']
CM score components:
transition disagreement: 0.4164
reward disagreement: 0.4416
state disagreement: 0.4678
action disagreement: 0.5503
total CM score: 1.8761
friction is complete. CM score: 1.8761
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 10001 steps, reward: -0.063
Reset #2: visual intervention applied (success: True)
episode 2: 10001 steps, reward: -0.063
Reset #3: visual intervention applied (success: True)
episode 3: 10001 steps, reward: -0.063
total data points collected: 50005
average episode length: 10001.0
average episode reward: -0.063
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([50005, 56]), actions: torch.Size([50005, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1325', '7.7663', '7.0783', '7.1774', '7.4593']
Training reward models...
Reward model losses: ['0.0504', '0.3432', '0.0841', '0.0144', '0.0331']
Training state VAE models...
Training state VAE models...
State VAE losses: ['0.9373', '0.8941', '0.9194', '1.0645', '0.9258']
Training action VAE models...
Action VAE losses: ['1.4557', '1.1938', '1.3221', '1.3584', '1.4580']
CM score components:
transition disagreement: 0.4398
reward disagreement: 0.1551
state disagreement: 0.4611
action disagreement: 0.5308
total CM score: 1.5869
visual is complete. CM score: 1.5869
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 13 steps, reward: 0.733
Reset #2: pose intervention applied (success: True)
episode 2: 10001 steps, reward: 0.414
Reset #3: pose intervention applied (success: True)
episode 3: 10001 steps, reward: 0.943
total data points collected: 30047
average episode length: 6009.4
average episode reward: 0.761
termination reasons: ['success', 'max_length', 'max_length', 'max_length', 'success']
success rate: 2/5
tensor shapes - states: torch.Size([30047, 56]), actions: torch.Size([30047, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7996', '7.6604', '7.6633', '6.5987', '7.4474']
Training reward models...
Reward model losses: ['2.8160', '0.7792', '0.1184', '0.0065', '0.0656']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2670', '1.0664', '1.0845', '1.0927', '1.1535']
Training action VAE models...
Action VAE losses: ['1.2676', '1.4032', '1.3021', '1.3638', '1.1652']
CM score components:
transition disagreement: 0.4463
reward disagreement: 0.6653
state disagreement: 0.5124
action disagreement: 0.5571
total CM score: 2.1811
pose is complete. CM score: 2.1811
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 10001 steps, reward: 2.524
Reset #2: random intervention applied (success: True)
