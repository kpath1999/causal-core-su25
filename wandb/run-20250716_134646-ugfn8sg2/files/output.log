
==================================================
starting meta-rl teacher-student training
   task: pushing
   meta-episodes: 10
   student training steps: 10000
==================================================
INFO:root:Loaded student PPO from ppo_pushing_sb3/final_model.zip

==================================================
initial evaluation
==================================================

evaluating student performance (5 episodes)...
episode 1: length=27, reward=-0.773, success=True
episode 2: length=27, reward=-0.773, success=True
episode 3: length=27, reward=-0.773, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.773
average episode length: 27.0
initial student success rate: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 3.078
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 0.039
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.888
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.207
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
meta_teacher_student.py:227: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  states = torch.tensor([d[0] for d in data], dtype=torch.float32).to(device)
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4553', '7.8204', '7.3470', '7.0077', '7.6039']
Training reward models...
Reward model losses: ['0.3503', '0.1899', '0.0223', '0.9518', '0.0046']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4623', '1.2807', '1.2986', '1.2467', '1.3687']
Training action VAE models...
Action VAE losses: ['0.9379', '0.9613', '0.9328', '0.9110', '0.8237']
CM score components:
transition disagreement: 0.3879
reward disagreement: 0.3452
state disagreement: 0.4949
action disagreement: 0.5153
total CM score: 1.7433
goal is complete. CM score: 1.7433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.514
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.913
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.497
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.392
termination reasons: ['success', 'success', 'success', 'success', 'max_length']
success rate: 4/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2346', '6.9808', '7.6331', '7.4808', '7.8795']
Training reward models...
Reward model losses: ['0.1229', '0.0236', '0.0047', '0.0676', '0.0269']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6173', '1.5881', '1.5435', '1.7845', '1.4572']
Training action VAE models...
Action VAE losses: ['0.8484', '0.8387', '0.9033', '0.9349', '0.8478']
CM score components:
transition disagreement: 0.4289
reward disagreement: 0.1058
state disagreement: 0.5085
action disagreement: 0.5139
total CM score: 1.5571
mass is complete. CM score: 1.5571
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.469
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.387
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.374
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.033
termination reasons: ['success', 'max_length', 'max_length', 'success', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6817', '7.4381', '7.5190', '7.9081', '6.7578']
Training reward models...
Reward model losses: ['0.0101', '0.1329', '0.5792', '0.0445', '0.0593']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3495', '1.5457', '1.8414', '1.6155', '1.6662']
Training action VAE models...
Action VAE losses: ['0.9919', '0.9318', '1.0576', '0.9212', '0.9704']
CM score components:
transition disagreement: 0.3535
reward disagreement: 0.2075
state disagreement: 0.4994
action disagreement: 0.5360
total CM score: 1.5964
friction is complete. CM score: 1.5964
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.394
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.394
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.394
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.394
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6733', '7.6405', '7.7265', '7.4395', '7.3555']
Training reward models...
Reward model losses: ['0.0281', '0.0091', '0.0754', '0.0428', '0.0034']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.6746', '1.3558', '1.4321', '1.5538']
Training action VAE models...
Action VAE losses: ['0.9533', '0.9017', '0.9212', '0.9613', '0.9143']
CM score components:
transition disagreement: 0.3583
reward disagreement: 0.0487
state disagreement: 0.4757
action disagreement: 0.5367
total CM score: 1.4193
visual is complete. CM score: 1.4193
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.548
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.759
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.746
total data points collected: 2505
average episode length: 501.0
average episode reward: -0.003
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3837', '7.3598', '7.6672', '7.8313', '7.5547']
Training reward models...
Reward model losses: ['0.0174', '0.0555', '0.0182', '0.0147', '0.0228']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7699', '1.7515', '1.6568', '1.7130', '1.9990']
Training action VAE models...
Action VAE losses: ['1.3782', '1.2266', '1.2776', '1.2658', '1.3935']
CM score components:
transition disagreement: 0.4286
reward disagreement: 0.1073
state disagreement: 0.5431
action disagreement: 0.5396
total CM score: 1.6187
pose is complete. CM score: 1.6187
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.268
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.021
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.050
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.212
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3331', '6.8425', '7.3860', '7.5206', '7.6092']
Training reward models...
Reward model losses: ['0.0393', '0.1791', '0.1075', '0.0412', '1.3311']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3205', '1.5355', '1.5504', '1.3586', '1.4045']
Training action VAE models...
Action VAE losses: ['1.3345', '1.3945', '1.3678', '1.3667', '1.4311']
CM score components:
transition disagreement: 0.4240
reward disagreement: 0.3868
state disagreement: 0.5242
action disagreement: 0.5467
total CM score: 1.8817
random is complete. CM score: 1.8817
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 462       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5050368   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 2           |
|    time_elapsed         | 28          |
|    total_timesteps      | 5054464     |
| train/                  |             |
|    approx_kl            | 0.030029923 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.529      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0914     |
|    std                  | 2.66        |
|    value_loss           | 0.0522      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 3           |
|    time_elapsed         | 48          |
|    total_timesteps      | 5058560     |
| train/                  |             |
|    approx_kl            | 0.034628637 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.00025     |
|    loss                 | -0.539      |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.088      |
|    std                  | 2.67        |
|    value_loss           | 0.0245      |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=30, reward=-0.071, success=True
episode 2: length=30, reward=-0.071, success=True
episode 3: length=30, reward=-0.071, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: -0.071
average episode length: 30.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.667
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.987
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.255
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.654
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6616', '8.3757', '7.3218', '7.7552', '8.1955']
Training reward models...
Reward model losses: ['0.0039', '0.2415', '0.0301', '1.4375', '1.7175']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2921', '1.3409', '1.1455', '1.1171', '1.1787']
Training action VAE models...
Action VAE losses: ['0.8644', '0.8392', '0.9063', '0.9003', '0.8912']
CM score components:
transition disagreement: 0.4428
reward disagreement: 0.4744
state disagreement: 0.4850
action disagreement: 0.5218
total CM score: 1.9240
goal is complete. CM score: 1.9240
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.419
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 5.381
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.717
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.910
termination reasons: ['max_length', 'success', 'success', 'max_length', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6777', '7.3139', '7.8172', '7.5691', '6.7929']
Training reward models...
Reward model losses: ['0.5977', '0.4633', '0.0107', '0.1204', '0.2557']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5592', '1.3000', '1.5235', '1.5057', '1.8064']
Training action VAE models...
Action VAE losses: ['0.8598', '0.8510', '0.8082', '0.8861', '0.8890']
CM score components:
transition disagreement: 0.4293
reward disagreement: 0.3214
state disagreement: 0.5049
action disagreement: 0.5000
total CM score: 1.7556
mass is complete. CM score: 1.7556
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.754
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.535
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.490
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.085
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3770', '7.3520', '7.2140', '6.8148', '8.0313']
Training reward models...
Reward model losses: ['0.7387', '0.0319', '0.0344', '0.0375', '0.2270']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8222', '1.6877', '1.4305', '1.8662', '1.5514']
Training action VAE models...
Action VAE losses: ['0.9324', '0.9244', '0.9332', '0.9486', '0.9968']
CM score components:
transition disagreement: 0.4115
reward disagreement: 0.2794
state disagreement: 0.5201
action disagreement: 0.5249
total CM score: 1.7360
friction is complete. CM score: 1.7360
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.482
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.482
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.482
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.482
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6613', '7.5968', '7.6609', '7.6448', '7.2517']
Training reward models...
Reward model losses: ['0.5076', '0.5199', '0.0033', '0.0122', '0.1794']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4844', '1.7714', '1.6763', '1.6355', '1.4203']
Training action VAE models...
Action VAE losses: ['0.8892', '0.9243', '1.0302', '0.9597', '0.9364']
CM score components:
transition disagreement: 0.4080
reward disagreement: 0.1841
state disagreement: 0.5319
action disagreement: 0.5574
total CM score: 1.6814
visual is complete. CM score: 1.6814
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.801
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.316
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.943
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.191
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.9448', '7.0877', '8.2069', '8.0133', '6.6458']
Training reward models...
Reward model losses: ['0.3822', '0.0100', '0.1310', '0.0140', '0.0188']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1908', '2.1702', '1.9373', '1.7970', '1.9431']
Training action VAE models...
Action VAE losses: ['1.3757', '1.3086', '1.2725', '1.4055', '1.3669']
CM score components:
transition disagreement: 0.4310
reward disagreement: 0.0856
state disagreement: 0.5827
action disagreement: 0.5143
total CM score: 1.6136
pose is complete. CM score: 1.6136
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.760
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.612
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: 0.540
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.087
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.6740', '7.5919', '6.8594', '7.9568', '7.8938']
Training reward models...
Reward model losses: ['0.9668', '0.4467', '0.0963', '0.9782', '0.0237']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3650', '1.3890', '1.5778', '1.3587', '1.4321']
Training action VAE models...
Action VAE losses: ['1.4838', '1.5010', '1.4033', '1.3524', '1.4127']
CM score components:
transition disagreement: 0.4184
reward disagreement: 0.5055
state disagreement: 0.5086
action disagreement: 0.5534
total CM score: 1.9858
random is complete. CM score: 1.9858
INFO:root:Meta-Episode 1/10: Teacher chose 'visual', Reward: 0.0000, Student Success: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.667
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 1.987
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.255
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.654
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3026', '7.0852', '6.7911', '7.1700', '7.6244']
Training reward models...
Reward model losses: ['0.2056', '0.1715', '0.2099', '0.0028', '0.0204']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2566', '1.1252', '1.1211', '1.3449', '1.3014']
Training action VAE models...
Action VAE losses: ['0.9671', '0.8902', '0.9159', '0.8809', '0.8870']
CM score components:
transition disagreement: 0.4066
reward disagreement: 0.0675
state disagreement: 0.4693
action disagreement: 0.5421
total CM score: 1.4855
goal is complete. CM score: 1.4855
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.419
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 5.381
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 4.717
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.910
termination reasons: ['max_length', 'success', 'success', 'max_length', 'success']
success rate: 3/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0624', '7.6792', '7.0893', '6.8278', '8.2569']
Training reward models...
Reward model losses: ['0.0028', '0.3337', '0.0029', '0.3234', '0.5612']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4528', '1.5117', '1.6126', '1.6127', '2.1347']
Training action VAE models...
Action VAE losses: ['0.9259', '0.9412', '0.8355', '0.9687', '0.8549']
CM score components:
transition disagreement: 0.4190
reward disagreement: 0.1947
state disagreement: 0.5649
action disagreement: 0.5386
total CM score: 1.7172
mass is complete. CM score: 1.7172
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.754
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.535
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.490
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.085
termination reasons: ['max_length', 'success', 'success', 'max_length', 'max_length']
success rate: 2/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8694', '7.2359', '6.8674', '7.6468', '7.0596']
Training reward models...
Reward model losses: ['0.0615', '1.0626', '0.4661', '0.0168', '0.2187']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6335', '1.5705', '1.6210', '1.6398', '1.6720']
Training action VAE models...
Action VAE losses: ['0.8357', '0.9141', '0.8114', '0.8531', '0.9181']
CM score components:
transition disagreement: 0.4191
reward disagreement: 0.4117
state disagreement: 0.5282
action disagreement: 0.5061
total CM score: 1.8650
friction is complete. CM score: 1.8650
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.482
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.482
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.482
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.482
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1094', '7.7457', '7.1144', '7.1921', '7.4941']
Training reward models...
Reward model losses: ['0.0332', '0.2982', '0.0913', '0.0123', '0.0591']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3200', '1.3582', '1.4603', '1.4436', '1.4568']
Training action VAE models...
Action VAE losses: ['0.8759', '0.8061', '0.8673', '0.9492', '0.9800']
CM score components:
transition disagreement: 0.4333
reward disagreement: 0.1503
state disagreement: 0.4644
action disagreement: 0.5233
total CM score: 1.5714
visual is complete. CM score: 1.5714
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.801
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.316
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.943
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.191
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7714', '7.7406', '7.6878', '6.6214', '7.4146']
Training reward models...
Reward model losses: ['2.8350', '1.0335', '0.1344', '0.0131', '0.0255']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.0565', '1.7016', '1.6650', '1.8089', '1.6980']
Training action VAE models...
Action VAE losses: ['1.2948', '1.4035', '1.3408', '1.3971', '1.2571']
CM score components:
transition disagreement: 0.4407
reward disagreement: 0.7045
state disagreement: 0.5634
action disagreement: 0.5576
total CM score: 2.2662
pose is complete. CM score: 2.2662
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.760
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -3.612
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: 0.540
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.087
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4423', '7.3254', '8.3070', '7.4273', '7.3056']
Training reward models...
Reward model losses: ['0.1041', '0.0445', '0.0373', '0.3019', '0.1524']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4302', '1.4134', '1.4517', '1.4034', '1.4716']
Training action VAE models...
Action VAE losses: ['1.4518', '1.4195', '1.4758', '1.4632', '1.4562']
CM score components:
transition disagreement: 0.3708
reward disagreement: 0.1246
state disagreement: 0.5068
action disagreement: 0.5855
total CM score: 1.5876
random is complete. CM score: 1.5876
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 462       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5062656   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5066752     |
| train/                  |             |
|    approx_kl            | 0.027582042 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.00025     |
|    loss                 | -0.489      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.0441     |
|    std                  | 2.71        |
|    value_loss           | 0.0264      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 2.2542775  |
| time/                   |            |
|    fps                  | 442        |
|    iterations           | 3          |
|    time_elapsed         | 27         |
|    total_timesteps      | 5070848    |
| train/                  |            |
|    approx_kl            | 0.02637476 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.00025    |
|    loss                 | -0.466     |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0383    |
|    std                  | 2.74       |
|    value_loss           | 0.0486     |
----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=5.151, success=False
episode 2: length=501, reward=5.151, success=False
episode 3: length=501, reward=5.151, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 5.151
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.953
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 3.532
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.406
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.562
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1575', '7.1332', '7.7535', '7.8417', '8.8626']
Training reward models...
Reward model losses: ['0.2718', '0.0098', '0.3576', '0.0035', '0.0114']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2420', '1.2308', '1.3385', '1.2447', '1.3309']
Training action VAE models...
Action VAE losses: ['1.0080', '1.0371', '0.8708', '0.9607', '0.9659']
CM score components:
transition disagreement: 0.4489
reward disagreement: 0.1755
state disagreement: 0.4763
action disagreement: 0.5586
total CM score: 1.6593
goal is complete. CM score: 1.6593
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.107
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.569
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.919
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.248
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4277', '7.2739', '7.6477', '8.5208', '7.9367']
Training reward models...
Reward model losses: ['0.0078', '0.1115', '1.7345', '0.1827', '0.2047']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5171', '1.4977', '1.7625', '1.6637', '1.7721']
Training action VAE models...
Action VAE losses: ['0.8924', '0.8866', '0.9222', '0.9042', '0.8626']
CM score components:
transition disagreement: 0.4162
reward disagreement: 0.4383
state disagreement: 0.5494
action disagreement: 0.5200
total CM score: 1.9239
mass is complete. CM score: 1.9239
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 5.133
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.146
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.110
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.134
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1425', '7.6287', '6.9910', '7.6507', '6.4597']
Training reward models...
Reward model losses: ['0.0060', '0.0177', '0.1675', '0.0210', '0.2231']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4710', '1.4751', '1.5824', '1.4650', '1.8828']
Training action VAE models...
Action VAE losses: ['0.9576', '0.9353', '0.8420', '0.8031', '1.0313']
CM score components:
transition disagreement: 0.4319
reward disagreement: 0.1409
state disagreement: 0.5348
action disagreement: 0.5191
total CM score: 1.6267
friction is complete. CM score: 1.6267
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.151
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.151
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.151
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.151
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9025', '7.5823', '7.2747', '6.7914', '7.5596']
Training reward models...
Reward model losses: ['0.0013', '0.0343', '0.3222', '0.2569', '0.1370']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4842', '1.5684', '1.2846', '1.7471', '1.5221']
Training action VAE models...
Action VAE losses: ['1.0330', '0.8935', '0.9212', '0.9511', '0.9047']
CM score components:
transition disagreement: 0.4246
reward disagreement: 0.1538
state disagreement: 0.5214
action disagreement: 0.5536
total CM score: 1.6533
visual is complete. CM score: 1.6533
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.959
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.049
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.572
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.658
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4295', '7.3028', '7.5236', '7.3306', '7.3270']
Training reward models...
Reward model losses: ['0.1916', '0.0327', '0.3649', '0.0180', '0.0091']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7933', '1.8004', '1.6126', '1.9451', '1.6483']
Training action VAE models...
Action VAE losses: ['1.2378', '1.3134', '1.2916', '1.4134', '1.4111']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.1671
state disagreement: 0.5543
action disagreement: 0.5241
total CM score: 1.6658
pose is complete. CM score: 1.6658
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.789
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.952
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.958
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.749
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8063', '7.6506', '8.3930', '7.8315', '7.4310']
Training reward models...
Reward model losses: ['0.1123', '0.1059', '0.3756', '0.0989', '0.2449']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4439', '1.6110', '1.6202', '1.4802', '1.5037']
Training action VAE models...
Action VAE losses: ['1.4915', '1.5022', '1.4088', '1.3802', '1.5233']
CM score components:
transition disagreement: 0.4227
reward disagreement: 0.1459
state disagreement: 0.5267
action disagreement: 0.5728
total CM score: 1.6682
random is complete. CM score: 1.6682
INFO:root:Meta-Episode 2/10: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.953
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 3.532
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.406
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.562
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2808', '7.0780', '6.7764', '7.1723', '7.6112']
Training reward models...
Reward model losses: ['0.1931', '0.1588', '0.1990', '0.0025', '0.0212']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2767', '1.1321', '1.1383', '1.4039', '1.3096']
Training action VAE models...
Action VAE losses: ['1.0128', '0.9419', '0.9667', '0.9520', '0.9323']
CM score components:
transition disagreement: 0.4074
reward disagreement: 0.0685
state disagreement: 0.4716
action disagreement: 0.5456
total CM score: 1.4931
goal is complete. CM score: 1.4931
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 4.107
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 4.569
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.919
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.248
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0631', '7.6717', '7.0751', '6.8136', '8.2686']
Training reward models...
Reward model losses: ['0.0026', '0.2914', '0.0025', '0.3467', '0.5337']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4746', '1.5466', '1.6858', '1.6965', '2.1094']
Training action VAE models...
Action VAE losses: ['0.9686', '0.9386', '0.8467', '0.9861', '0.8713']
CM score components:
transition disagreement: 0.4209
reward disagreement: 0.1694
state disagreement: 0.5687
action disagreement: 0.5395
total CM score: 1.6985
mass is complete. CM score: 1.6985
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 5.133
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.146
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.110
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.134
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8627', '7.2397', '6.8549', '7.6352', '7.0612']
Training reward models...
Reward model losses: ['0.0554', '1.0911', '0.4701', '0.0190', '0.2293']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5789', '1.4933', '1.5976', '1.6128', '1.6168']
Training action VAE models...
Action VAE losses: ['0.8203', '0.8946', '0.7883', '0.8522', '0.9150']
CM score components:
transition disagreement: 0.4169
reward disagreement: 0.4149
state disagreement: 0.5227
action disagreement: 0.5043
total CM score: 1.8588
friction is complete. CM score: 1.8588
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.151
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.151
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.151
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.151
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1041', '7.7314', '7.1341', '7.2077', '7.4920']
Training reward models...
Reward model losses: ['0.0350', '0.3117', '0.0860', '0.0103', '0.0605']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3578', '1.4321', '1.4978', '1.4678', '1.5141']
Training action VAE models...
Action VAE losses: ['0.9078', '0.8052', '0.9102', '0.9873', '1.0021']
CM score components:
transition disagreement: 0.4315
reward disagreement: 0.1529
state disagreement: 0.4695
action disagreement: 0.5238
total CM score: 1.5777
visual is complete. CM score: 1.5777
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.959
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.049
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.572
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.658
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7786', '7.7238', '7.6926', '6.5919', '7.3942']
Training reward models...
Reward model losses: ['2.8517', '1.0677', '0.1256', '0.0124', '0.0270']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.0041', '1.7213', '1.6882', '1.7734', '1.6662']
Training action VAE models...
Action VAE losses: ['1.2502', '1.3955', '1.3535', '1.4002', '1.2677']
CM score components:
transition disagreement: 0.4398
reward disagreement: 0.7119
state disagreement: 0.5648
action disagreement: 0.5514
total CM score: 2.2679
pose is complete. CM score: 2.2679
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 2.789
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.952
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -0.958
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.749
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4976', '7.3226', '8.3247', '7.4277', '7.3256']
Training reward models...
Reward model losses: ['0.1731', '0.1158', '0.1044', '0.3556', '0.2414']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4387', '1.3903', '1.4957', '1.4237', '1.5078']
Training action VAE models...
Action VAE losses: ['1.4990', '1.4003', '1.3925', '1.4777', '1.4988']
CM score components:
transition disagreement: 0.3770
reward disagreement: 0.1176
state disagreement: 0.5098
action disagreement: 0.5781
total CM score: 1.5825
random is complete. CM score: 1.5825
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 461       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5074944   |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 251          |
|    ep_rew_mean          | 2.2542775    |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 2            |
|    time_elapsed         | 18           |
|    total_timesteps      | 5079040      |
| train/                  |              |
|    approx_kl            | 0.0065653515 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -21.9        |
|    explained_variance   | 0.697        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.375       |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.0143      |
|    std                  | 2.77         |
|    value_loss           | 0.208        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5083136     |
| train/                  |             |
|    approx_kl            | 0.006763179 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.823       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.45       |
|    n_updates            | 1275        |
|    policy_gradient_loss | -0.0169     |
|    std                  | 2.78        |
|    value_loss           | 0.0462      |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=3.840, success=False
episode 2: length=501, reward=3.840, success=False
episode 3: length=501, reward=3.840, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 3.840
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 6.035
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.904
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.442
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.289
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1662', '7.1487', '7.7578', '7.8414', '8.8587']
Training reward models...
Reward model losses: ['0.2560', '0.0102', '0.3374', '0.0027', '0.0105']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2707', '1.2483', '1.3872', '1.2663', '1.3495']
Training action VAE models...
Action VAE losses: ['1.0237', '1.0594', '0.8942', '0.9969', '0.9930']
CM score components:
transition disagreement: 0.4491
reward disagreement: 0.1694
state disagreement: 0.4793
action disagreement: 0.5597
total CM score: 1.6576
goal is complete. CM score: 1.6576
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 2.576
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.933
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.626
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.002
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4222', '7.2760', '7.6375', '8.5141', '7.9437']
Training reward models...
Reward model losses: ['0.0084', '0.1186', '1.7435', '0.1884', '0.1980']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5389', '1.5110', '1.7708', '1.6794', '1.7569']
Training action VAE models...
Action VAE losses: ['0.9394', '0.9301', '0.9404', '0.9381', '0.9046']
CM score components:
transition disagreement: 0.4185
reward disagreement: 0.4444
state disagreement: 0.5512
action disagreement: 0.5215
total CM score: 1.9357
mass is complete. CM score: 1.9357
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.881
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 4.117
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.882
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.936
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1510', '7.6243', '6.9934', '7.6514', '6.4604']
Training reward models...
Reward model losses: ['0.0055', '0.0185', '0.1691', '0.0216', '0.2257']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4714', '1.4637', '1.5955', '1.4535', '1.9040']
Training action VAE models...
Action VAE losses: ['0.9779', '0.9580', '0.8856', '0.8282', '1.0874']
CM score components:
transition disagreement: 0.4327
reward disagreement: 0.1439
state disagreement: 0.5357
action disagreement: 0.5210
total CM score: 1.6332
friction is complete. CM score: 1.6332
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.840
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.840
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.840
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.840
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9033', '7.5816', '7.2719', '6.7934', '7.5575']
Training reward models...
Reward model losses: ['0.0012', '0.0340', '0.3116', '0.2552', '0.1419']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4724', '1.5609', '1.2691', '1.7540', '1.5148']
Training action VAE models...
Action VAE losses: ['1.1089', '0.9479', '0.9882', '1.0008', '0.9539']
CM score components:
transition disagreement: 0.4241
reward disagreement: 0.1523
state disagreement: 0.5205
action disagreement: 0.5565
total CM score: 1.6535
visual is complete. CM score: 1.6535
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.985
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.695
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -5.256
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.738
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4436', '7.3095', '7.5270', '7.3403', '7.3254']
Training reward models...
Reward model losses: ['0.1952', '0.0379', '0.3715', '0.0177', '0.0125']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7940', '1.8500', '1.6294', '1.9218', '1.6888']
Training action VAE models...
Action VAE losses: ['1.2042', '1.2985', '1.2840', '1.4091', '1.3900']
CM score components:
transition disagreement: 0.4210
reward disagreement: 0.1727
state disagreement: 0.5508
action disagreement: 0.5227
total CM score: 1.6672
pose is complete. CM score: 1.6672
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 4.873
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.358
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.101
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.744
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.7830', '7.6147', '8.3657', '7.8118', '7.4281']
Training reward models...
Reward model losses: ['0.0397', '0.0332', '0.2927', '0.0302', '0.1563']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3518', '1.5718', '1.5682', '1.3798', '1.3696']
Training action VAE models...
Action VAE losses: ['1.4647', '1.5286', '1.3728', '1.3411', '1.4862']
CM score components:
transition disagreement: 0.4183
reward disagreement: 0.1377
state disagreement: 0.5153
action disagreement: 0.5709
total CM score: 1.6422
random is complete. CM score: 1.6422
INFO:root:Meta-Episode 3/10: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 6.035
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.904
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 2.442
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.289
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.2842', '7.0792', '6.7851', '7.1708', '7.6162']
Training reward models...
Reward model losses: ['0.2048', '0.1688', '0.2112', '0.0031', '0.0199']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2922', '1.1623', '1.1631', '1.4138', '1.3272']
Training action VAE models...
Action VAE losses: ['1.0565', '0.9672', '1.0003', '0.9760', '0.9584']
CM score components:
transition disagreement: 0.4080
reward disagreement: 0.0674
state disagreement: 0.4721
action disagreement: 0.5450
total CM score: 1.4925
goal is complete. CM score: 1.4925
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 2.576
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.933
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 3.626
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.002
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0613', '7.6772', '7.0708', '6.8196', '8.2607']
Training reward models...
Reward model losses: ['0.0028', '0.3056', '0.0025', '0.3502', '0.5497']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4820', '1.5591', '1.6468', '1.6743', '2.0966']
Training action VAE models...
Action VAE losses: ['1.0098', '1.0061', '0.9000', '1.0396', '0.9339']
CM score components:
transition disagreement: 0.4199
reward disagreement: 0.1821
state disagreement: 0.5681
action disagreement: 0.5419
total CM score: 1.7120
mass is complete. CM score: 1.7120
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.881
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 4.117
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.882
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.936
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8632', '7.2456', '6.8587', '7.6333', '7.0578']
Training reward models...
Reward model losses: ['0.0571', '1.0938', '0.4735', '0.0190', '0.2332']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5778', '1.5009', '1.5919', '1.6076', '1.6307']
Training action VAE models...
Action VAE losses: ['0.8551', '0.9189', '0.8209', '0.8865', '0.9576']
CM score components:
transition disagreement: 0.4167
reward disagreement: 0.4185
state disagreement: 0.5229
action disagreement: 0.5071
total CM score: 1.8652
friction is complete. CM score: 1.8652
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.840
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.840
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.840
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.840
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1041', '7.7346', '7.1291', '7.2020', '7.4925']
Training reward models...
Reward model losses: ['0.0367', '0.3165', '0.0889', '0.0108', '0.0604']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3682', '1.4220', '1.5016', '1.4670', '1.5126']
Training action VAE models...
Action VAE losses: ['0.9751', '0.8732', '0.9487', '1.0472', '1.0598']
CM score components:
transition disagreement: 0.4308
reward disagreement: 0.1548
state disagreement: 0.4700
action disagreement: 0.5258
total CM score: 1.5814
visual is complete. CM score: 1.5814
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.985
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 0.695
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -5.256
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.738
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7847', '7.7284', '7.7131', '6.6127', '7.4086']
Training reward models...
Reward model losses: ['2.9277', '1.1032', '0.1525', '0.0152', '0.0218']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.1238', '1.7453', '1.7054', '1.8047', '1.6564']
Training action VAE models...
Action VAE losses: ['1.2824', '1.3845', '1.3300', '1.3594', '1.2394']
CM score components:
transition disagreement: 0.4397
reward disagreement: 0.7275
state disagreement: 0.5667
action disagreement: 0.5511
total CM score: 2.2849
pose is complete. CM score: 2.2849
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 4.873
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.358
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.101
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.744
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4643', '7.3199', '8.3023', '7.4204', '7.3063']
Training reward models...
Reward model losses: ['0.1007', '0.0450', '0.0321', '0.3180', '0.1798']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4188', '1.3359', '1.4004', '1.3748', '1.4572']
Training action VAE models...
Action VAE losses: ['1.4832', '1.3740', '1.3976', '1.4880', '1.4504']
CM score components:
transition disagreement: 0.3754
reward disagreement: 0.1266
state disagreement: 0.5066
action disagreement: 0.5786
total CM score: 1.5872
random is complete. CM score: 1.5872
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 463       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5087232   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5091328     |
| train/                  |             |
|    approx_kl            | 0.006463511 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.82        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.439      |
|    n_updates            | 1305        |
|    policy_gradient_loss | -0.0134     |
|    std                  | 2.78        |
|    value_loss           | 0.0558      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 441         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5095424     |
| train/                  |             |
|    approx_kl            | 0.005358737 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.902       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.441      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0114     |
|    std                  | 2.78        |
|    value_loss           | 0.0483      |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=37, reward=0.061, success=True
episode 2: length=37, reward=0.061, success=True
episode 3: length=37, reward=0.061, success=True
performance summary:
success rate: 1.000 (5/5)
average reward: 0.061
average episode length: 37.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.279
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.180
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.848
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.955
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1625', '7.1570', '7.7632', '7.8304', '8.8685']
Training reward models...
Reward model losses: ['0.2492', '0.0099', '0.3297', '0.0024', '0.0105']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2215', '1.2033', '1.3309', '1.2211', '1.2872']
Training action VAE models...
Action VAE losses: ['0.9693', '1.0230', '0.8646', '0.9504', '0.9547']
CM score components:
transition disagreement: 0.4500
reward disagreement: 0.1649
state disagreement: 0.4724
action disagreement: 0.5561
total CM score: 1.6433
goal is complete. CM score: 1.6433
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.675
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.007
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.954
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.373
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4083', '7.2717', '7.6376', '8.5255', '7.9486']
Training reward models...
Reward model losses: ['0.0077', '0.1206', '1.7447', '0.1835', '0.2028']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5490', '1.4985', '1.7625', '1.6745', '1.7347']
Training action VAE models...
Action VAE losses: ['0.9215', '0.8961', '0.8870', '0.8918', '0.8588']
CM score components:
transition disagreement: 0.4188
reward disagreement: 0.4460
state disagreement: 0.5530
action disagreement: 0.5188
total CM score: 1.9367
mass is complete. CM score: 1.9367
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.314
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.290
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.054
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.751
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1561', '7.6034', '7.0044', '7.6493', '6.4604']
Training reward models...
Reward model losses: ['0.0058', '0.0175', '0.1651', '0.0192', '0.2191']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4585', '1.4718', '1.5762', '1.4386', '1.8795']
Training action VAE models...
Action VAE losses: ['0.9256', '0.9179', '0.8455', '0.8024', '1.0225']
CM score components:
transition disagreement: 0.4320
reward disagreement: 0.1379
state disagreement: 0.5324
action disagreement: 0.5193
total CM score: 1.6216
friction is complete. CM score: 1.6216
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.572
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.572
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.572
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.572
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9083', '7.6082', '7.2714', '6.8035', '7.5459']
Training reward models...
Reward model losses: ['0.0013', '0.0406', '0.3100', '0.2348', '0.1530']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4377', '1.5053', '1.2079', '1.7120', '1.4593']
Training action VAE models...
Action VAE losses: ['0.9952', '0.8310', '0.8645', '0.8878', '0.8578']
CM score components:
transition disagreement: 0.4266
reward disagreement: 0.1475
state disagreement: 0.5150
action disagreement: 0.5527
total CM score: 1.6419
visual is complete. CM score: 1.6419
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.799
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.178
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.576
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.599
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4420', '7.2948', '7.5125', '7.3146', '7.3526']
Training reward models...
Reward model losses: ['0.1861', '0.0310', '0.3591', '0.0165', '0.0089']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.6875', '1.7112', '1.5287', '1.8128', '1.5781']
Training action VAE models...
Action VAE losses: ['1.0636', '1.1306', '1.1184', '1.2375', '1.2129']
CM score components:
transition disagreement: 0.4186
reward disagreement: 0.1631
state disagreement: 0.5464
action disagreement: 0.5173
total CM score: 1.6454
pose is complete. CM score: 1.6454
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 5.307
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.397
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.326
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.884
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8052', '7.6202', '8.3855', '7.8383', '7.4468']
Training reward models...
Reward model losses: ['0.0661', '0.0590', '0.3226', '0.0551', '0.1795']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3667', '1.5943', '1.5475', '1.3992', '1.3588']
Training action VAE models...
Action VAE losses: ['1.4305', '1.5425', '1.3712', '1.3666', '1.5274']
CM score components:
transition disagreement: 0.4180
reward disagreement: 0.1400
state disagreement: 0.5159
action disagreement: 0.5740
total CM score: 1.6479
random is complete. CM score: 1.6479
INFO:root:Meta-Episode 4/10: Teacher chose 'visual', Reward: 1.0000, Student Success: 1.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 5.279
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.180
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.848
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.955
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3060', '7.0841', '6.8007', '7.1753', '7.6198']
Training reward models...
Reward model losses: ['0.2081', '0.1769', '0.2170', '0.0032', '0.0192']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2512', '1.1259', '1.1342', '1.3697', '1.3027']
Training action VAE models...
Action VAE losses: ['1.0031', '0.9340', '0.9574', '0.9246', '0.9110']
CM score components:
transition disagreement: 0.4091
reward disagreement: 0.0649
state disagreement: 0.4692
action disagreement: 0.5431
total CM score: 1.4862
goal is complete. CM score: 1.4862
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.675
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.007
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.954
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.373
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0581', '7.6867', '7.0791', '6.8196', '8.2731']
Training reward models...
Reward model losses: ['0.0028', '0.3122', '0.0024', '0.3495', '0.5361']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4708', '1.5658', '1.6416', '1.6626', '2.1320']
Training action VAE models...
Action VAE losses: ['0.9572', '0.9772', '0.8627', '1.0133', '0.9055']
CM score components:
transition disagreement: 0.4195
reward disagreement: 0.1811
state disagreement: 0.5687
action disagreement: 0.5390
total CM score: 1.7083
mass is complete. CM score: 1.7083
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 4.314
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 5.290
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 5.054
total data points collected: 2505
average episode length: 501.0
average episode reward: 4.751
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8579', '7.2460', '6.8694', '7.6289', '7.0584']
Training reward models...
Reward model losses: ['0.0598', '1.0726', '0.4787', '0.0187', '0.2299']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5432', '1.5115', '1.5809', '1.5838', '1.6221']
Training action VAE models...
Action VAE losses: ['0.8223', '0.8751', '0.7749', '0.8415', '0.9063']
CM score components:
transition disagreement: 0.4191
reward disagreement: 0.4158
state disagreement: 0.5201
action disagreement: 0.5052
total CM score: 1.8603
friction is complete. CM score: 1.8603
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 5.572
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 5.572
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 5.572
total data points collected: 2505
average episode length: 501.0
average episode reward: 5.572
termination reasons: ['success', 'success', 'success', 'success', 'success']
success rate: 5/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1024', '7.7369', '7.1356', '7.1764', '7.4977']
Training reward models...
Reward model losses: ['0.0437', '0.3049', '0.0961', '0.0096', '0.0643']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3140', '1.3470', '1.4440', '1.4119', '1.4829']
Training action VAE models...
Action VAE losses: ['0.8783', '0.7797', '0.8654', '0.9365', '0.9669']
CM score components:
transition disagreement: 0.4307
reward disagreement: 0.1499
state disagreement: 0.4641
action disagreement: 0.5243
total CM score: 1.5690
visual is complete. CM score: 1.5690
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.799
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.178
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.576
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.599
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7870', '7.7440', '7.7007', '6.5750', '7.4223']
Training reward models...
Reward model losses: ['2.7959', '1.0604', '0.1221', '0.0125', '0.0190']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.8438', '1.6275', '1.5673', '1.6531', '1.5993']
Training action VAE models...
Action VAE losses: ['1.0471', '1.1791', '1.1311', '1.2199', '1.0495']
CM score components:
transition disagreement: 0.4409
reward disagreement: 0.7007
state disagreement: 0.5528
action disagreement: 0.5411
total CM score: 2.2355
pose is complete. CM score: 2.2355
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: 5.307
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -0.397
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.326
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.884
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4779', '7.3144', '8.3238', '7.4301', '7.3087']
Training reward models...
Reward model losses: ['0.1210', '0.0651', '0.0561', '0.3200', '0.1933']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3725', '1.3286', '1.4027', '1.3945', '1.4567']
Training action VAE models...
Action VAE losses: ['1.4717', '1.4255', '1.3882', '1.4575', '1.4971']
CM score components:
transition disagreement: 0.3764
reward disagreement: 0.1185
state disagreement: 0.5049
action disagreement: 0.5756
total CM score: 1.5754
random is complete. CM score: 1.5754
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 471       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5099520   |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 453         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 5103616     |
| train/                  |             |
|    approx_kl            | 0.005439507 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.853       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.437      |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0108     |
|    std                  | 2.78        |
|    value_loss           | 0.0461      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 251          |
|    ep_rew_mean          | 2.2542775    |
| time/                   |              |
|    fps                  | 449          |
|    iterations           | 3            |
|    time_elapsed         | 27           |
|    total_timesteps      | 5107712      |
| train/                  |              |
|    approx_kl            | 0.0045423303 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -22          |
|    explained_variance   | 0.874        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.437       |
|    n_updates            | 1365         |
|    policy_gradient_loss | -0.00906     |
|    std                  | 2.79         |
|    value_loss           | 0.0366       |
------------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=3.973, success=False
episode 2: length=501, reward=3.973, success=False
episode 3: length=501, reward=3.973, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 3.973
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 4.504
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.069
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.272
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.431
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1652', '7.1680', '7.7653', '7.8277', '8.8727']
Training reward models...
Reward model losses: ['0.2333', '0.0104', '0.3158', '0.0021', '0.0100']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2257', '1.2283', '1.3479', '1.2405', '1.3044']
Training action VAE models...
Action VAE losses: ['0.9797', '1.0397', '0.8905', '0.9665', '0.9725']
CM score components:
transition disagreement: 0.4508
reward disagreement: 0.1585
state disagreement: 0.4742
action disagreement: 0.5551
total CM score: 1.6386
goal is complete. CM score: 1.6386
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 2.627
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.195
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.183
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.551
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4068', '7.2722', '7.6316', '8.5372', '7.9546']
Training reward models...
Reward model losses: ['0.0065', '0.1252', '1.7564', '0.1813', '0.2111']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5445', '1.4915', '1.7435', '1.6611', '1.7321']
Training action VAE models...
Action VAE losses: ['0.9444', '0.9205', '0.9032', '0.9067', '0.8741']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.4507
state disagreement: 0.5496
action disagreement: 0.5196
total CM score: 1.9401
mass is complete. CM score: 1.9401
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.281
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 2.973
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.899
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.191
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1567', '7.6039', '7.0097', '7.6486', '6.4587']
Training reward models...
Reward model losses: ['0.0066', '0.0182', '0.1677', '0.0198', '0.2199']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4799', '1.4802', '1.6075', '1.4581', '1.8895']
Training action VAE models...
Action VAE losses: ['0.9673', '0.9543', '0.8823', '0.8433', '1.0415']
CM score components:
transition disagreement: 0.4320
reward disagreement: 0.1415
state disagreement: 0.5326
action disagreement: 0.5207
total CM score: 1.6268
friction is complete. CM score: 1.6268
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.973
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.973
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.973
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.973
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9085', '7.6088', '7.2693', '6.8098', '7.5431']
Training reward models...
Reward model losses: ['0.0014', '0.0347', '0.3091', '0.2451', '0.1464']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4576', '1.5220', '1.2405', '1.7454', '1.5116']
Training action VAE models...
Action VAE losses: ['1.0548', '0.8909', '0.9429', '0.9535', '0.9110']
CM score components:
transition disagreement: 0.4267
reward disagreement: 0.1508
state disagreement: 0.5184
action disagreement: 0.5535
total CM score: 1.6495
visual is complete. CM score: 1.6495
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.718
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.159
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.930
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.236
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4400', '7.2766', '7.5213', '7.3330', '7.3428']
Training reward models...
Reward model losses: ['0.1993', '0.0311', '0.3738', '0.0187', '0.0093']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7116', '1.7087', '1.5558', '1.8787', '1.5798']
Training action VAE models...
Action VAE losses: ['1.1911', '1.3038', '1.2596', '1.4062', '1.3747']
CM score components:
transition disagreement: 0.4205
reward disagreement: 0.1715
state disagreement: 0.5472
action disagreement: 0.5254
total CM score: 1.6645
pose is complete. CM score: 1.6645
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.595
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.124
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.364
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.036
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8115', '7.6265', '8.3909', '7.8409', '7.4392']
Training reward models...
Reward model losses: ['0.0436', '0.0335', '0.2844', '0.0293', '0.1528']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3543', '1.6206', '1.5311', '1.4124', '1.3938']
Training action VAE models...
Action VAE losses: ['1.4286', '1.5137', '1.3817', '1.3980', '1.5520']
CM score components:
transition disagreement: 0.4185
reward disagreement: 0.1331
state disagreement: 0.5191
action disagreement: 0.5786
total CM score: 1.6493
random is complete. CM score: 1.6493
INFO:root:Meta-Episode 5/10: Teacher chose 'visual', Reward: -1.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 4.504
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.069
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.272
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.431
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3173', '7.0894', '6.8081', '7.1734', '7.6219']
Training reward models...
Reward model losses: ['0.2181', '0.1886', '0.2291', '0.0035', '0.0183']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2722', '1.1530', '1.1446', '1.3922', '1.3178']
Training action VAE models...
Action VAE losses: ['1.0035', '0.9424', '0.9777', '0.9434', '0.9240']
CM score components:
transition disagreement: 0.4102
reward disagreement: 0.0636
state disagreement: 0.4719
action disagreement: 0.5432
total CM score: 1.4888
goal is complete. CM score: 1.4888
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 2.627
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 2.195
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.183
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.551
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0543', '7.6964', '7.0763', '6.8233', '8.2795']
Training reward models...
Reward model losses: ['0.0030', '0.3221', '0.0024', '0.3509', '0.5342']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4659', '1.5576', '1.6452', '1.6655', '2.1095']
Training action VAE models...
Action VAE losses: ['0.9729', '0.9935', '0.8760', '1.0351', '0.9381']
CM score components:
transition disagreement: 0.4189
reward disagreement: 0.1861
state disagreement: 0.5673
action disagreement: 0.5413
total CM score: 1.7137
mass is complete. CM score: 1.7137
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.281
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 2.973
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 3.899
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.191
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8629', '7.2562', '6.8678', '7.6275', '7.0539']
Training reward models...
Reward model losses: ['0.0627', '1.0821', '0.4735', '0.0191', '0.2367']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5831', '1.5527', '1.6030', '1.6150', '1.6489']
Training action VAE models...
Action VAE losses: ['0.8499', '0.9028', '0.8049', '0.8804', '0.9438']
CM score components:
transition disagreement: 0.4198
reward disagreement: 0.4187
state disagreement: 0.5241
action disagreement: 0.5062
total CM score: 1.8688
friction is complete. CM score: 1.8688
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 3.973
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 3.973
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 3.973
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.973
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1112', '7.7383', '7.1337', '7.1921', '7.4962']
Training reward models...
Reward model losses: ['0.0428', '0.3091', '0.0934', '0.0098', '0.0662']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3545', '1.3889', '1.4875', '1.4535', '1.4852']
Training action VAE models...
Action VAE losses: ['0.9415', '0.8384', '0.9183', '1.0000', '1.0086']
CM score components:
transition disagreement: 0.4308
reward disagreement: 0.1523
state disagreement: 0.4683
action disagreement: 0.5257
total CM score: 1.5771
visual is complete. CM score: 1.5771
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.718
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.159
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -1.930
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.236
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7804', '7.7421', '7.6964', '6.5828', '7.4119']
Training reward models...
Reward model losses: ['2.8494', '1.0818', '0.1281', '0.0131', '0.0238']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.2018', '1.6039', '1.5898', '1.6824', '1.5615']
Training action VAE models...
Action VAE losses: ['1.2146', '1.3514', '1.3044', '1.3784', '1.1947']
CM score components:
transition disagreement: 0.4409
reward disagreement: 0.7118
state disagreement: 0.5529
action disagreement: 0.5555
total CM score: 2.2611
pose is complete. CM score: 2.2611
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.595
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.124
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.364
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.036
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5061', '7.3039', '8.3205', '7.4455', '7.3100']
Training reward models...
Reward model losses: ['0.1018', '0.0335', '0.0307', '0.2667', '0.1709']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3882', '1.3529', '1.4150', '1.4082', '1.4751']
Training action VAE models...
Action VAE losses: ['1.4829', '1.3874', '1.4166', '1.5417', '1.5244']
CM score components:
transition disagreement: 0.3814
reward disagreement: 0.1013
state disagreement: 0.5080
action disagreement: 0.5825
total CM score: 1.5733
random is complete. CM score: 1.5733
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 469       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5111808   |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 251          |
|    ep_rew_mean          | 2.2542775    |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 2            |
|    time_elapsed         | 18           |
|    total_timesteps      | 5115904      |
| train/                  |              |
|    approx_kl            | 0.0047915997 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -22          |
|    explained_variance   | 0.873        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.436       |
|    n_updates            | 1395         |
|    policy_gradient_loss | -0.00858     |
|    std                  | 2.79         |
|    value_loss           | 0.0424       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 2.2542775   |
| time/                   |             |
|    fps                  | 449         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.004472932 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.861       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.434      |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00792    |
|    std                  | 2.79        |
|    value_loss           | 0.0415      |
-----------------------------------------

evaluating student performance (5 episodes)...
episode 1: length=501, reward=2.763, success=False
episode 2: length=501, reward=2.763, success=False
episode 3: length=501, reward=2.763, success=False
performance summary:
success rate: 0.000 (0/5)
average reward: 2.763
average episode length: 501.0
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 4.459
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.684
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.385
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.730
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.1593', '7.1724', '7.7759', '7.8199', '8.8607']
Training reward models...
Reward model losses: ['0.2244', '0.0110', '0.3056', '0.0022', '0.0090']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2409', '1.1936', '1.3365', '1.2327', '1.2870']
Training action VAE models...
Action VAE losses: ['0.9231', '1.0094', '0.8697', '0.9565', '0.9528']
CM score components:
transition disagreement: 0.4506
reward disagreement: 0.1528
state disagreement: 0.4726
action disagreement: 0.5500
total CM score: 1.6260
goal is complete. CM score: 1.6260
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.067
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 1.948
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.965
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.806
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4006', '7.2519', '7.6244', '8.5425', '7.9530']
Training reward models...
Reward model losses: ['0.0054', '0.1255', '1.7263', '0.1694', '0.1987']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5213', '1.4762', '1.7402', '1.6351', '1.6970']
Training action VAE models...
Action VAE losses: ['0.9143', '0.9029', '0.8632', '0.8660', '0.8332']
CM score components:
transition disagreement: 0.4197
reward disagreement: 0.4470
state disagreement: 0.5473
action disagreement: 0.5179
total CM score: 1.9319
mass is complete. CM score: 1.9319
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.260
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.081
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 2.387
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.028
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['8.1663', '7.5775', '7.0062', '7.6397', '6.4632']
Training reward models...
Reward model losses: ['0.0091', '0.0193', '0.1651', '0.0180', '0.2175']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4369', '1.4473', '1.5773', '1.4075', '1.8905']
Training action VAE models...
Action VAE losses: ['0.9429', '0.9353', '0.8705', '0.8212', '1.0322']
CM score components:
transition disagreement: 0.4311
reward disagreement: 0.1449
state disagreement: 0.5336
action disagreement: 0.5204
total CM score: 1.6300
friction is complete. CM score: 1.6300
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 2.763
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 2.763
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 2.763
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.763
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.9168', '7.6060', '7.2693', '6.8182', '7.5403']
Training reward models...
Reward model losses: ['0.0021', '0.0395', '0.2810', '0.2280', '0.1830']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4021', '1.4745', '1.1795', '1.6721', '1.4144']
Training action VAE models...
Action VAE losses: ['1.0492', '0.8860', '0.9019', '0.9141', '0.8780']
CM score components:
transition disagreement: 0.4260
reward disagreement: 0.1458
state disagreement: 0.5125
action disagreement: 0.5564
total CM score: 1.6408
visual is complete. CM score: 1.6408
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.740
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.195
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.089
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.402
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.4406', '7.2698', '7.5201', '7.3379', '7.3365']
Training reward models...
Reward model losses: ['0.2016', '0.0303', '0.3885', '0.0193', '0.0094']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.7557', '1.7742', '1.6230', '1.8679', '1.5972']
Training action VAE models...
Action VAE losses: ['1.2194', '1.3244', '1.2855', '1.4219', '1.4116']
CM score components:
transition disagreement: 0.4203
reward disagreement: 0.1782
state disagreement: 0.5493
action disagreement: 0.5276
total CM score: 1.6755
pose is complete. CM score: 1.6755
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.790
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.197
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.369
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.833
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.8264', '7.6370', '8.4091', '7.8661', '7.4282']
Training reward models...
Reward model losses: ['0.0309', '0.0171', '0.2639', '0.0142', '0.1409']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3567', '1.5946', '1.5124', '1.3565', '1.3628']
Training action VAE models...
Action VAE losses: ['1.3784', '1.4251', '1.3043', '1.3095', '1.4446']
CM score components:
transition disagreement: 0.4236
reward disagreement: 0.1280
state disagreement: 0.5100
action disagreement: 0.5707
total CM score: 1.6323
random is complete. CM score: 1.6323
INFO:root:Meta-Episode 6/10: Teacher chose 'visual', Reward: 0.0000, Student Success: 0.000
Computing teacher state (CM scores for all interventions)...
Processing intervention 1/6: goal
IntervenedCausalWorld created with goal intervention
Evaluating CM score for goal intervention...
Reset #1: goal intervention applied (success: True)
episode 1: 501 steps, reward: 4.459
Reset #2: goal intervention applied (success: True)
episode 2: 501 steps, reward: 2.684
Reset #3: goal intervention applied (success: True)
episode 3: 501 steps, reward: 1.385
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.730
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.3248', '7.0841', '6.8119', '7.1755', '7.6252']
Training reward models...
Reward model losses: ['0.2261', '0.1997', '0.2382', '0.0035', '0.0173']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2274', '1.1395', '1.1559', '1.3375', '1.3178']
Training action VAE models...
Action VAE losses: ['1.0223', '0.9266', '0.9677', '0.9097', '0.9177']
CM score components:
transition disagreement: 0.4096
reward disagreement: 0.0650
state disagreement: 0.4688
action disagreement: 0.5423
total CM score: 1.4857
goal is complete. CM score: 1.4857
Processing intervention 2/6: mass
IntervenedCausalWorld created with mass intervention
Evaluating CM score for mass intervention...
Reset #1: mass intervention applied (success: True)
episode 1: 501 steps, reward: 3.067
Reset #2: mass intervention applied (success: True)
episode 2: 501 steps, reward: 1.948
Reset #3: mass intervention applied (success: True)
episode 3: 501 steps, reward: 2.965
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.806
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0438', '7.7119', '7.0874', '6.8240', '8.2643']
Training reward models...
Reward model losses: ['0.0026', '0.3340', '0.0030', '0.3314', '0.5377']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.4416', '1.5119', '1.5580', '1.6080', '2.0779']
Training action VAE models...
Action VAE losses: ['0.9317', '0.9698', '0.8446', '0.9824', '0.9097']
CM score components:
transition disagreement: 0.4175
reward disagreement: 0.1875
state disagreement: 0.5612
action disagreement: 0.5412
total CM score: 1.7075
mass is complete. CM score: 1.7075
Processing intervention 3/6: friction
IntervenedCausalWorld created with friction intervention
Evaluating CM score for friction intervention...
Reset #1: friction intervention applied (success: True)
episode 1: 501 steps, reward: 3.260
Reset #2: friction intervention applied (success: True)
episode 2: 501 steps, reward: 3.081
Reset #3: friction intervention applied (success: True)
episode 3: 501 steps, reward: 2.387
total data points collected: 2505
average episode length: 501.0
average episode reward: 3.028
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.8454', '7.2377', '6.8897', '7.6352', '7.0371']
Training reward models...
Reward model losses: ['0.0630', '1.0588', '0.4713', '0.0189', '0.2106']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.5254', '1.4919', '1.5824', '1.5510', '1.6309']
Training action VAE models...
Action VAE losses: ['0.8404', '0.9070', '0.7989', '0.8594', '0.9095']
CM score components:
transition disagreement: 0.4195
reward disagreement: 0.4140
state disagreement: 0.5214
action disagreement: 0.5070
total CM score: 1.8619
friction is complete. CM score: 1.8619
Processing intervention 4/6: visual
IntervenedCausalWorld created with visual intervention
Evaluating CM score for visual intervention...
Reset #1: visual intervention applied (success: True)
episode 1: 501 steps, reward: 2.763
Reset #2: visual intervention applied (success: True)
episode 2: 501 steps, reward: 2.763
Reset #3: visual intervention applied (success: True)
episode 3: 501 steps, reward: 2.763
total data points collected: 2505
average episode length: 501.0
average episode reward: 2.763
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.0970', '7.7422', '7.1177', '7.1637', '7.4887']
Training reward models...
Reward model losses: ['0.0438', '0.2943', '0.0969', '0.0087', '0.0572']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.2716', '1.3075', '1.4036', '1.3452', '1.5224']
Training action VAE models...
Action VAE losses: ['0.9070', '0.8045', '0.8739', '0.9570', '0.9849']
CM score components:
transition disagreement: 0.4301
reward disagreement: 0.1434
state disagreement: 0.4633
action disagreement: 0.5258
total CM score: 1.5625
visual is complete. CM score: 1.5625
Processing intervention 5/6: pose
IntervenedCausalWorld created with pose intervention
Evaluating CM score for pose intervention...
Reset #1: pose intervention applied (success: True)
episode 1: 501 steps, reward: 4.740
Reset #2: pose intervention applied (success: True)
episode 2: 501 steps, reward: 1.195
Reset #3: pose intervention applied (success: True)
episode 3: 501 steps, reward: -2.089
total data points collected: 2505
average episode length: 501.0
average episode reward: 1.402
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['6.7774', '7.7328', '7.6917', '6.5851', '7.4051']
Training reward models...
Reward model losses: ['2.8960', '1.0877', '0.1337', '0.0146', '0.0239']
Training state VAE models...
Training state VAE models...
State VAE losses: ['2.3026', '1.6534', '1.6312', '1.7075', '1.6121']
Training action VAE models...
Action VAE losses: ['1.2439', '1.3869', '1.3111', '1.4164', '1.2214']
CM score components:
transition disagreement: 0.4407
reward disagreement: 0.7189
state disagreement: 0.5583
action disagreement: 0.5563
total CM score: 2.2742
pose is complete. CM score: 2.2742
Processing intervention 6/6: random
IntervenedCausalWorld created with random intervention
Evaluating CM score for random intervention...
Reset #1: random intervention applied (success: True)
episode 1: 501 steps, reward: -0.790
Reset #2: random intervention applied (success: True)
episode 2: 501 steps, reward: -1.197
Reset #3: random intervention applied (success: True)
episode 3: 501 steps, reward: -1.369
total data points collected: 2505
average episode length: 501.0
average episode reward: 0.833
termination reasons: ['max_length', 'max_length', 'max_length', 'max_length', 'max_length']
success rate: 0/5
tensor shapes - states: torch.Size([2505, 56]), actions: torch.Size([2505, 9])
Creating ensembles (obs_dim=56, act_dim=9, hidden_dim=64)
Training transition models...
Transition model losses: ['7.5486', '7.3157', '8.3317', '7.4639', '7.3324']
Training reward models...
Reward model losses: ['0.1096', '0.0155', '0.0178', '0.2182', '0.1679']
Training state VAE models...
Training state VAE models...
State VAE losses: ['1.3497', '1.3171', '1.4300', '1.3894', '1.4683']
Training action VAE models...
Action VAE losses: ['1.3795', '1.3357', '1.3337', '1.4322', '1.4988']
CM score components:
transition disagreement: 0.3849
reward disagreement: 0.0858
state disagreement: 0.5054
action disagreement: 0.5719
total CM score: 1.5480
random is complete. CM score: 1.5480
IntervenedCausalWorld created with visual intervention
Reset #1: visual intervention applied (success: True)
Logging to ppo_pushing_sb3/PPO_0
Reset #2: visual intervention applied (success: True)
Reset #3: visual intervention applied (success: True)
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 251       |
|    ep_rew_mean     | 2.2542775 |
| time/              |           |
|    fps             | 477       |
|    iterations      | 1         |
|    time_elapsed    | 8         |
|    total_timesteps | 5124096   |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 251          |
|    ep_rew_mean          | 2.2542775    |
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 2            |
|    time_elapsed         | 17           |
|    total_timesteps      | 5128192      |
| train/                  |              |
|    approx_kl            | 0.0042965803 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -22          |
|    explained_variance   | 0.734        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.401       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.0104      |
|    std                  | 2.79         |
|    value_loss           | 0.175        |
------------------------------------------
Traceback (most recent call last):
  File "meta_teacher_student.py", line 782, in <module>
    main()
  File "meta_teacher_student.py", line 748, in main
    student_model.learn(total_timesteps=args.student_train_steps, reset_num_timesteps=False)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 250, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 178, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 44, in step_wait
    self.actions[env_idx]
  File "meta_teacher_student.py", line 167, in step
    return self.base_env.step(action)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/causalworld.py", line 268, in step
    self._robot.apply_action(action)
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/robot/trifinger.py", line 350, in apply_action
    self.step_simulation()
  File "/home/kpatherya3/anaconda3/envs/causal_env/lib/python3.7/site-packages/causal_world/envs/robot/trifinger.py", line 317, in step_simulation
    physicsClientId=self._pybullet_client_full_id)
KeyboardInterrupt
